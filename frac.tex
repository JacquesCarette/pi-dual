\documentclass{llncs}
\usepackage{url}
\usepackage{proof}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{listings}
\usepackage{graphicx}

\newcommand{\inkscape}[2][1.5]{
\begin{center}
\scalebox{#1}{
\includegraphics{diagrams/#2}
}
\end{center}
}

\newcommand{\todo}[1]{\textbf{TODO:} #1}
\newcommand{\jacques}[1]{\textsc{Jacques says:} #1}

\hyphenation{a-reas}

%subcode-inline{bnf-inline} name langRev
%! swap+ = \mathit{swap}^+
%! swap* = \mathit{swap}^*
%! dagger =  ^{\dagger}
%! assocl+ = \mathit{assocl}^+
%! assocr+ = \mathit{assocr}^+
%! assocl* = \mathit{assocl}^*
%! assocr* = \mathit{assocr}^*
%! identr* = \mathit{uniti}
%! identl* = \mathit{unite}
%! dist = \mathit{distrib}
%! factor = \mathit{factor}
%! (o) = \fatsemi
%! (;) = \fatsemi
%! (*) = \times
%! (+) = +
%! foldB = fold_B
%! unfoldB = unfold_B
%! foldN = fold_N
%! unfoldN = unfold_N
%! trace+ = \mathit{trace}^{+}
%! trace* = \mathit{trace}^{\times}
%! :-* = \multimap
%! :-+ = \multimap^{+}
%! emptyset = \emptyset

%subcode-inline{bnf-inline} regex \{\{(((\}[^\}])|[^\}])*)\}\} name main include langRev
%! [^ = \ulcorner
%! ^] = \urcorner
%! [v = \llcorner
%! v] = \lrcorner
%! [[ = \llbracket
%! ]] = \rrbracket
%! eta* = \eta
%! eps* = \epsilon
%! Union = \bigcup
%! in = \in
%! |-->* = \mapsto^{*}
%! |-->> = \mapsto_{\ggg}
%! |-->let = \mapsto_{let}
%! |--> = \mapsto
%! <--| = \mapsfrom
%! |- = \vdash
%! <=> = \Longleftrightarrow
%! <-> = \leftrightarrow
%! ~> = \leadsto
%! ::= = ::=
%! /= = \neq
%! vi = v_i
%! di = d_i
%! si = s_i
%! sj = s_j
%! F = \texttt{F}
%! T = \texttt{T}
%! forall = \forall
%! exists = \exists
%! empty = \emptyset
%! eta = \eta
%! where = \textbf{where}
%! epsilon = \varepsilon
%! least = \phi
%! loop+ = loop_{+}
%! loop* = loop_{\times}
%! CatC = {\mathcal C}
%! CatA = {\mathcal A}
%! gamma = \gamma
%! {[ = \{
%! ]} = \}
%! elem = \in
%! dagger = ^\dagger
%! alpha = \alpha
%! beta = \beta
%! rho = \rho
%! @@ = \mu
%! @ = \,@\,
%! langRev = \Pi
%! langRevT = \Pi^{o}
%! langRevEE = \Pi^{\eta\epsilon}_{+}
%! bullet = \bullet
%! * = \times
%! langRevT = \Pi^{o}
%! langRevTF = \Pi^{/}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{Fractional Types}
\author{Roshan P. James \and Zachary Sparks \and Jacques Carette \and Amr Sabry}
\institute{}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
  In previous work, we developed a \emph{first-order},
  information-preserving, and reversible programming language {{langRev}}
  founded on type isomorphisms. Being restricted to first-order types limits
  the expressiveness of the language: it is not possible, for example, to
  abstract common program fragments into a higher-level combinator. In this
  paper, we introduce a higher-order extension of {{langRev}} based on the
  novel concept of \emph{fractional types} {{1/b}}. Intuitively, a value of a
  fractional type {{1/v}} represents \emph{negative} information. A
  higher-order function is modeled by a pair {{(1/v1,v2)}} with {{1/v1}}
  representing the needed argument and {{v2}} representing the
  result. Fractional values are first-class: they can be freely propagated
  and transformed but must ultimately --- in a complete program --- be offset
  by the corresponding amount of positive information.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction} 

We are witnessing a convergence of ideas from several distinct research
communities (physics, mathematics, and computer science) towards replacing
\emph{equalities} by \emph{isomorphisms}. The combined programme originating
from these research areas has sparked a huge amount of research that unveiled
new and surprising connections between geometry, algebra, logic, and
computation (see~\cite{baez2011physics} for an overview).

In the physics community, Landauer~\cite{Landauer:1961}, Feynman, and others
have interpreted the laws of physics as fundamentally related to
computation. In more detail, the great majority of the laws of physics are
formulated as equalities between different physical observables. For
instance, Newton's second law of classical mechanics equates the force acting
on a system to its rate of change of momentum. The insight of Landauer,
Feynman, etc. is that \emph{different} physical observables should not be
related by an \emph{equality} but more appropriately by an \emph{isomorphism}
that witnesses, explains, and models the process of transforming one
observable to the other.

In the mathematics and logic community, Martin-L\"of developed an extension
of the simply typed $\lambda$-calculus originally intended to provide a
rigorous framework for constructive mathematics. This has been further
extended (first by Streicher and Awodey, and subsequently by many others)
with a new \emph{identity type} representing the proposition
that two terms are ``equal.'' Specifically, given two terms $a$ and $b$ of
the same type $A$, one forms the type $\texttt{Id}_A(a,b)$ representing the
proposition that $a$ and $b$ are equal: in other words, a term of type
$\texttt{Id}_A(a,b)$ witnesses, explains, and models the process of
transforming $a$ to $b$ and vice-versa.

In the computer science community, the theory and practice of type
isomorphisms is well-established. Originally, such type isomorphisms were
motivated by the pragmatic concern of searching large libraries of functions
by providing one of the many possible isomorphic types for the desired
function. More recently, type isomorphisms have taken a more central role as
\emph{the} fundamental computational mechanism from which more conventional,
i.e., irreversible computation, is derived~\cite{infeffects}. In more detail,
in our own previous work, we started with the notion of type isomorphism as
the ``right'' starting point for a theory of computation. The central
technical contribution was the development of a family of programming
languages, {{langRev}} with various superscripts, in which every computation
is an isomorphism. Other major results are that all such computations
preserve the information-theoretic entropy, that the language is logically
reversible, and that it has a model in symmetric monoidal categories (which
are the backbone of models for linear logic and quantum computation). We have
also invested a significant amount of time in using {{langRev}} as an actual
programming language and established several idioms for programming large
classes of interesting (recursive) programs in {{langRev}}.

A major open problem remains, however. This concerns a higher-order extension
of {{langRev}}. This extension is of fundamental importance in all the
originating research areas. In physics, it allows the process itself or the
observer to be treated as ``data'' that can be transformed by higher-level
processes or observed by meta-level observers. In mathematics and logic, it
allows the equivalence between different proofs of type $\texttt{Id}_A(a,b)$
to be itself expressed as an isomorphism as a higher type
$\texttt{Id}_{\texttt{Id}_A(a,b)}(p,q)$. Finally, in computer science,
higher-order types allow code to abstract over other code fragments as well
as the manipulation of code as data and data as code which is central to many
algorithms and application domains.

This paper addresses this higher-order extension.  It does this via a novel
type construction, which we dub a \emph{fractional type} because of its
interaction with product types.  By adding a formal (linear) dual to
(type theoretic) products, we can internalize the notion that ``taking an
input'' corresponds to negative information, which turns out to be the 
basis of first-class functions.

The plan of the paper is therefore
as follows: \todo{roadmap}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background: {{langRev}} }
\label{sec:pi}

We review our language {{langRev}} providing the necessary background and
context for our higher-order extension.\footnote{The presentation in this
  section focuses on the simplest version of {{langRev}}. Other versions
  include the empty type, recursive types, and trace operators but these
  extensions are orthogonal to the higher-order extension emphasized in this
  paper.} The terms of {{langRev}} are not classical values and functions;
rather, the terms are isomorphism witnesses.  In other words, the terms of
{{langRev}} are proofs that certain ``shapes of values'' are isomorphic.
And, in classical Curry-Howard fashion, our operational semantics shows how
these proofs can be directly interpreted as actions on ordinary values which
effect this shape transformation. Of course, ``shapes of values'' are very
familiar already: they are usually called \emph{types}.  But usually one
designs a type system as a method of classifying terms, with the eventual
purpose to show that certain properties of well-typed terms hold, such as
safety.  Our approach is different: we start from a type system, and then
present a term language which naturally inhabits these types, along with an
appropriate operational semantics.

\paragraph*{Data.}
We view {{langRev}} as having two levels:  it has traditional values, given by
%subcode{bnf} include main
% values, v ::= () | left v | right v | (v, v)
\noindent and these are classified by ordinary types
%subcode{bnf} include main
% value types, b ::= 1 | b + b | b * b 

Types include the unit type {{1}}, sum types {{b1+b2}}, and products types
{{b1*b2}}.  Values includes {{()}} which is the only value of type {{1}},
{{left v}} and {{right v}} which inject {{v}} into a sum type, and
{{(v1,v2)}} which builds a value of product type. But these values should be
regarded as largely ancillary.  In particular we do not treat the above
values as first-class citizens.  They only occur when we want to observe the
effect of an isomorphism.

\paragraph*{Isomorphisms.} The terms of {{langRev}} are witnesses to type
isomorphisms.  They have (iso) types {{b <-> b}}.  
Specifically, they are witnesses to the following type isomorphisms:
%subcode{bnf} include main
%! columnStyle = r@{\hspace{-0.5pt}}c@{\hspace{-0.5pt}}l
%swap+ :&  b1 + b2 <-> b2 + b1 &: swap+
%assocl+ :&  b1 + (b2 + b3) <-> (b1 + b2) + b3 &: assocr+
%identl* :&  1 * b <-> b &: identr*
%swap* :&  b1 * b2 <-> b2 * b1 &: swap*
%assocl* :&  b1 * (b2 * b3) <-> (b1 * b2) * b3 &: assocr*
%dist :&~ (b1 + b2) * b3 <-> (b1 * b3) + (b2 * b3)~ &: factor

\noindent Each line of the above table introduces a pair of dual constants%
\footnote{where {{swap*}} and {{swap+}} are self-dual} that witness the type
isomorphism in the middle.  These are the base (non-reducible) terms of the
second, principal level of {{langRev}}.

Note how the above has two readings: first as a set of typing relations for a
set of constants. Second, if these axioms are seen as universally quantified,
orientable statements, they also induce transformations of the (traditional)
values.  The (categorical) intuition here is that these axioms have
computational content because they witness isomorphisms rather than merely
stating an extensional equality.

The isomorphisms are extended to form a congruence relation by adding the
following constructors that witness equivalence and compatible closure:
%subcode{proof} include main
%@  ~
%@@ id : b <-> b 
%
%@ c : b1 <-> b2
%@@ sym c : b2 <-> b1
%
%@ c1 : b1 <-> b2
%@ c2 : b2 <-> b3
%@@ c1(;)c2 : b1 <-> b3
%---
%@ c1 : b1 <-> b3
%@ c2 : b2 <-> b4
%@@ c1 (+) c2 : b1 + b2 <-> b3 + b4
%
%@ c1 : b1 <-> b3
%@ c2 : b2 <-> b4
%@@ c1 (*) c2 : b1 * b2 <-> b3 * b4

\noindent The syntax is overloaded: we use the same symbol at the value-type level
and at the isomorphism-type level for denoting sums and products.  Hopefully
this will not cause undue confusion.

It is important to note that ``values'' and ``isomorphisms'' are 
completely separate syntactic categories which do not intermix.  The 
semantics of the language come when these are made to interact at
the ``top level'', a new syntactic category:
%subcode{bnf} include main
% top level term, l ::= c v
\noindent We refer to this as \emph{application}.

\noindent
To summarize, the syntax of {{langRev}} is given as follows. 

\begin{definition}{(Syntax of {{langRev}})}
\label{def:Pi}
%subcode{bnf} include main
% value types, b ::= 1 | b+b | b*b 
% values, v ::= () | left v | right v | (v,v) 
%
% iso.~types, t ::= b <-> b
% base iso ::= swap+ | assocl+ | assocr+ 
%     &|& unite | uniti | swap* | assocl* | assocr* 
%     &|& dist | factor 
% iso comb., c ::= iso | id | sym c | c (;) c | c (+) c | c (*) c 
% top level term, l ::= c v
\end{definition}

Mathematically, {{langRev}} can be interpreted in the category of finite sets
and bijections: each value type denotes a finite set of the corresponding size
and each combinator {{c : b1 <-> b2}} denotes a bijection between the sets
denoted by {{b1}} and {{b2}}. Operationally, the semantics of {{langRev}} is
given using two mutually recursive interpreters: one going forward and one
going backwards. The use of {{sym}} switches control from one evaluator to
the other. We will not present the operational semantics until the next
section, in the context of the extended language. For now, we state without
proof that the evaluation of well-typed combinators always terminates and
that {{langRev}} is logically reversible, i.e., that for all combinators 
{{c : b1 <-> b2}} and values {{v1 : b1}} and {{v2 : b2}} we have the forward
evaluation of {{c v1}} produces {{v2}} iff the backwards evaluation of 
{{c v2}} produces {{v1}}.

\jacques{Since we have Agda code for the above, should we refer to it
here, and make it available?}

Note that a small modification of {{langRev}} (the addition of an
empty type {{0}} with its usual laws) would exactly capture, at the type
level, the notion of \emph{semiring} (occasionally called a \emph{rig}) where
we replace equality by isomorphism.  This can also be also compared to
\emph{bimonoidal categories}.  As is usual with with type theories, the
coherence conditions (pentagonal and triangle identity) are not explicit,
but would correspond to certain identity types being trivial.  {{langRev}}
instead, at the type level, models a commutative ringoid where the
multiplicative structure forms a commutative monoid, but the additive structure
is just a commutative magma.  We chose not to include {{0}} as a technical
convenience for the next section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Language: {{langRevTF}} }

The language {{langRev}} models isomorphisms of values rather well.  However,
purposefully, it has no notion of \emph{input} or \emph{output}; more
precisely, because of strict preservation of information, these two concepts
coincide.  This is the root of reversibility.

But if we want to model functions of any flavor, we need to differentiate
between these notions.  Our idea, inspired by computational dualities and
polarity, is to introduce a \emph{formal dual} for our types.  We thus
consider a type in a negative position to possess negative information, as it
is really a request for information.  Since information is logarithmic, this
means that a request for information should behave like a \emph{fractional
  type}.

Syntactically, the extension with fractional types is simple. 

\begin{definition}[Syntax of {{langRevTF}}]
\label{def:langRevT}
We extend Def.~\ref{def:Pi} with:
%subcode{bnf} include main
% value types, b ::= ... | 1/b
% values, v ::= ... | 1/v
%
% base iso ::= ... | eta*_b | eps*_b
\end{definition}

For a given type {{b}}, the values of type {{1/b}} are of the form {{1/v}}
where {{v : b}}. Note that {{1/v}} is purely formal, as is {{1/b}}.  However,
we think of a value like {{1/left ()}} as a \emph{first-class constraint}
that can only be satisfied if it is matched with an actual value {{left ()}}
or as representing the absence of some information (i.e., \emph{negative
  information}), that can only be supplied by {{left ()}}. The combinator
{{eta*_b : 1 <-> 1/b * b}} represents a fission point which creates --- out
of no information --- an equal amount of negative and positive
information. Symmetrically, the combinator {{eps*_b : 1/b * b <-> 1}} matches
up an equal amount of negative and positive information producing no residual
information.  The best analogy is probably that of vectors and co-vectors,
also know as column vectors and row vectors (respectively): in the finite
dimensional case, they are isomorphic, but they are nevertheless quite
different, being $(1,0)$ and $(0,1)$ tensors (respectively).  In other words,
since we are explicitly interested in \emph{isomorphisms}, we should not
conflate negative-information values with positive-information values just
because they happen to be isomorphic.

In {{langRev}} presented in Sec.~\ref{sec:pi}, we have been able to maintain
the simple idea that a syntactic isomorphism denotes a \emph{bijection}. In
richer languages, e.g., in a Turing complete language that admits
non-termination, isomorphisms necessarily denote \emph{partial
  bijections}~\cite{rc2011,infeffects}. As we demonstrate in this section,
once we make the extension to higher-order functions, we must interpret the
syntactic isomorphisms as \emph{relations}. The intuitive idea, is that the
fission point combinator {{eta*_b : 1 <-> 1/b * b}} must be able to map the
element {{() : 1}} to any pair of the form {{(1/v,v)}} with
{{v:b}}. Similarly, the combinator {{eps*_b : 1/b * b <-> 1}} must be able to
accept pairs {{(1/v1,v2)}} in which the negative information {{1/v1}} does
not match the positive information {{v2}}. On such pairs, {{eps*_b}} is
undefined, i.e., it does not produce {{():1}}. In other words, some of the
combinators are not partial and some produce more than one result for a
particular input, and hence we move to the setting of general binary
relations to interpret our combinators. 

This setting is not only consistent with logical reversibility but is
arguably the \emph{right} framework for considering expressive reversible
languages (including the interesting examples of quantum languages).  Indeed,
even though the categorical semantics of {{langRevTF}} is not the main focus
of this paper, we find it helpful to state that the category of finite sets
and relations in which we are going to interpret our language, is an example
of a \emph{biproduct dagger compact closed category} which are categories
introduced by Abramsky and Coecke~\cite{Abramsky:2004:CSQ:1018438.1021878}
and by Selinger~\cite{Selinger:2007:DCC:1229185.1229207} to model quantum
computation.\footnote{The other common example of these categories is that of
  finite dimensional Hilbert spaces.} Two of the most relevant features of
these categories for our purposes are that they support an adjoint for each
morphism (to model reversibility) and that they support higher-order relation
(built from the categorical dagger which is modeled by our fractional
types). The latter is intuitively clear as any relation between $A$ and $B$
can be represented as a subset of {{A * B}}, i.e., as a value.

To interpret {{langRevTF}} in this category, we associate a finite set
{{ [[ b ]] }} with each type {{b}} and a relation {{ [[ c ]] }} over
{{ [[ b1 ]] }} and {{ [[ b2 ]] }} for each combinator {{ c : b1 <-> b2 }}.

\begin{definition}[Denotation of Value Types {{ [[ b ]] }}]
\label{chx:def:denot}
Each type denotes a finite set of values as follows:
\[\begin{array}{rcl}
\llbracket 1 \rrbracket &=& \{ () \} \\
\llbracket b_1 + b_2 \rrbracket &=& \{ \mathit{left}~v ~|~ v \leftarrow \llbracket b_1 \rrbracket \} \cup \{ \mathit{right}~v ~|~ v \leftarrow \llbracket b_2 \rrbracket \} \\
\llbracket b_1 \times b_2 \rrbracket &=& \{ (v_1,v_2) ~|~ v_1 \leftarrow \llbracket b_1 \rrbracket, v_2 \leftarrow \llbracket b_2 \rrbracket \} \\
\llbracket 1/b \rrbracket &=& \{ 1/v ~|~ v \leftarrow \llbracket b \rrbracket \} 
\end{array}\]
\end{definition}

The denotation of combinators as relations is given below. We specify
relations using a deductive system whose judgments are of the form 
{{ v1 ~~c~~ v2 }} indicating that the pair {{(v1,v2)}} is in the relation 
denoted by {{c}}.

\begin{definition}[Relational semantics]
\label{def:relational-langRevTF}
Each combinator {{c : b1 <-> b2}} in {{langRevTF}} denotes a relation
as specified below. 

%subcode{proof} include main
%@ ~
%@@ (left v) ~~swap+~~ (right v)
%
%@ ~
%@@ (right v) ~~swap+~~ (left v)
%----
%@ ~
%@@ (left v) ~~assocl+~~ (left (left v))
%
%@ ~
%@@ (right (left v)) ~~assocl+~~ (left (right v))
%----
%@ ~
%@@ (right (right v)) ~~assocl+~~ (right v)
%
%@ ~
%@@ (left (left v)) ~~assocr+~~ (left v)
%----
%@ ~
%@@ (left (right v)) ~~assocr+~~ (right (left v))
%
%@ ~
%@@ (right v) ~~assocr+~~ (right (right v))
%----
%@ ~
%@@ ((), v) ~~unite~~ v
%
%@ ~
%@@ v ~~uniti~~ ((), v)
%----
%@ ~
%@@ (v1, v2) ~~swap*~~ (v2, v1)
%----
%@ ~
%@@ (v1, (v2, v3)) ~~assocl*~~ ((v1, v2), v3)
%
%@ ~
%@@ ((v1, v2), v3) ~~assocr*~~ (v1, (v2, v3))
%----
%@ ~
%@@ (left v1, v3) ~~dist~~ (left (v1, v3))
%
%@ ~
%@@ (right v2, v3) ~~dist~~ (right (v2, v3))
%----
%@ ~
%@@ (left (v1, v3)) ~~factor~~ (left v1, v3)
%
%@ ~
%@@ (right (v2, v3)) ~~factor~~ (right v2, v3)
%----
%@ ~
%@@ v ~~id~~ v
%
%@ v' ~~c~~ v
%@@ v ~~(sym c)~~ v'
%
%@ v1 ~~c1~~ v2
%@ v2 ~~c2~~ v3
%@@ v1 ~~(c1(;)c2)~~ v3
%----
%@ v ~~c1~~ v'
%@@ (left v) ~~(c1 (+) c2)~~ (left v')
%
%@ v ~~c2~~ v'
%@@ (right v) ~~(c1 (+) c2)~~ (right v')
%----
%@ v1 ~~c1~~ v1'
%@ v2 ~~c2~~ v2'
%@@ (v1, v2) ~~(c1 (*) c2)~~ (v1', v2')
%----
%@ v in [[ b ]]  
%@@ () ~~eta*_b~~ (1/v,v)
%
%@ ~
%@@ (1/v,v) ~~eps*_b~~ ()
\end{definition}

The language {{langRevTF}} is still logically reversible. This is intuitively
clear as the relation denoted by a combinator can be used in the reverse
direction by swapping inputs and outputs.

\begin{proposition}[Logical Reversibility]
\label{chx:prop:logrev-tracep}
For all combinators {{c : b1 <-> b2}} and values {{v1 : b1}} and 
{{v2 : b2}} we have {{v1 ~~c~~ v2}} iff {{v2 ~~(sym c)~~ v1}}.
\end{proposition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Expressiveness} 

Fractional types and values add considerable expressiveness to our
language. In the following, we use {{bool}} as an abbreviation of {{1+1}} 
with {{true}} as an abbreviation for {{left ()}} and {{false}} as an 
abbreviation for {{right ()}}.

%%%%%%%%%%%%
\subsection{First-Class Relations}

As motivated, with fractional types, we can define a value that denotes a
first-class function or relation. Indeed a value of type {{1/b1 * b2}} is a
pair of a \emph{constraint} that can only be satisfied by some {{v1 : b1}}
and a value {{v2 : b2}}. In other words, it corresponds to a function or
relation which when given matched with a value {{v1 : b1}} ``releases'' the
value {{v2 : b2}}. To emphasize this view, we introduce the abbreviation:

%subcode{bnf} include main
%  b1 :-* b2 &::=& 1/b1 * b2

\noindent which suggests a function-like behavior for the pair of a fractional value
and a regular value. What is remarkable is that we can almost trivially turn
any combinator {{c : b1 <-> b2}} into a (constant) value of 
type {{b1 :-* b2}} as shown below:

%subcode{opsem} include main
%! columnStyle = rcl
% name &:& (b1 <-> b2) -> (1 <-> (b1 :-* b2))
% name c &=& eta*_{b1} (;) (id (*) c)

As a simple example, consider combinator {{not}} denoting boolean negation.
It is straightforward to check that the combinator 
{{name not : 1 <-> (bool :-* bool)}} denotes the following
relation:

%subcode{proof} include main
%@ ~
%@@ () ~~(name not)~~ (1/false,true)
%
%@ ~
%@@ () ~~(name not)~~ (1/true,false)

\noindent The relation {{name not}} can therefore be used to convert any
{{false}} to {{true}} and vice-versa. We formalize this ``apply'' combinator
below.

%%%%%%%%%%%%
\subsection{Higher-Order Relations}

We are now a small step to implementing various higher-order
combinators that manipulate functions or relations. In particular, we
can \emph{apply} and \emph{compose} values representing relations as
shown below:

%subcode{opsem} include main
%! columnStyle = rcl
% apply &:& (b1 :-* b2) * b1 <-> b2
% apply &=& swap* (;) assocl* (;) (swap* (*) id) (;) (eps*_{b1} (*) id) (;) unite

\noindent At the type level, the computation proceeds as follows:
%subcode{bnf} include main
%  (b1 :-* b2) * b1 &=& (1/b1 * b2) * b1 & 
%     &<->& b1 * (1/b1 * b2) & (swap*)
%     &<->& (b1 * 1/b1) * b2) & (assocl*)
%     &<->& (1/b1 * b1) * b2) & (swap* (*) id)
%     &<->& 1 * b2 & (eps*{b1} (*) id)
%     &<->& b2 & (unite)

\noindent Intuitively, we simply match the incoming argument of type {{b1}} with
the constraint encoded by the function. If they match, they cancel
each other and the value of type {{b2}} is exposed with no
constraints. Otherwise, the mismatched values are \emph{annihilated}.

Function or relation composition is only slightly more involved:

%subcode{opsem} include main
%! columnStyle = rcl
% compose &:& (b1 :-* b2) * (b2 :-* b3) -> (b1 :-* b3)
% compose &=& assocr* (;) (id (*) (assocl* (;) (swap* (*) id) (;) (eps*_{b2} (*) id) (;) unite))

\noindent Again, at the type level, the computation proceeds as follows:

%subcode{bnf} include main
%  (b1 :-* b2) * (b2 :-* b3) &=& (1/b1 * b2) * (1/b2 * b3) & 
%     &<->& (1/b1 * (b2 * (1/b2 * b3)) & (assocr*)
%     &<->& (1/b1 * ((b2 * 1/b2) * b3) & (assocl*)
%     &<->& (1/b1 * ((1/b2 * b2) * b3) & (assocl*)
%     &<->& (1/b1 * (1 * b3) & (eps*_{b2})
%     &<->& (1/b1 * b3) & (unite)
%     &=& b1 :-* b3

%%%%%%%%%%%%
\subsection{Duality}

The final remarkable property of fractional types is that they
constitute, a form of self-inverse negation. In particular, we have:

%subcode{opsem} include main
%! columnStyle = rcl
% doubleNeg &:& b <-> 1/(1/b)
% doubleNeg &=& uniti (;) (eta*_{1/b} (*) id) (;) assocr* (;) (id (*) eps*_b) (;) swap* (;) unite

\noindent At the type level, this computation proceeds as follows:

%subcode{bnf} include main
%  b &<->& (1 * b) & (uniti)
%    &<->& (1/(1/b) * 1/b) * b & (eta*_{1/b} (*) id)
%    &<->& 1/(1/b) * (1/b * b) & (assocr*)
%    &<->& 1/(1/b) * 1 & (id (*) eps*_b)
%    &<->& 1 * 1/(1/b) & (swap*)
%    &<->& 1/(1/b) & (unite)

%%%%%%%%%%%%
\subsection{Feedback, Iteration, and Trace Operators}

Mathematically speaking, recursion and iteration can be expressed using
categorical trace
operators~\cite{joyal1996traced,Hasegawa:1997:RCS:645893.671607}.  In a
language like {{langRev}} there are two natural families of trace operators
that can be defined, an additive family (explored in detail in our previous
work~\cite{rc2011}) and a multiplicative family (which is the focus of this
section).

In this case, we are given a computation {{c:b1 * b2 <-> b1 * b3}} and we
want to cancel the common type {{b1}}, to produce a new combinator
{{trace*~c:b2<->b3}}. For the evaluation of such a combinator, we are only
given a value {{v2 : b2}} but to evaluate {{c}}, we must provide a pair
consisting of some value {{v1 : b1}} together with the given {{v2 : b2}}. The
needed value {{v1 : b1}} cannot be arbitrary, however. We do have a
constraint that the value {{v1 : b1}} must be such that it is also produced
by as the first component of the result. In general, there may be several
such values fixed-point values or none.  A few examples should help
understand the subtleties.

\begin{example}
Consider the combinator {{c}}:

{{c : bool * 1 <-> bool * 1}} 

{{c = id}}

\noindent Using the multiplicative trace operator, we can construct the
combinator {{trace* c : 1 <-> 1}}. Applying this combinator to 
{{() : 1}} requires us to find a value {{b : bool}} such that 
{{(b,()) ~~c~~ (b,())}}. Given that {{c}} is the identity and that the type {{bool}} 
has two values, there are two values {{false}} and {{true}} that satisfy the 
constraint. In other words, expect that {{() ~~(trace* c)~~() }}.
\end{example}

\begin{example}
\label{ch3:ex:annihilate}
Consider a small variation on the combinator {{c}} above:

{{c : bool * 1 <-> bool * 1}} 

{{c = swap+ (*) id }}

\noindent which negates the boolean component of its incoming
pair. Using the multiplicative trace operator, we can construct the
combinator {{trace* c : 1 <-> 1}} as before. But now, applying this
combinator to {{() : 1}} requires us to find a value {{b : bool}} such
that {{(b,()) ~~c~~ (b,())}} which is impossible since {{c}} negates
{{b}}. Operationally, we would expect the evaluation of such a
combinator to produce no value. 
\end{example}

With fractionals, the multiplicative trace operator becomes expressible:

%subcode{opsem} include main
%! columnStyle = rcl
% trace*_b &:& ((b * b1) <-> (b * b2)) -> (b1 <-> b2)
% trace*_b c &=& uniti (;) (eta*_b (*) id) (;) assocr* (;) 
%            && (id (*) c) (;) assocl* (;) (eps*_b (*) id) (;) unite

Let's use the semantics to calculate the result of applying
{{trace*_{bool} (not (*) id)}} to {{false}}. We start
with the set {{ {[ false ]} }} which gets transformed as follows:
%subcode{opsem} include main
%! columnStyle = ll
% {[ ((),false) ]}  & (uniti)
% {[ ((1/false,false),false), ((1/true,true),false) ]} & (eta*_{bool})
% {[ (1/false, (false,false)), (1/true,(true,false)) ]} & (assocr*)
% {[ (1/false, (true,false)), (1/true,(false,false)) ]} & (not (*) id)
% {[ ((1/false,true),false), ((1/true,false),false) ]} & (assocl*)
% emptyset & eps*_{bool}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Examples} 

Now that we have the formal semantics, we consider a generalized version of
Ex.~\ref{ch3:ex:annihilate}.

\begin{example}[Annihilation]
\label{ch3:ex;annihilation}
Consider the circuit below:
\begin{center}
\scalebox{1.2}{
  \includegraphics{diagrams/not_trace.pdf}
}
\end{center}
As motivated earlier, it is clearly impossible for the constraint implied by
{{trace*}} to be satisfied as a boolean value {{v}} can never be the same as
its negation. Thus there is no incoming value of type {{b}} that can ever go
through the circuit. Indeed, if we call the circuit above {{c}}, then we can
calculate that {{c}} does not relate {{v}} to anything. We call such a
circuit an annihilation circuit.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{(Finite) Relational Programming}
\label{ch3:sec:lp}

Relational programming leverages set-theoretic relations and their
composition to express computational tasks in a declarative way. For
instance, consider this example:

{{parent = {[ (A,B), (B,C), (A,D) ]} }}

{{grandparent = parent (o) parent}}

\noindent The example defines a relation {{parent}} specified using a
set of tuples and another relation {{grandparent}} specified using the
composition of two parent relations. If we wrote this example in a
relational language (e.g., Prolog) and we executed the query
{{grandparent(A)}}, we would get the answer {{ {[C]} }}.

It turns out that with the addition of the multiplicative trace, and
the move to relations motivated in the previous section, we can
express relational programming. We illustrate the idea using a
complete example. 

Consider the relation {{R}} on booleans given by:

{{ {[(false,false), (false,true), (true,false) ]}. }}

We show how to define a combinator {{c_R}} whose semantics is such that:

%subcode{proof} include main
%@ ~
%@@ false ~~c_R~~ false
%
%@ ~
%@@ false ~~c_R~~ true
%
%@ ~
%@@ true ~~c_R~~ false

The key idea is to find a combinator {{cInner : (a * bool) <-> (a * bool)}}
for some type {{a}} such that {{trace* cInner}} is the desired combinator
{{c_R}}. A little experimentation shows that we get the desired behavior
if {{cInner}} is chosen to behave as follows:

%subcode{proof} include main
%@ ~
%@@ ((false,false),false) ~~cInner~~ ((false,false),false) 
%----
%@ ~
%@@ ((false,true),false) ~~cInner~~ ((false,true),true) 
%----
%@ ~
%@@  ((false,true),true) ~~cInner~~ ((false,true),false) 
%----
%@ ~
%@@ ((false,false),true) ~~cInner~~ ((true,false),true) 
%----
%@ ~
%@@ ((true,false),false) ~~cInner~~ ((true,true),false)  
%----
%@ ~
%@@ ((true,false),true) ~~cInner~~ ((true,true),true)  
%----
%@ ~
%@@ ((true,true),false) ~~cInner~~ ((true,false),false) 
%----
%@ ~
%@@ ((true,true),true) ~~cInner~~ ((false,false),true) 

In the first three lines, the first argument is a fixed point and hence the
multiplicative trace would map the second input to the second output
producing the desired relation. In the remaining five cases, the first
argument is not a fixed point and hence all these cases would be rejected as
solutions to the constraint imposed by the multiplicative trace. It simply
remains to find the actual realization of {{cInner}} that would produce the
behavior above. One can infer the following implementation:

%subcode{opsem} include main
%! columnStyle = rcl
% cInner &:& (bool * bool) * bool <-> (bool * bool) * bool
% cInner &=& sym (swap* (;) assocl* (;) (cnot (*) id) (;) assoct* (;) swap* (;) 
%        &&  toffoli (;) ((swap* (;) cnot (;) swap*) (*) id) (;)
%        &&  toffoli (;) (assocr* (;) swap* (;) toffoli (;) swap* (;) assocl*) (;)
%        &&  toffoli (;) (cnot (*) id))
%     
% c_R &:& bool <-> bool
% c_R &=& trace* cInner

The above example should convince the reader that the language {{langRev}}
with multiplicative trace is expressive enough to model finite relational
programming. We will not prove this claim. We develop an even more
substantial example: a SAT solver.

%%%%%%%%%%%%%%%
\subsection{Solving Constraints}
\label{ch3:sec:constraints}

A large class of constraint satisfaction problems can be expressed
using multiplicative traces. We illustrate the main ideas with the
implementation of a SAT solver.

In the usual setting, an instance of SAT is a function~{{f}} which,
when given some boolean inputs, returns {{true}} or {{false}}. The
function returns {{true}} when the inputs satisfy the constraints
imposed by the structure of {{f}} and a solution to the SAT problem is
the set of all inputs on which {{f}} produces {{true}}. The basic idea
of our construction is to generalize the annihilation circuit from
Ex.~\ref{ch3:ex;annihilation} to only annihilate values that fail to
satisfy the constraints represented by the SAT instance {{f}}. To
achieve this goal, we must however deal with several important
details.

First, because we are in a reversible world, our instance of SAT must be
expressed as an isomorphism: this is easily achieved by the construction
which embeds any boolean function {{f}} into a reversible one {{iso_f}} with
a larger domain and range.  Given such a reversible function {{iso_f}} which
represents a SAT instance, we first construct the circuit below:

\begin{center}
\scalebox{1.2}{
  \includegraphics{diagrams/sat2.pdf}
}
\end{center}  

As shown in the circuit, the reversible SAT instance {{iso_f}} takes
two sets of values and produces two outputs. The incoming values
labeled \textsf{inputs} are the inputs we need to test for
satisfiability. The other incoming values labeled \textsf{heap} are
the additional inputs needed to embed the original SAT instance {{f}}
into a reversible function. If these \textsf{heap} values are all
initialized to {{false}}, the output wire \textsf{satisfied?}
corresponds to the output that {{f}} would have produced on
\textsf{inputs}. The other outputs labeled \textsf{garbage} are not
needed for their own sake but they are important because they are used
as inputs to the adjoint of {{iso_f}} to reproduce the inputs exactly,
in anticipation of closing the loop with {{trace*}}.

To summarize, the top half of the circuit is the identity function
except that we have also managed to produce a boolean wire labeled
\textsf{satisfied?} that tells us if the inputs satisfy the desired
constraints. We can take this boolean value and use it to decide
whether to negate the bottom wire (labeled \textsf{control
  wire}). Specifically, if the inputs do \emph{not} satisfy {{f}}, the
control wire is negated. The last wire labeled \textsf{heap control
  wire} is negated if the heap values do not have the right initial
values, i.e., are not all {{false}}.

Let us call the above construction {{sat_f}}. If we now close the loop
using {{trace*}}, two things should happen:
\begin{itemize}
\item configurations in which the \textsf{heap} values are not all
  {{false}} will be annihilated;
\item configurations in which the \textsf{inputs} do not satisfy {{f}}
  will cause the \textsf{satisfied?} wire to be negated and hence will
  also be annihilated.
\end{itemize}
In other words, the only configurations that will survive are the ones in
which the \textsf{inputs} satisfy {{f}}. We simply need to arrange to
\emph{clone} these values and produce them as the output of the whole
circuit. The final construction is therefore:

\begin{center}
\scalebox{1.5}{
  \includegraphics{diagrams/sat3.pdf}
}
\end{center}  

To make the previous discussion concrete, we present a small, but
complete, example. In our example, the SAT instance {{f}} is tiny: it
takes two inputs. This function is embedded into a reversible function
{{iso_f}} of type 
{{((bool * bool) * bool) <-> ((bool * bool) * bool)}} where the last
input represents the heap and the first two outputs represent the garbage. 
The realization of {{sat_f}} given below is parametrized by such 
a function {{iso_f}}. The inputs to {{sat_f}} are 
\textsf{heap control}, \textsf{control}, \textsf{heap}, \textsf{input-1}, and 
\textsf{input-2}. Its operation is simple: if the \textsf{heap} is {{true}}, 
\textsf{heap control} is negated, and if the last output of {{iso_f}} 
is {{false}}, \textsf{control} is negated:

%subcode{opsem} include main
%! columnStyle = rcl
% sat_f &:& ((((bool * bool) * bool) * bool) * bool) <-> ((((bool * bool) * bool) * bool) * bool)
% sat_f &=& ((swap* (*) id) (*) id) (;) 
% && ((assocl* (*) id) (*) id) (;) 
% &&   (((cnot (*) id) (*) id) (*) id) (;) 
% &&   assocr* (;) 
% &&   (assocr* (*) id) (;) 
% &&   (swap* (*) id) (;) 
% &&   assocr* (;) 
% &&   (id (*) assocl*) (;) 
% &&   (id (*) isof) (;) 
% &&   swap* (;) 
% &&   assocr* (;) 
% &&   (id (*) (id (*) swap*)) (;) 
% &&   (id (*) assocl*) (;) 
% &&   (id (*) ((inot (*) id) (*) id)) (;)
% &&   (id (*) (cnot (*) id)) (;) 
% &&   (id (*) ((inot (*) id) (*) id)) (;)
% &&   (id (*) assocr*) (;) 
% &&   (id (*) (id (*) swap*)) (;) 
% &&   assocl* (;)
% &&   swap* (;)
% &&   (id (*) Sym isof) (;) 
% &&   (id (*) assocr*) (;) 
% &&   assocl* (;) 
% &&   assocl* 

Given the construction of {{sat_f}} we can build the full solver as
follows. The overall input is the cloning heap. The combinator given
to {{trace*}} takes the cloning heap and the inputs flowing around the
loop and produces two copies of these inputs. One copy is produced as
the overall output and another is fed back around the loop. 

%subcode{opsem} include main
%! columnStyle = rcl
% solve_f &:& bool * bool <-> bool * Bool
% solve_f &=& trace* (
% && (assocr* (*) id) (;) 
% && assocr* (;)
% && (id (*) swap*) (;)
% && assocl* (;)
% && (assocr* (*) id) (;)
% && (clone2 (*) id) (;)
% && swap* (;)
% && (swap* (*) id) (;)
% && ((swap* (*) id) (*) id) (;)
% && assocl* (;)
% && assocl* (;)
% && (sat_f (*) id) (;)
% && (assocr* (*) id) (;)
% && (swap* (*) id) (;) 
% && ((id (*) swap*) (*) id) (;)
% && (assocl* (*) id) (;)
% && ((id (*) swap*) (*) id))

We can test our {{solve_f}} combinator using several SAT
instances. Here are two possible instances. The first instance is
satisfied by {{(false,false)}} and the
second is satisfied by {{(false,true)}} and {{(true,true)}}.

%subcode{opsem} include main
%! columnStyle = rcl
% iso_{f_1} &:& ((bool * bool) * bool) <-> ((bool * bool) * bool)
% iso_{f_1} &=& (assocr* (;) swap* (;) toffoli (;) swap* (;) assocl*) (;)
% && (((swap+ (*) id) (*) id) (;) toffoli (;) ((swap+ (*) id) (*) id)) (;) 
% && (id (*) swap+)
% 
% iso_{f_2} &:& ((bool * bool) * bool) <-> ((bool * bool) * bool)
% iso_{f_2} &=& toffoli

It can indeed be verified using the semantics that the {{solve_f}}
combinators instantiated with the SAT instances produce the expected
results. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

Give the denotation of a few combinators. The denotation of {{eta_{1+1} }} is
the relation:

{{ {[ (1/(), (left(), left())), (1/(), (right(),right ())) ]} }}

give some intuition; example with eta/epsilon

homotopy equivalence???

axioms for field or meadow; actually we probably a semifield

definition of logical reversibility with relations

definition of information preservation in the case of relations; fanout as an
example which we can write

explain in detail the size of 2 * 1/2; it should have exactly one element;
there are two elemens in 2 but the 1/2 identifies them somehow; be precise

we can get the empty relation (eta ; id * swap; eps) what does that mean? 

connection to vector spaces; projectors; inner product. Given dual vectors,
we can have a standalone |0><0| which is a piece of a function or a projector
if viewed by itself; we can also have |v><v'| which produces the scalars YES
or NO. We have an isomorphism between matrices and vectors by viewing 
|0><0|+|1><1| as |00>+|11>

Perhaps if restrict the ``top-level'' to non-fractional types, we can never
observe a function built from from fractionals; we must apply it and in that
case, the system overall behaves like Pi. (no empty relation for example)???
Idea suggested by Zach is to change eps to (1/a) * a * a <-> a

The code implementing the 16 relations... all 16 relations have the same
information content as () ??? So we should treat (1/bool * bool) as opaque
things; we can only extract information from them by applying them.

Include dneg, name (inv cnot) as example of programs which when fed
non-fractional values produce fractional values

Also include recipT which shows that 1/ distributes over products and tens
which shows that we can manipulate fractional types in interesting ways

Using relational semantics is good because it's clear; but it's also bad
unless we spend time to explain the operational model

Explain 'name' in detail; it IS a faithful representation of the input
relation as a value; we don't get to see the full relation though; we
interact with it by giving it inputs and observing the related ouptut come
out. If we make two copies and use each in a different context (applying each
to a different value, we can ``see'' that it's really a full relation and not
just one branch.) 

total functions have size 1; partial functions have fractional sizes!!!

denotation of type in Pi is a set; in Pi/ we get a set with an equivalence
relation (Id by default) but fractionals introduce first-class equivalence
relations that can be used to restrict the sets.

---

Having re-read Baez-Dolan, I now feel better.

On 13-02-08 08:53 AM, Amr Sabry wrote:
> I think all the observations in this email are due to the fact that we
> haven't formalized the size of fractionals using homotopy equivalence. I
> started doing this but abandoned it for now but the bottom line is that
> the size of b * 1/b is 1.

Again, agree - more deeply so now.  In more detail, my current thinking:
- v \in b has size 1.  So |b| counts how many elements it has, whenever 
b is a type of Pi.  The sum and product rules apply.
- for b * 1/b to have size 1, then 1/b must have size 1/|b| (assuming 
these are independent types, which we have been assuming all along)

Attempt #1
- there are |b| elements in 1/b, and if size is additive, ( 1/|b| = 
sum_{1/v \in 1/b} |1/v| ) should hold; since |1/v| should be constant, 
this sum is |b|*c, so c = 1/|b|^2.  Weird.

Let us look at a single 'function' in bool -o bool, namely the identity 
function.  It should be 'the same' as { (1/true, true), (1/false, false) 
}.  The cardinality of that set is (1/(2^2)*1 + 1/(2^2)*1) = 1/4 + 1/4 = 
1/2.   From weird to probably-wrong.

Attempt #2
- the elements of 1/b are not distinguishable from each other (i.e. they 
are all isomorphic).  So each element 1/v has b automorphisms, so |1/b| 
= sum_{aut classes of 1/v \in 1/b} 1/|aut 1/v| = sum_{singleton} 1/|b| = 
1/|b|.  [as per p.14 of Baez-Dolan]

The size of the set { (1/true, true), (1/false, false) } is now 1/2*1 + 
1/2*1 = 1.  So this 'set' indeed represents a single "concept".

Note that in attempt #2, we get a new phenomenon.  Consider the 
(partial) relation {(1/true, false)}, also of type bool -o bool.  It has 
size 1/2 !  So, if I have not erred somewhere, only total functions have 
size 1.  Which, I must admit, I rather like.  It's not that partial 
functions are outlawed, they just don't pull as much weight [pun intended].

So 1/b is very much like a collection of |b| constraints, all of which 
are "the same" (externally).  While they may have internal structure, it 
is not visible to any of our combinators, so up to iso, there is only a 
single element of 1/b, of size 1/|b|.

Yet another way to look at it: let's assume we are looking at FinSet_0, 
where in fact our universe U has finitely (say N) things in it.  Then 
the singleton set 1 is actually the isomorphism class of {u_1, ..., 
u_N}.  There are N of them, but they have N automorphisms in a single 
orbit, so the quotient has size 1 (i.e. |S| = N but |G| = N too, so |S 
// G| = 1 ).

So an element of 1/b is closer to an action of b on b; we call it a 
constraint, or a pattern-match.  I think thinking of it as 'actions' is 
definitely the right way to go, as we will be able to have different 
kinds of actions on a type b that just those that come from the elements.

> I don't know how much of a priority this is but if it is we should try
> to work it out in detail. The good and bad news is that we will have a
> model much richer and much more complicated than relations. My thought
> when writing sec. 3 was that we would use the simple but slightly
> inadequate model of relations (which for one thing confuses v and 1/v)
> and simply mention that the right abstraction is that of 'biproduct
> dagger compact closed categories' which presumably would include the
> richer model.

I am as convinced that the right model is 'biproduct dagger compact 
closed categories' as I am that the right model for the type level is 
'categorified field'.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{splncs03} 
\bibliography{cites}
\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\appendix
\section{Categorical Background}
\label{app:cat} 

We recall that a \emph{symmetric monoidal category} is a category together
with a bifunctor $\otimes$, a distinguished object $I$, and natural
isomorphisms $\alpha_{A,B,C} : (A \otimes B) \otimes C \rightarrow A \otimes
(B \otimes C)$, $\lambda_A : A \rightarrow I \otimes A$, and $\sigma_{A,B} :
A \otimes B \rightarrow B \otimes A$ subject to standard coherence
conditions~\cite{nla.cat-vn1051288}. Following common practice
(e.g.,~\cite{Selinger:2007:DCC:1229185.1229207}), we write $\rho_A =
\sigma_{I,A} \circ \lambda_A : A \rightarrow A \otimes I$.

A \emph{compact closed category} is a symmetric monoidal category where each
object $A$ is assigned a dual object $A^*$, together with a unit map $\eta_A
: I \rightarrow A^* \otimes A$ and a counit map $\epsilon_A : A \otimes A^*
\rightarrow I$, such that:
\[\begin{array}{rcl}
\lambda_A^{-1} \circ (\epsilon_A \otimes A) \circ \alpha_{A,A^*,A}^{-1} \circ (A \otimes \eta_A) \circ \rho_A &=& \mathit{id}_A \\
\rho_{A^*}^{-1} \circ (A^* \otimes \epsilon_A) \circ \alpha_{A^*,A,A^*} \circ (\eta_A \otimes A^*) \circ \lambda_A &=& \mathit{id}_{A^*}
\end{array}\]

A \emph{dagger category} is a category together with an involutive,
identity-on-objects, contravariant functor $\dagger$. Concretely, this means
that to every morphism $F : A \rightarrow B$, one associates a morphism
$f^{\dagger} : B \rightarrow A$, called the \emph{adjoint} of $f$, such that
for all $f : A \rightarrow B$ and $g : B \rightarrow C$, we have:
\[\begin{array}{rcl}
\mathit{id}^\dagger_A &=& \mathit{id}_A \\
(g \circ f)^\dagger &=& f^\dagger \circ g^\dagger \\
(f^\dagger)^\dagger &=& f
\end{array}\]

A \emph{dagger symmetric monoidal category} is a symmetric monoidal category
with a dagger structure such that the contravariant functor $\dagger$
coherently preserves the symmetric monoidal structure. Concretely, this
requirement means that for all $f : A \rightarrow B$ and $g : C \rightarrow
D$, we have:
\[\begin{array}{rcl}
(f \otimes g)^\dagger &=& f^\dagger \otimes g^\dagger \\
\alpha^\dagger_{A,B,C} &=& \alpha^{-1}_{A,B,C} \\
\lambda^\dagger_A &=& \lambda^{-1}_A \\
\sigma^\dagger_{A,B} &=& \sigma^{-1}{A,B}
\end{array}\]

\begin{definition}[Dagger Compact Closed Category]
\label{def:cat}
A \emph{dagger compact closed category} is a dagger symmetric monoidal
category that is also compact closed and such that for all $A$:
\[
\eta_A = \sigma_{A,A^*} \circ \epsilon^\dagger_A
\]
\end{definition}

\todo{biproducts}
