There are two ways to read types and Curry-Howard: 
\begin{enumerate}
\item One first concentrates on the \emph{types}, as propositions.  We can come up with a ``type system'' which 
reflects a logic.  Then, as a second step, choose the proofs which inhabit these types/propositions, and call these
proofs \emph{terms}.
\item Alternatively, one first concentrates on the \emph{terms}, focusing on their operational semantics as primary.
Then, as a second step, one designs a ``type system'' to classify the terms, so as to be able to prove progress and
preservations theorems, to ensure that well-typed terms ``don't go wrong''.
\end{enumerate}

Under the first interpretation, coming up with an operational interpretation of the terms can be challenging:
witness the ongoing effort to understand aspects of classical logic as a type system for a (programming) calculus.
Similarly, various modal logics as well as linear logic are still being given different interpretations as type
systems for calculi, to different degrees of effectiveness.

The second interpretation is not straightforward either.  The definition of ``doing wrong'' frequently gets 
expanded to covering effects, permissions, memory access, and so forth, which can seriously affect the decidability
properties of the type system (not to mention efficiency when decidability is not an issue).

Even in the first interpretation, the understanding is that terms represent \emph{values}, which are irreducible
terms (proof terms reduce principally via cut elimination, but there are other reduction rules for projections, etc).

We wish to shift the focus and consider \emph{equivalences} as our types.  This will mean, under the first 
interpretation, that our terms will be witnesses for these equivalences.  The underlying reason is that we wish
to examine \emph{reversible computation}.  From an information-theoretic point of view, this means that information
\emph{must be preserved}, although it can certainly be rearranged in many different ways.

In other words, we wish to regard types with shape $T_1\equiv T_2$, where $T_1$ and $T_2$ are usual value-types.
In fact, we want more: we wish to regard these as \emph{oriented}: the eventual operational interpretation
will regard equivalence witnesses as \emph{inducing} a transformation from values of types $T_1$ to values of type $T_2$.
Of course, as these are equivalences, there will always be a term which inhabits $T_2\equiv T_1$ whenever there
is one inhabiting $T_1 \equiv T_2$.  This will be clearly visible when we give rules for terms, as these will always
be symmetric (and will provably induce invertible transformations).

So we start with a type theory $\mathcal{T}$, and move to another type theory whose only shape is $T_1\equiv T_2$.
The constants of this new type theory (denoted $\mathcal{T}^{\equiv}$) will be
proofs of type equivalences of the original theory.  Note that this is a very
general set-up: given any equational theory, we can ``reify'' the axioms as
constants.  [Insert the example
of doing this for Monoid here].  If some of the constructors of $\mathcal{T}$ respect equivalences, then this induces
combinators on $\mathcal{T}^{\equiv}$; the rules of equivalences (reflexivity, transitivity, symmetry) induce more.

Note that it is very important that the theories be equational: the only thing we can talk about are total equivalences.  
One of the most natural theory to consider, at least in the usual setting of type theories $\mathcal{T}$ with 
sums and products, is that of \emph{semirings} (sometimes jokingly called \emph{rigs}), as the rules of semirings
turns out to exactly reflect the judgments of such a type theory. 

What happens in equational theories is that we declare a number of values to be \emph{equal}.  Moving to the categorical
setting, we are no longer interested in equalities, but just equivalences.  In equational theories, the equality
witnesses are not particularly interesting, as they have no computational content  -- if $a = b$, there is no way we can
distinguish $a$ and $b$, whereas if we only have $a \equiv b$, that may not be the case.  [The difference is that
between syntax and semantics, where we have equivalences on the syntax which reflect equilities in the semantics.  In
other words, plus(one,one) and two are easily distinguished, but 1+1 and 2 are not.]

Furthermore, we can use these equivalences in an effective way: we can create a (total!) function
$ev : T_1 \equiv T_2 \rightarrow T_1 \rightarrow T_2$.  In other words, given a proof of an equivalence between terms,
and a value of the first type, we can get a value of the second type.

It is important to understand that $ev$ \emph{mixes} two levels: that of (proofs of) equivalence and that of values.
$ev$ is an \emph{induced action}.  

One further remark: our language of equivalences is actually a \emph{free} theory.  There are absolutely no
equalities (or equivalences) given between its terms.  Of course, some of its terms are ``observationally equivalent'',
in the sense that $ev$ will not be able to witness any difference.  Nevertheless, we will not regard such terms as
equal.

What needs to be shown then is the relationship between the equational theory of semirings and the first-order type
theory with sums and products.

Then we can investigate how adding further axioms to our equational theory (and corresponding witnesses to our type theory)
changes things.
