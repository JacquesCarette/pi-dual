\documentclass[preprint]{sigplanconf}

%% I am getting an error about too many math packages used!!!
%% I commented the ones we don't seem to be using.

\usepackage{graphicx}
%%\usepackage{longtable}
\usepackage{comment}
\usepackage{amsmath}
%%\usepackage{mdwlist}
%%\usepackage{txfonts}
\usepackage{xspace}
%%\usepackage{amstext}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{proof}
\usepackage{multicol}
\usepackage[nodayofweek]{datetime}
\usepackage{etex}
\usepackage[all, cmtip]{xy}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{multicol}
\newcommand\hmmax{0} % default 3
\newcommand\bmmax{0} % default 4
\usepackage{bm}
\usepackage{cmll}


%% \newtheorem{theorem}{Theorem}[section]
%% \newtheorem{lemma}[theorem]{Lemma}
%% \newtheorem{proposition}[theorem]{Proposition}
%% \newtheorem{corollary}[theorem]{Corollary}

\newcommand{\xcomment}[2]{\textbf{#1:~\textsl{#2}}}
\newcommand{\amr}[1]{\xcomment{Amr}{#1}}
\newcommand{\roshan}[1]{\xcomment{Roshan}{#1}}

\newcommand{\ie}{\textit{i.e.}\xspace}
\newcommand{\eg}{\textit{e.g.}\xspace}

\newcommand{\lcal}{\ensuremath{\lambda}-calculus\xspace}
\newcommand{\G}{\ensuremath{\mathcal{G}}\xspace}

\newcommand{\code}[1]{\lstinline[basicstyle=\small]{#1}\xspace}
\newcommand{\name}[1]{\code{#1}}

\def\newblock{}

\newenvironment{floatrule}
    {\hrule width \hsize height .33pt \vspace{.5pc}}
    {\par\addvspace{.5pc}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newenvironment{proof}[1][Proof.]{\begin{trivlist}\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\arrow}[1]{\mathtt{#1}}

%subcode-inline{bnf-inline} name langRev
%! swap+ = \mathit{swap}^+
%! swap* = \mathit{swap}^*
%! dagger =  ^{\dagger}
%! assocl+ = \mathit{assocl}^+
%! assocr+ = \mathit{assocr}^+
%! assocl* = \mathit{assocl}^*
%! assocr* = \mathit{assocr}^*
%! identr* = \mathit{uniti}
%! identl* = \mathit{unite}
%! dist = \mathit{distrib}
%! factor = \mathit{factor}
%! eta = \eta
%! eps = \epsilon
%! eta+ = \eta^+
%! eps+ = \epsilon^+
%! eta* = \eta^{\times}
%! eps* = \epsilon^{\times}
%! trace+ = trace^+
%! trace* = trace^{\times}
%! ^^^ = ^{-1}
%! (o) = \fatsemi
%! (;) = \fatsemi
%! (*) = \times
%! (+) = +
%! LeftP = L^+
%! RightP = R^+
%! LeftT = L^{\times}
%! RightT = R^{\times}
%! alpha = \alpha
%! bool = \textit{bool}
%! color = \textit{color}
%! Gr = G

%subcode-inline{bnf-inline} regex \{\{(((\}[^\}])|[^\}])*)\}\} name main include langRev
%! Gx = \Gamma^{\times}
%! G = \Gamma
%! [] = \Box
%! |-->* = \mapsto^{*}
%! |-->> = \mapsto_{\ggg}
%! |--> = \mapsto
%! <--| = \mapsfrom
%! |- = \vdash
%! <><> = \approx
%! ==> = \Longrightarrow
%! <== = \Longleftarrow
%! <=> = \Longleftrightarrow
%! <-> = \leftrightarrow
%! ~> = \leadsto
%! -o+ = \multimap^{+}
%! -o* = \multimap^{\times}
%! -o = \multimap
%! ::= = &::=&
%! /= = \neq
%! @@ = \mu
%! forall = \forall
%! exists = \exists
%! empty = \epsilon
%! langRev = \Pi
%! langRevT = \Pi^{o}
%! langRevEE = \Pi^{\eta\epsilon}
%! theseus = Theseus
%! sqrt(x) = \sqrt{#x}
%! surd(p,x) = \sqrt[#p]{#x}
%! inv(x) = \frac{1}{#x}
%! frac(x,y) = \frac{#x}{#y}
%! * = \times

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\conferenceinfo{ICFP'12}{}
\CopyrightYear{}
\copyrightdata{}
\titlebanner{}
\preprintfooter{}

\title{The Two Dualities of Computation: Negative and Fractional Types}
\authorinfo{Roshan P. James}
           {Indiana University}
           {rpjames@indiana.edu}
\authorinfo{Amr Sabry}
           {Indiana University}
           {sabry@indiana.edu}
\maketitle

\begin{abstract}
  Every functional programmer knows about sum and product types, {{a+b}} and
  {{a*b}} respectively. Negative and fractional types, {{a-b}} and {{a/b}}
  respectively, are much less known and their computational interpretation is
  unfamiliar and often complicated. We show that in a programming model in
  which information is preserved (such as the model introduced in our recent
  paper on \emph{Information Effects}), these types have particularly natural
  computational interpretations. Intuitively, values of negative types are
  values that flow ``backwards'' to satisfy demands and values of fractional
  types are values that impose constraints on their context.  The combination
  of these negative and fractional types enables greater flexibility in
  programming by breaking global invariants into local ones that can be
  autonomously satisfied by a subcomputation. Theoretically, these types give
  rise to \emph{two} function spaces and to \emph{two} notions of
  continuations, suggesting that the previously observed duality of
  computation conflated two orthogonal notions: an additive duality that
  corresponds to backtracking and a multiplicative duality that corresponds
  to constraint propagation.
\end{abstract}

\category{D.3.1}{Formal Definitions and Theory}{}
\category{F.3.2}{Semantics of Programming Languages}{}
\category{F.3.3}{Studies of Program Constructs}{Type structure}

\terms
Languages, Theory

\keywords continuations, information flow, linear logic, logic programming,
quantum computing, reversible logic, symmetric monoidal categories, compact
closed categories.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

In a recent paper~\cite{infeffects}, we argued that, because they include
irreversible physical primitives, conventional abstract models of computation
have inadvertently included some \emph{implicit} computational effects which
we called \emph{information effects}. We then developed a pure reversible
model of computation that is obtained from the type isomorphisms and
categorical structures that underlie models of linear logic and quantum
computing and that treats information as a linear resource that can neither
be erased nor duplicated. In this paper, we show that our pure reversible
model unveils deeper and more elegant symmetries of computation than have
previously been reported. In particular, we expose two notions of duality of
computation: an additive duality and a multiplicative duality that give rise
to negative types and fractional types respectively. Although these types
have previously appeared in the literature (see Sec.~\ref{sec:related}), they
have typically appeared in the context of conventional languages with
information effects, which obscured their appeal and murked their properties.

\paragraph*{Negative Types.} 
Consider the following algebraic manipulation relating a natural number {{a}}
to itself (ignoring the dotted line for a moment):
\begin{center}
\scalebox{1.1}{
\includegraphics{diagrams/thesis/algebra-wire1.pdf}
}
\end{center}
Although seemingly pointless, this algebraic proof corresponds, in our model,
to an isomorphism of type {{a <-> a}} with a non-trivial and interesting
computational interpretation. The witness for this isomorphism is a
computation that takes a value of type~{{a}}, say \$20.00, and eventually
produces another \$20.00 value as its output. As the semantics of
Sec.~\ref{sec:rat} formalizes, this computation flows along the dotted line
with the following intermediate steps:
\begin{itemize}
\item We start at line (0) with \$20.00; 
\item We proceed to line (1) with the same \$20.00 but tagged as being
  in the left summand of the sum type {{a+0}}; we indicate this value
  as {{left 20}};
\item We continue to line (2) with the same value {{left 20}};
\item At line (3), as a result of re-association the tag on the \$20.00
  changes to indicate that it is in the left-left summand, i.e., the value is
  now {{left (left 20)}};
\item At line (4), we find ourselves needing to produce a value of type 0
  which is impossible; this signals the beginning of a reverse execution
  which sends us back to line (3) with a value {{left (right 20)}};
\item Execution continues in reverse to line (2) with the value 
  {{right (left 20)}};
\item At line (1) we find ourselves again facing an empty type so we reverse
  execution again; we go to line (2) with a value {{right (right 20)}};
\item We proceed to line (3) and (4) with the value {{right 20}};
\item We finally reach line 5 with the value {{20}}.
\end{itemize}
\roshan{Why was this deleted?}
\amr{because it says nothing new}
The example illustrates that the empty type and negative types have a
computational interpretation related to continuations: negative types denote
values that backtrack to satisfy dependencies, or in other words act as debts
that are satisfied by the backward flow of information.

\paragraph*{Fractional Types.} 
Consider a similar algebraic manipulation involving fractional types.
\begin{center}
\scalebox{1.1}{
\includegraphics{diagrams/thesis/algebra-wire2.pdf}
}
\end{center}
In the case of negatives, the dotted line indicated the flow of control
whereas for fractionals it indicates the flow of constraints. At the heart of
logic programming is the idea of variables that capture constraints. Hence it
is useful to trace the computation corresponding to the algebraic proof
above, with the analogy to logic variables in mind.

As before, the execution begins at line~(0) with the value {{20}}. At
line~(1) two values, {{20}} and {{()}}, flow forward. One can think of the
value {{()}} (of type {{1}}) as ``having a credit card.'' The credit card
isn't money, nor is it debt, but is the option to generate a credit-debt
constraint.  At line~(2) we exercise this option and hence have three values:
the initial value {{20}} flowing from line (1) and two entangled values,
{{1/alpha}} and {{alpha}}. The {{alpha}} and {{1/alpha}} are unspecified
values, i.e., we don't yet know how much money we need to borrow, but we do
know that what is borrowed must be what is returned. Hence~{{alpha}} denotes
the presence of an unknown quantity and dually {{1/alpha}} should be thought
of as the absence of an unknown quantity.

% Unlike with negative types, wherein only one value existed at a time and the
% computation backtracked, here we have three values that \emph{exist at the
%   same time}. In other words, the computation with fractions is realized with
% a schedule in which every value independently and concurrently proceeds
% through its subcomputation.

At line (3), the missing unknown {{1/alpha}} is fixed to be {{20}}, thereby
fixing values of all three branches, i.e., we have paid our debt of the missing
information and the third subcomputation has the known value {{20}}. After
payment of our debt, as line (4) shows, we have use of our credit card again.

\roshan{Also, why was this deleted?}
\amr{no real info is communicated; you're saying that pairs are different 
than sums}
Unlike with negative types, wherein only one value existed at a time and the
computation backtracked, here we have three values that \emph{exist at the
  same time}. In other words, the computation with fractions is realized with
a schedule in which every value independently and concurrently proceeds
through its subcomputation.

%% At some intuitive level, one might understand the situation as
%% modeling a credit card payment in which money is created at line (2)
%% for the consumer together with a corresponding debt. The producer's
%% bank pays for the debt at line (3) and waits for the actual money to
%% arrive from the producer.

The example illustrates that fractional types also have a computational
interpretation that have some flavor of continuations: the fractional types
denote values ({{1/alpha}}) that represent missing information that must be
supplied in much the same sense that continuations denote evaluation contexts
with holes that must be filled.  There are at least three fundamental points
about the examples above that must be emphasized:

\begin{itemize}
\item 
As the example illustrates, both negative types and fractional types
corresponds to ``debts'' but in different ways: negatives are satisfied
by backtracking and fractionals are satisfied by constraint
propagation.

\item 
It would clearly be disastrous if debts could be deleted or
duplicated. This simple observation explains why these types are much
simpler and much more appealing in a framework where information is
guaranteed to be preserved. In previous work that used negative types
(see Sec.~\ref{sec:related}), complicated mechanisms are typically
needed to constrain the propagation and use of negative values because
the surrounding computational framework is, generally speaking,
careless in its treatment of information.

\item The main reason credit card transactions are convenient is because they
  disentangle the propagation of the resources (money) from the propagation
  of the services. Not every transaction needs both the resources and
  services to be brought together: it is sufficient to have a promise that
  the demand for resources will be somehow satisfied, as long as the
  infrastructure can be trusted with such promises. This idea that
  dependencies can be freely decoupled and propagated can be a powerful
  programming tool and we leverage this in the construction of a novel
  SAT-solver (see Sec.~\ref{sec:sat-solver}).

%% Computationally, this means that negative types allow us
%%   to break the dependency between producers and consumers and let each
%%   compute independently.

%% \item Consider a value of type {{-a + b}}. The above discussion suggests that
%%   this value demands {{a}} and produces {{b}}, i.e., that it is a
%%   higher-order funtion mapping {{a}} to {{b}}. Dually and as the underlying
%%   categorical semantics justifies (see Sec.~\ref{sec:neg}), the same value is
%%   a continuation which produces {{b}} and then demands {{a}}, which
%%   introduces a twist on how the function can be used: get the goods {{b}} now
%%   and pay {{a}} later.
\end{itemize}

\paragraph*{Functions and Dependency Splitting.} 
%% Consider the following realization of boolean negation written using a
%% pattern-matching style:
%% \begin{verbatim}
%% notB b = case b of 
%%   False -> True
%%   True -> False
%% \end{verbatim}
%% In a functional language, we are not used to treating the pattern-matching
%% clauses \verb|False -> True| and \verb|True -> False| as first-class entities
%% but fractional types allow us to do just that! Informally, we can treat each
%% of the pattern-matching clauses as a first-class value of type 
%% {{(1/bool) * bool}}. 
%% A small but complete example will illustrate the main aspects of this idea.

%% Let \verb|color| be the datatype with three constructors \verb|R|, \verb|G|,
%% and \verb|B| and let's try to construct a function that rotates the
%% colors. In other words, we would like to build the following three
%% pattern-matching clauses \verb|R -> G|, \verb|G -> B|, and \verb|B -> R|.  

The pair {{(1/a) * a}} corresponds to a function type written {{a-o*a}})
whose witness maybe thought of as the identity function at the type {{a}}.
For example, consider a datatype {{color = R|Gr|B}}, and let us consider the
following manipulations:

%% for the
%% there are three possible
%% witnesses for this isomorphism mapping {{()}} to either {{(1/R,R)}} or
%% {{(1/Gr,Gr)}} or {{(1/B,B)}} with the particular witness determined by
%% further constraints encountered during the computation. More generally, the
%% witness for the isomorphism above may be represented as {{(1/alpha,alpha)}}
%% where~{{alpha}} is a fresh logic variable of type {{a}}. With this view of
%% {{a -o* a}} functions, 
%% 
%% Returning to our goal of creating the function that rotates the colors, we
%% proceed as follows:

\begin{itemize}
\item Using the fact that {{1}} is the multiplicative unit, generate
  from the input {{()}} the value {{((),())}} of type {{1 * 1}};
\item Apply the isomorphism {{1 <-> (1/a) * a}} in parallel to each of
  the components of the above tuple. The resulting value is
  {{((1/alpha1,alpha1),(1/alpha2,alpha2))}} where {{alpha1}} and
  {{alpha2}} are fresh logic variables;
\item Using the fact that {{*}} is associative and commutative, we can
  rearrange the above tuple to produce the value:

{{((1/alpha1,alpha2),(1/alpha2,alpha1))}}.


%% \begin{itemize}
%% \item Using the fact that {{1}} is the multiplicative unit, generate from the
%%   input {{()}} the value {{((),((),()))}} of type {{1 * (1 * 1)}};
%% \item Apply the isomorphism {{1 <-> (1/a) * a}} in parallel to each of the
%%   components of the above tuple. The resulting value is 
%% {{((1/alpha1,alpha1),((1/alpha2,alpha2),(1/alpha3,alpha3)))}} where {{alpha1}},
%% {{alpha2}}, and {{alpha3}} are fresh logic variables.
%% \item Using the fact that {{*}} is associative and commutative, we can
%%   rearrange the above tuple to produce the value:

%% {{((1/alpha1,alpha2),((1/alpha2,alpha3),(1/alpha3,alpha1)))}}.

%% \item At this point, we have produced the skeleton for our desired function.
%%   Assume now that a client uses the first clause of the function in a context
%%   that contains the value \verb|R|, i.e., the client has the value
%%   {{((1/alpha1,alpha2),R)}} consiting of one of the clauses of the function
%%   and an argument. This value can be rearranged to {{((1/alpha1,R),alpha2)}}.
%%   At this point, we can use the isomorphism {{1 <-> (1/a) * a}} in reverse on
%%   the first component of the pair which has the following effects:
%%   \begin{itemize}
%%     \item the logic variable {{alpha1}} is unified with {{R}};
%%     \item the first component of the pair then reduces to () and can be
%%       absorbed by the fact that {{1}} is the multiplicative unit. In other
%%       words, the value reduces to simply {{alpha2}} reflecting that the
%%       pattern-matching was successful and the result of the clause is now
%%       available;
%%     \item the first clause of the function has now disappeared and the
%%       remaining two clauses become {{((1/alpha2,alpha3),(1/alpha3,R))}}.
%%   \item The same client or another client can now use the second clause
%%     applying it to {{Gr}} which would produce the result {{alpha3}} for the
%%     second client and ground the result of the first client to {{Gr}}.
%%   \item Finally the remaining clause {{(1/alpha3,R)}} can be applied to {{B}}
%%     which grounds the result of the second client to {{B}}.
%%   \end{itemize}
\end{itemize}

At this point we have constructed a strange mix of two {{a-o*a}} functions;
inputs of one function manifest themselves as outputs of the other. If
{{(1/alpha1,alpha2)}} is held by one subcomputation and {{(1/alpha2,alpha1)}}
is held by another subcomputation, these remixed functions form a
communication channel between the two concurrent subcomputations. Unifying
{{1/alpha1}} with {{color}} {{R}} in one subcomputation, fixes {{alpha1}} to
be {{R}} in the other. The type {{a}} thus takes the role of the type of the
communication channel, indicating how much information can be communicated
between the two subcomputations.  Depending on the choice of the type {{a}},
an arbitrary number of bits may be communicated.

Dually, the additive reading of the above manipulations correspond to
functions of the form {{a-o+a}}, witnessing isomorphisms of the form
{{0<->(-a)+a}}). The remixed additive functions express control flow transfer
between two subcomputations, \emph{only one of which exists} at any point,
i.e., they capture the essence of coroutines. Additive and multiplicative
functions are further discussed in Sec.~\ref{sec:rat}.  It should be evident
that in a universe in which information is not guaranteed to be preserved by
the computational infrastructure, the above slicing and dicing of functions
would make no sense. Once the computational framework guarantees that
information is maintained, it becomes possible to break dependencies,
generate debts and constraints, and let these flow as first-class values,
because they \emph{must} eventually be satisfied. 

%% If one branch uses a clause that matches one part of the input to
%% produce one part of the output, all other clauses are automatically
%% adjusted to account for this to ensure that this input is not matched
%% again and that this output is not produced again.


%% \item At this point, we have produced the skeleton for our desired function.
%%   Assume now that a client uses the first clause of the function in a context
%%   that contains the value \verb|R|, i.e., the client has the value
%%   {{((1/alpha1,alpha2),R)}} consiting of one of the clauses of the function
%%   and an argument. This value can be rearranged to {{((1/alpha1,R),alpha2)}}.
%%   At this point, we can use the isomorphism {{1 <-> (1/a) * a}} in reverse on
%%   the first component of the pair which has the following effects:
%%   \begin{itemize}
%%     \item the logic variable {{alpha1}} is unified with {{R}};
%%     \item the first component of the pair then reduces to () and can be
%%       absorbed by the fact that {{1}} is the multiplicative unit. In other
%%       words, the value reduces to simply {{alpha2}} reflecting that the
%%       pattern-matching was successful and the result of the clause is now
%%       available;
%%     \item the first clause of the function has now disappeared and the
%%       remaining two clauses become {{((1/alpha2,alpha3),(1/alpha3,R))}}.
%%   \item The same client or another client can now use the second clause
%%     applying it to {{Gr}} which would produce the result {{alpha3}} for the
%%     second client and ground the result of the first client to {{Gr}}.
%%   \item Finally the remaining clause {{(1/alpha3,R)}} can be applied to {{B}}
%%     which grounds the result of the second client to {{B}}.
%%   \end{itemize}



%% Furthermore, just like the case for negative types, a dual reading of
%% the pattern-matching clauses as continuations allows the context to
%% use the result of the clause and provide the matching argument later.

\paragraph*{Contributions and Outline.} 
To summarize, in a computational framework that guarantees that information
is preserved, negative and fractional types provide fascinating mechanisms in
which computations can be sliced and diced, decomposed and recomposed, run
forwards and backwards, in arbitrary ways. The remainder of the paper
formalizes these informal observations. Specifically our main contributions
are:
\begin{itemize}
\item We extend {{langRev}} our reversible programming language of type
  isomorphisms~\cite{rc2011,infeffects} (reviewed in Sec.~\ref{sec:pi}) with a notion
  of negative types, that satisfies the isomorphism {{a + (-a) <-> 0}}. The
  semantics of this extension is expressed by having a \emph{dual} evaluator
  that reverses the flow of execution for negative
  values. (Sec.~\ref{sec:neg})
\item We independently extend {{langRev}} with a notion of fractional types,
  that satisfies the isomorphism {{a * (1/a) <-> 1}}. The semantics of this
  extension is expressed by introducing logic variables and a unification
  mechanism to model and resolve the constraints introduced by the fractional
  types. (Sec.~\ref{sec:frac})
\item We combine the above two extensions into one extension that includes
  both negative and fractional types. The resulting language allows any
  rational number to be used as a type. The types satisfy the same familiar
  and intuitive isomorphisms that are satisfied in the mathematical field of
  rational numbers. (Sec.~\ref{sec:rat})
\item We develop programming intuition and argue that negative and fractional
  types ought to be part of the vocabulary of every
  programmer. (Sec.~\ref{sec:prog})
\item We relate our notions of negative and fractional types to previous work
  on continuations. Briefly, we argue that conventional continuations
  conflate negative and fractional components. This observation allows us to
  relate two apparently unrelated lines of work: the first pioneered by
  Filinski~\cite{Filinski:1989:DCI:648332.755574} relating continuations to
  negative types and the second~\cite{Bernardi:2010:CSL:1749618.1749689}
  relating continuations to the fractional types of the Lambek-Grishin
  calculus. (Sec.~\ref{sec:related})
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Core Reversible Language: {{langRev}} }
\label{sec:pi}

We review our reversible language {{langRev}}: the presentation in this
section differs from the one in our previous paper~\cite{infeffects} in two
aspects. First, we add the empty type {{0}} which is necessary to express the
additive duality. Second, instead of explaining evaluation using a natural
semantics, we give a small-step operational semantics that is more
appropriate for the connections with continuations explored in this paper.

%%%%%%%%%%%%%%%%%%%%
\subsection{Syntax and Types} 
\label{sec:pi-syntax}

The set of types includes the empty type {{0}}, the unit type {{1}}, sum
types {{b1+b2}}, and products types {{b1*b2}}. The set of values {{v}}
includes {{()}} which is the only value of type {{1}}, {{left v}} and {{right v}} 
which inject {{v}} into a sum type, and {{(v1,v2)}} which builds a
value of product type. There are no values of type {{0}}:
%subcode{bnf} include main
% value types, b ::= 0 | 1 | b + b | b * b 
% values, v ::= () | left v | right v | (v, v)

The combinators of {{langRev}} are witnesses to the following type
isomorphisms: 
%subcode{bnf} include main
%! columnStyle = r@{\hspace{-0.5pt}}c@{\hspace{-0.5pt}}l
%zeroe :&  0 + b <-> b &: zeroi
%swap+ :&  b1 + b2 <-> b2 + b1 &: swap+
%assocl+ :&  b1 + (b2 + b3) <-> (b1 + b2) + b3 &: assocr+
%identl* :&  1 * b <-> b &: identr*
%swap* :&  b1 * b2 <-> b2 * b1 &: swap*
%assocl* :&  b1 * (b2 * b3) <-> (b1 * b2) * b3 &: assocr*
%dist0 :& 0 * b <-> 0 &: factor0
%dist :&~ (b1 + b2) * b3 <-> (b1 * b3) + (b2 * b3)~ &: factor
Each line of the above table introduces one or two combinators that witness
the isomorphism in the middle. Collectively the isomorphisms state that the
structure {{(b,+,0,*,1)}} is a \emph{commutative semiring}, i.e., that each
of {{(b,+,0)}} and {{(b,*,1)}} is a commutative monoid and that
multiplication distributes over addition. The isomorphisms are extended to
form a congruence relation by adding the following constructors that witness
equivalence and compatible closure:
%subcode{proof} include main
%@  ~
%@@ id : b <-> b 
%
%@ c : b1 <-> b2
%@@ sym c : b2 <-> b1
%
%@ c1 : b1 <-> b2
%@ c2 : b2 <-> b3
%@@ c1(;)c2 : b1 <-> b3
%---
%@ c1 : b1 <-> b3
%@ c2 : b2 <-> b4
%@@ c1 (+) c2 : b1 + b2 <-> b3 + b4
%
%@ c1 : b1 <-> b3
%@ c2 : b2 <-> b4
%@@ c1 (*) c2 : b1 * b2 <-> b3 * b4

To summarize, the syntax of {{langRev}} is given as follows. 

\begin{definition}{(Syntax of {{langRev}})}
\label{def:langRev}
We collect our types, values, and combinators, to get the full language
definition.
%subcode{bnf} include main
% value types, b ::= 0 | 1 | b+b | b*b 
% values, v ::= () | left v | right v | (v,v) 
%
% comb.~types, t ::= b <-> b
% iso ::= zeroe | zeroi 
%     &|& swap+ | assocl+ | assocr+ 
%     &|& identl* | identr* 
%     &|& swap* | assocl* | assocr* 
%     &|& dist0 | factor0 | dist | factor 
% comb., c ::= iso | id | sym c | c (;) c | c (+) c | c (*) c 
\end{definition}

%%%%%%%%%%%%%%%
\subsection{Graphical Language}

The syntactic notation above is often obscure and hard to read.  Following
the tradition established for monoidal
categories~\cite{springerlink:10.1007/978-3-642-12821-94}, we present a
graphical language that conveys the intuitive semantics of the language
(which is formalized in the next section).

The general idea of the graphical notation is that combinators are modeled by
``wiring diagrams'' or ``circuits'' and that values are modeled as
``particles'' that may appear on the wires. Evaluation therefore is modeled
by the flow of particles along the wires.

%% In fact, when talking about {{langRev}} circuits, we will often use the
%% words {{value}} and {{particle}} interchangeably.

\begin{itemize}
\item The simplest sort of diagram is the {{id : b <-> b}} combinator which
  is simply represented as a wire labeled by its type {{b}}, as shown on the
  left. In more complex diagrams, if the type of a wire is obvious from the
  context, it may be omitted. When tracing a computation, one might imagine a
  value {{v}} of type {{b}} on the wire, as shown on the right.

  \begin{multicols}{2}
\begin{center}
\scalebox{0.95}{
%%subcode-line{pdfimage}[diagrams/thesis/b-wire.pdf]
\includegraphics{diagrams/thesis/b-wire.pdf}
}
\end{center}
\begin{center}
\scalebox{0.95}{
%%subcode-line{pdfimage}[diagrams/thesis/b-wire.pdf]
\includegraphics{diagrams/thesis/b-wire-value.pdf}
}
\end{center}
  \end{multicols}

\item The product type {{b1*b2}} may be represented using either one wire
  labeled {{b1*b2}} or two parallel wires labeled {{b1}} and {{b2}}. In the
  case of products represented by a pair of wires, when tracing execution
  using particles, one should think of one value/particle on each wire.
\begin{multicols}{2}
\begin{center}
\scalebox{0.95}{
%%subcode-line{pdfimage}[diagrams/thesis/pair-one-wire.pdf]
\includegraphics{diagrams/thesis/product-one-wire.pdf}
}
\end{center}
\begin{center}
\scalebox{0.95}{
\includegraphics{diagrams/thesis/product-one-wire-value.pdf}
}
\end{center}
\end{multicols}
\begin{multicols}{2}
\begin{center}
\scalebox{0.95}{
%%%subcode-line{pdfimage}[diagrams/thesis/pair-of-wires.pdf]
\includegraphics{diagrams/thesis/product-two-wires.pdf}
}
\end{center}
\begin{center}
\scalebox{0.95}{
\includegraphics{diagrams/thesis/product-two-wires-value.pdf}
}
\end{center}
\end{multicols}

\item Sum types may similarly be represented by one wire or using using
  parallel wires with a {{+}} operator between them. When tracing the
  execution of two additive wires, a value can reside on only one of the two
  wires.
\begin{multicols}{2}
\begin{center}
\scalebox{0.95}{
%%subcode-line{pdfimage}[diagrams/thesis/sum-one-wire.pdf]
\includegraphics{diagrams/thesis/sum-one-wire.pdf}
}
\end{center}
\begin{center}
\scalebox{0.95}{
\includegraphics{diagrams/thesis/sum-two-wires-left-value.pdf}
}
\end{center}
\end{multicols}
\begin{multicols}{2}
\begin{center}
\scalebox{0.95}{
%%subcode-line{pdfimage}[diagrams/thesis/sum-of-wires.pdf]
\includegraphics{diagrams/thesis/sum-two-wires.pdf}
}
\end{center}
\begin{center}
\scalebox{0.95}{
\includegraphics{diagrams/thesis/sum-two-wires-right-value.pdf}
}
\end{center}
\end{multicols}


%% \item
%% When representing complex types like {{(b1*b2)+b3}} some visual
%% grouping of the wires may be done to aid readability. The exact type
%% however will always be clarified by the context of the diagram.

%% \begin{center}
%% \scalebox{0.95}{
%% %subcode-line{pdfimage}[diagrams/thesis/complex-type-crop.pdf]
%% }
%% \end{center}

\item Associativity is implicit in the graphical language. Three parallel
  wires represent {{b1*(b2*b3)}} or {{(b1*b2)*b3}}, based on the context.
\begin{center}
\scalebox{0.95}{
%%subcode-line{pdfimage}[diagrams/thesis/associate.pdf]
\includegraphics{diagrams/thesis/assoc.pdf}
}
\end{center}

\item Commutativity is represented by crisscrossing wires.
\begin{multicols}{2}
\begin{center}
\scalebox{0.95}{
%%subcode-line{pdfimage}[diagrams/thesis/swap-pair.pdf]
\includegraphics{diagrams/thesis/swap_times.pdf}
}
\end{center}
\begin{center}
\scalebox{0.95}{
%%subcode-line{pdfimage}[diagrams/thesis/swap-sum.pdf]
\includegraphics{diagrams/thesis/swap_plus.pdf}
}
\end{center}
\end{multicols}

\item The morphisms that witness that {{0}} and {{1}} are the additive and
  multiplicative units are represented as shown below. Note that since there
  is no value of type 0, there can be no particle on a wire of type {{0}}.
  Also since the monoidal units can be freely introduced and eliminated, in
  many diagrams they are omitted and dealt with explicitly only when they are
  of special interest.
\begin{multicols}{2}
\begin{center}
\scalebox{0.95}{
%%subcode-line{pdfimage}[diagrams/thesis/identr1.pdf]
\includegraphics{diagrams/thesis/uniti.pdf}
}
\end{center}
\begin{center}
\scalebox{0.95}{
%%subcode-line{pdfimage}[diagrams/thesis/identl1.pdf]
\includegraphics{diagrams/thesis/unite.pdf}
}
\end{center}  
\end{multicols}
\begin{multicols}{2}
\begin{center}
\scalebox{0.95}{
%%subcode-line{pdfimage}[diagrams/thesis/identr0.pdf]
\includegraphics{diagrams/thesis/zeroi.pdf}
}
\end{center}
\columnbreak
\begin{center}
\scalebox{0.95}{
%%subcode-line{pdfimage}[diagrams/thesis/identl0.pdf]
\includegraphics{diagrams/thesis/zeroe.pdf}
}
\end{center}
\end{multicols}

\item Finally, distributivity and factoring are represented using the dual
  boxes shown below:
\begin{multicols}{2}
\begin{center}
\scalebox{0.95}{
%%subcode-line{pdfimage}[diagrams/thesis/distrib-crop.pdf]
\includegraphics{diagrams/thesis/dist.pdf}
}
\end{center}
\begin{center}
\scalebox{0.95}{
%%subcode-line{pdfimage}[diagrams/thesis/factor-crop.pdf]
\includegraphics{diagrams/thesis/factor.pdf}
}
\end{center}
\end{multicols}

\end{itemize}

\noindent 
\textit{Example.}  We use the type {{bool}} as a shorthand to denote
the type {{1+1}} and use {{left ()}} to be {{true}} and {{right ()}}
to be {{false}}. The following combinator is represented by the given
diagram:

{{c : b * bool <-> b + b}}

{{c = swap* (;) dist (;) (identl* (+) identl*)}}

\begin{center}
\scalebox{0.95}{
%%subcode-line{pdfimage}[diagrams/thesis/example1-crop.pdf]
\includegraphics{diagrams/thesis/example1.pdf}
}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Semantics}

The semantics of the primitive combinators is given by the following
single-step reductions. Since there are no values of type {{0}}, the rules
omit the impossible cases:
\begin{scriptsize}
%subcode{opsem} include main
%! columnStyle = rlcl
% swap+ & (left v) &|-->& right v
% swap+ & (right v) &|-->& left v 
% assocl+ & (left v1) &|-->& left (left v1)
% assocl+ & (right (left v2)) &|-->& left (right v2)
% assocl+ & (right (right v3)) &|-->& right v3 
% assocr+ & (left (left v1)) &|-->& left v1
% assocr+ & (left (right v2)) &|-->& right (left v2)
% assocr+ & (right v3) &|-->& right (right v3)
% identl* & ((), v) &|-->& v 
% identr* & v &|-->& ((), v) 
% swap* & (v1, v2) &|-->& (v2, v1) 
% assocl* & (v1, (v2, v3)) &|-->& ((v1, v2), v3) 
% assocr* & ((v1, v2), v3) &|-->& (v1, (v2, v3)) 
% dist & (left v1, v3) &|-->& left (v1, v3)
% dist & (right v2, v3) &|-->& right (v2, v3)
% factor & (left (v1, v3)) &|-->& (left v1, v3) 
% factor & (right (v2, v3)) &|-->& (right v2, v3)   
\end{scriptsize}
The reductions for the primitive isomorphisms above are exactly the same as
have been presented before~\cite{infeffects}. The reductions for the closure
combinators are however presented in a small-step operational style using the
following definitions of evaluation contexts and machine states:

\begin{scriptsize}
%subcode{bnf} include main
% Combinator Contexts, C = [] | Fst C c | Snd c C 
%                  &|& LeftT C c v | RightT c v C 
%                  &|& LeftP C c | RightP c C 
% Machine states = <c, v, C> | {[c, v, C]}
% Start state = <c, v, []> 
% Stop State = {[c, v, []]}
\end{scriptsize}
The machine transitions below track the flow of particles through a
circuit. The start machine state, {{<c,v,[]>}}, denotes the
particle/value~{{v}} about to be evaluated by the circuit {{c}}. The end
machine state, {{[c, v, [] ]}}, denotes the situation where the particle
{{v}} has exited the circuit {{c}}.

\begin{scriptsize}
%subcode{opsem} include main
%! columnStyle = rclr
% <iso, v, C> &|-->& {[iso, v', C]} & (1)
% & & where iso v |--> v' &
% <c1(;)c2, v, C> &|-->& <c1, v, Fst C c2> & (2) 
% {[c1, v, Fst C c2]} &|-->& <c2, v, Snd c1 C> & (3) 
% {[c2, v, Snd c1 C]} &|-->& {[ c1(;)c2, v, C ]} & (4) 
% <c1(+)c2, left v, C> &|-->& <c1, v, LeftP C c2> & (5) 
% {[ c1, v, LeftP C c2 ]} &|-->& {[c1 (+) c2, left v, C ]} & (6)
% <c1(+)c2, right v, C> &|-->& <c2, v, RightP c1 C> & (7)
% {[ c2, v, RightP c1 C ]} &|-->& {[c1 (+) c2, right v, C ]} & (8)
% <c1(*)c2, (v1, v2), C> &|-->& <c1, v1, LeftT C c2 v2> & (9)
% {[ c1, v1, LeftT C c2 v2 ]} &|-->& <c2, v2, RightT c1 v1 C> & (10) 
% {[ c2, v2, RightT c1 v1 C ]} &|-->& {[ c1 (*) c2, (v1, v2), C ]} & (11)
\end{scriptsize}
Rule (1) describes evaluation by a primitive isomorphism. Rules (2), (3) and
(4) deal with sequential evaluation. Rule (2) says that for the value {{v}}
to flow through the sequence {{c1 (;) c2}}, it should first flow through
{{c1}} with {{c2}} pending in the context ({{Fst C c2}}). Rule (3) says the
value {{v}} that exits from {{c1}} should proceed to flow
through~{{c2}}. Rule (4) says that when the value {{v}} exits {{c2}}, it also
exits the sequential composition {{c1(;)c2}}. Rules (5) to (8) deal with 
{{c1 (+) c2}} in the same way. In the case of sums, the shape of the value,
i.e., whether it is tagged with {{left}} or {{right}}, determines whether
path {{c1}} or path {{c2}} is taken. Rules (9), (10) and (11) deal with 
{{c1 (*) c2}} similarly. In the case of products the value should have the
form {{(v1, v2)}} where {{v1}} flows through {{c1}} and {{v2}} flows through
{{c2}}. Both these paths are entirely independent of each other and we could
evaluate either first, or evaluate both in parallel. In this presentation we
have chosen to follow {{c1}} first, but this choice is entirely arbitrary.

The interesting thing about the semantics is that it represents a reversible
abstract machine. In other words, we can compute the start state from the
stop state by changing the reductions {{|-->}} to run backwards
{{<--|}}. When running backwards, we use the isomorphism represented by a
combinator {{c}} in the reverse direction which we indicate by writing
{{c{dagger}}}. For example, {{distrib{dagger} = factor}} and so on.

\begin{proposition}[Logical Reversibility]
\label{prop:logrev}
{{<c,v,[]> |--> [c,v',[]]}} iff 
{{<c{dagger},v',[]> |--> [c{dagger},v,[]]}} 
\end{proposition}

% \roshan{We really have to rethink this prop in the presence of
%   negatives and fractionals. With negatives, the program can actually
%   end at the beginning of the circuit -- i.e. the program {{c:b1<->b2}}
%   can stop with a value of type {{b1}}. With fractionals, there can be
%   several values of type {{b2}} produced for a value of type {{b1}}. }

% \begin{proposition}[Groupoid]
% \label{prop:groupoid}
% {{langRev}} is a groupoid. 
% \end{proposition}

% \begin{proposition}
% \label{prop:category}
% {{langRev}} is a dagger symmetric monoidal category. 
% \end{proposition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Negative and Fractional Types}

% The structure of this section and the next are the same. 

This section introduces the syntax and graphical languages for
negative types and fractionals.The general outline is as follows:
\footnote{We refer the reader to Selinger's excellent survey
  article of monoidal
  categories~\cite{springerlink:10.1007/978-3-642-12821-94} for the precise
  definitions.}
\begin{itemize}
\item As established in our earlier work~\cite{rc2011,infeffects}, the
  underlying categorical semantics of our core reversible language
  {{langRev}} is based on \emph{two} distinct symmetric monoidal structures,
  one with {{+}} as the monoidal tensor, and one with {{*}} as the monoidal
  tensor.
\item We extend each underlying symmetric monoidal structure to a
  \emph{compact closed} structure by adding a dual for each object and two
  special morphims traditionally called {{eta}} and {{eps}}. 
\end{itemize}
After presenting the extension at the syntactic level, we discuss the
%% categorical semantics informally via the graphical language. The
%% operational semantics is presented in Sec.~\ref{sec:rat}. As will be
%% detailed in this sections \ref{sec:neg} and \ref{sec:frac}, the
%% compact closed structure provides several properties of interest: (i)
%% morphisms or wires are allowed to run from right to left, (ii) the
%% structure admits a trace that, depending on the situation, can be used
%% to implement various notions of loops and
categorical semantics informally via the graphical language. The operational
semantics is presented in Sec.~\ref{sec:rat}. 

As will be detailed in the remainder of this section, the compact closed
structure provides several properties of interest: (i) morphisms or wires are
allowed to run from right to left, (ii) the structure admits a trace that,
depending on the situation, can be used to implement various notions of loops
and
recursion~\cite{joyal1996traced,Hasegawa:2009:TMC:1552068.1552069,Hasegawa:1997:RCS:645893.671607},
(iii) the structure includes an isomorphism showing that the dual operator is
an involution, and (iv) the structure is equipped with objects representing
higher-order functions and a morphism \textit{eval} that applies these
functional objects. Interestingly each monoidal structure provides the same
ingredients, resulting in two dualities, two traces, two involutions, and two
notions of higher-order functions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Additive Duality in {{langRev}} }
\label{sec:neg}

Describing the syntax and types of an extension to {{langRev}} with additive
duality is fairly straightforward. Basically, we extend the language with
negative types denoted {{-b}}, negative values denoted {{-v}}, and two
isomorphisms {{eta+}} and {{eps+}} with the following type judgements:

%subcode{bnf} include main
% Value Types, b = ... | -b
% Values, v = ... | -v
%
% Isomorphisms, iso &=& ... | eta+ | eps+

\vspace{-15pt}
\begin{multicols}{2}  
%subcode{opsem} include main
% eta+ &: 0 <-> (-b) + b :& eps+

%subcode{proof} include main
%@ |- v : b
%@@ |- -v : -b
\end{multicols}

For the graphical language, we visually present {{eta+}} and {{eps+}} as
U-shaped connectors. On the left below is {{eta+}} showing the map from {{0}}
to {{-b+b}} and dually on the right is {{eps+}} showing the map from {{-b+b}}
to 0. Even though the diagrams below show a {{0}} wire for completeness,
later diagrams will always drop them in contexts where we can implicitly
introduce/eliminate then using {{zeroi}}/{{zeroe}}:
\begin{multicols}{2}
\begin{center}
\scalebox{0.8}{
  \includegraphics{diagrams/eta.pdf}
}
\end{center}
  
\begin{center}
\scalebox{0.8}{
 \includegraphics{diagrams/eps.pdf}
}
\end{center}
\end{multicols}

The usual interpretation of the type {{b1+b2}} is that we either have an
appropriately tagged value of type {{b1}} or an appropriately tagged value of
type {{b2}}. This interpretation is maintained in the presence of {{eta+}}
and {{eps+}} in the following sense: a value of type {{right v : -b + b}}
flowing into an {{eps+}} on the lower wire switches to a value 
{{left (-v): -b + b}} that flows backwards on the upper wire. The inversion of
direction is captured by the negative sign on the value. 

\begin{multicols}{2}
\begin{center}
\scalebox{1.1}{
  \includegraphics{diagrams/eps_plus1.pdf}
}
\end{center}
  
\begin{center}
\scalebox{1.1}{
  \includegraphics{diagrams/eps_plus2.pdf}
}
\end{center}  
\end{multicols}

%% Hence, the intuitive way to read the diagrams is that they reverse the
%% flow of information -- i.e. they capture a notion of \emph{negative
%%   information flow}. In normal {{langRev}} circuits, information flows
%% from left-to-right. The operator {{eps+}} causes information to flow
%% from right-to-left.  The interpretation of {{eta+}} is the opposite:
%% it turns a right-to-left flow of information in a circuit into a
%% left-to-right flow.  The diagram below is an example of using {{eta+}}
%% and {{eps+}} and corresponds to the additive {{a <-> a}} isomorphism
%% from the introduction. (Here the {{0}} wires have been shown for
%% completeness.)
\paragraph*{Negative Information Flow.} 
The intuitive way to read the diagrams is that they reverse the flow of
information. In previous {{langRev}} circuits, information was always flowing
from left to right. The presence of {{eps+}} operations cause information to
flow from right to left which can be reversed again using {{eta+}}.  As an
example, the diagram representing the isomorphism from the introduction is:
\begin{center}
\scalebox{1.2}{
  \includegraphics{diagrams/algebra1.pdf}
}
\end{center}

% A time traveling intuition is also applicable here: if view the
% left-to-right direction of information flow as the canonical forward
% direction of information, then the action of {{eps+}} allows values,
% aka information particles, to flow backwards in time. Particles that
% flow backwards in time get to see and interact with the history of
% particles that they coexist with, i.e., values that they exist
% multiplicatively with. Hence the isomorphism {{(-b1)*b2<->-(b1*b2)}}
% (whose construction is shown in Section \ref{sec:neg-constructions}).

%\subsection{Compact Closure with Additive Inverses}
%% \emph{Coherence for Compact Closure with Additive Duality.}  The
%% appropriate coherence condition for compact closed categories,
%% expressed as the triangle axiom, is satisfied by {{eta+}}/{{eps+}} and
%% may be visualized (modulo {{0}} wires, swapping etc.) as explained by
%% Selinger \cite{springerlink:10.1007/978-3-642-12821-94} as the
%% operational equivalence:

\paragraph*{Coherence}
The coherence condition for compact closed categories may be visualized as
follows~\cite{springerlink:10.1007/978-3-642-12821-94}:
\begin{center}
  \includegraphics{diagrams/coherence.pdf}
\end{center}
It is intuitively clear that if the interpretations of {{eta+}} and {{eps+}}
are to reverse the flow of execution, then this coherence condition is
satisfied.

\paragraph*{Trace.}
Every compact closed categoy admits a trace. In the context of {{langRev}}
with {{eta+}}/{{eps+}}, we get the following construction:

%% \emph{Non-termination.}  Compact closed categories contain {{trace}}
%% operations and this implies that {{eta+}}/{{eps+}} allow the
%% construction of a {{trace+}} operator (shown in Section
%% \ref{sec:monoidal-constructions}).  As shown in previous work
%% \cite{infeffects}, non-terminating computation can be constructed from
%%      {{trace+}} and recursive types.
\begin{multicols}{2}
\begin{center}
  \includegraphics{diagrams/trace.pdf}
\end{center}

%subcode{proof} include main
%@ c : b2 + b1 <-> b2 + b3
%@@ trace c : b1 <-> b3

\end{multicols}

% \paragraph*{Zero Information.}
% The fact that {{eta+}} (and hence {{eps+}}) introduces no information
% can be understood is by viewing {{-b+b}} as a type whose value is
% observable only if you `pay' {{b}} amount of information to the {{-b}}
% branch. When {{b}} amount of information is supplied {{(-b+b)+b=b}}
% amount of information is returned to you. (TODO. more needs to be said
% here.)

% For this reason, our treatment of negative values in the context of
% {{langRev}} is particularly simple. We will have a type $0$ and an
% isomorphism {{0 <-> a + (-a)}} that when used in the left-to-right direction
% allows us to create a value and a corresponding debt out of nothing. Both the
% value and its negative counterpart can flow anywhere in the system. Because
% information is preserved, a closed program (which does not have any
% ``dangling'' negative values) will eventually have to match the negative
% value with some corresponding value, effectively using the isomorphism in the
% right-to-left direction. 

\emph{Involution (Principium Contradictiones)}

{{b <-> -(-b)}}

\begin{center}
  \includegraphics{diagrams/double_neg.pdf}
\end{center}

\emph{Functions}

Mention LL and {{a -o b}}.

\begin{center}
  \includegraphics{diagrams/function.pdf}
\end{center}

Let us use the shorthand {{b1 -o+ b2 = 0 <-> -b1 + b2}}. See
Sec. \ref{sec:hof} for a discussion of the additive functions
vs. multiplicative functions. 
\roshan{Where is the HOF discussion?}
\amr{deleted because it was weak and unfocused}

\emph{Function composition.}

\begin{multicols}{2}
\scalebox{1.1}{
    \includegraphics{diagrams/compose1.pdf}
}

\scalebox{0.8}{
    \includegraphics{diagrams/compose.pdf}
}
\end{multicols}
This is also equivalent to sequencing both the computation blocks. 

\begin{center}
  \includegraphics{diagrams/compose2.pdf}
\end{center}

\emph{Function application.}
test.
\begin{multicols}{2}
\begin{center}
\scalebox{1.0}{
  \includegraphics{diagrams/apply1.pdf}
}
\end{center}
\begin{center}
\scalebox{0.8}{
  \includegraphics{diagrams/apply2.pdf}
}
\end{center}
\end{multicols}

\emph{Delimited continuation}

\begin{center}
  \includegraphics{diagrams/delimc.pdf}
\end{center}

\emph{Negation distributes over {{+}}. }

{{-(b1 + b2) <-> (-b1) + (-b2)}}

\begin{center}
  \includegraphics{diagrams/dist_neg_plus.pdf}
\end{center}

\emph{Lifting an operation of positive types to negated types}

\begin{multicols}{2}
Given {{c : b1 <-> b2}}

\begin{center}
  \includegraphics{diagrams/neg_lift.pdf}
\end{center}  

%subcode{proof} include main
%@ c : b1 <-> b2
%@@ neg c : (-b1) <-> (-b2)
\end{multicols}

%%%%%%%%%%%%%%%%%%%%%%
\subsection{Multiplicative Duality in {{langRev}} }
\label{sec:frac}

%% %% The section follows the outline of the previous one, first presenting
%% %% the new syntax, and discussing the graphical language and the
%% %% properites implied by the underlying categorical structure.

Multiplicative duality is added to {{langRev}} by adding types
{{1/b}}, values {{1/v}} and isomorphisms {{eta*}} and {{eps*}}.

%subcode{bnf} include main
% Value Types, b = ... | 1/b
% Values, v = ... | 1/v
%
% Isomorphisms, iso &=& ... | eta* | eps*

These have type judgements, similar to those of their additive
couterparts.
\begin{multicols}{2}
%subcode{opsem} include main
% eta* &: 1 <-> (1/b) * b :& eps*

%subcode{proof} include main
%@ |- v : b
%@@ |- 1/v : 1/b  
\end{multicols}

For convenience we sometime use the notation {{b1/b2}} to indicate the
pair {{b1 * (1/b2)}}.  As with additive duals, we represent {{eta*}}
and {{eps*}} using U-shaped connectors. In later diagrams, the wires
for {{1}} will be dropped as they can freely introduced or eliminated
using {{uniti}} and {{unite}}.

\begin{multicols}{2}
\begin{center}
  \includegraphics{diagrams/eta_times.pdf}
\end{center}
  
\begin{center}
  \includegraphics{diagrams/eps_times.pdf}
\end{center}
\end{multicols}

%% The usual interpretation of {{b1*b2}} we have both a value of type
%% {{b1}} and a value of type {{b2}}. This interpretation is maintained
%% in the presence of fractionals. Hence an {{eta* : 1 <-> (1/b) * b}} is
%% to be viewed as a fission point for a value of type {{b}} and its
%% multiplicative inverse {{1/b}}. This naturally raises the question --
%% which value of type {{b}} is produced? The isomorphism {{eta*}} does
%% not produce any specific value of type {{b}}, instead it produces a
%% placeholder for an unspecified value and dually, {{1/b}} is inhabited
%% by the placeholder that indicates the absence of the unspecified
%% value.
In the usual interpretation of {{b1*b2}} we have both a value of type {{b1}}
and a value of type {{b2}}. This interpretation is maintained in the presence
of fractionals. Hence an {{eta* : 1 <-> (1/b) * b}} is to be viewed as a
fission point for a value of type {{b}} and its multiplicative inverse
{{1/b}}. This naturally raises the question -- which value of type {{b}} is
produced? The isomorphism {{eta*}} does not produce any specific value of
type {{b}}, instead it produces a place holder for a value such that the
value of type {{1/b}} is always the dual of the chosen one.

\begin{multicols}{2}
\begin{center}
\scalebox{1.5}{
  \includegraphics{diagrams/eta_times1.pdf}
}
\end{center}

\begin{center}
\scalebox{1.5}{
  \includegraphics{diagrams/eps_times1.pdf}
}
\end{center}  
\end{multicols}

\paragraph*{Coherence.}
The following coherence condition is satisfied as before.

In terms of \emph{logic programming}, {{eta* : 1 <-> (1/b)*b}} acts as
the site of creation of a new typed logic variable and constructs two
values using it: a value of type {{b}} and its dual of type
{{1/b}}. These two values are free to move independently through the
computation. Operations on one value affect what operations are possible
on the other since they are both related by an underlying logic
variable. The operator {{eps*}} acts as a unification site for values of
type {{b}} and {{1/b}} flowing along both branches. If the values are
not unifiable duals, then the specific program sequence fails. Like
with logic programming, one can has many worlds where some might fail,
while others make progress. Sec. \ref{sec:rat} has more details.  If
the values are exactly duals, then unification succeeds and {{eta*}}
returns {{1}}.
% \paragraph*{Unification.}
% A computational interpretation of this is that {{langRevEE}} acts like
% a logic programming system and {{eta*}} acts as the site of creation
% of a new typed logic variable of type {{b}} and its corresponding dual
% of type {{1/b}}. These two values are free to move independently
% through a circuit, but any operations on value affect what operations
% are possible on the other since they are both related by an underlying
% logic variable. Dually, {{eps*}} acts as a unification site for values
% flowing along both branches. If the values are not exactly duals, then
% the system fails to unify and hence the program is stuck. If the
% values are exactly duals, then unification succeeds and {{eta*}}
% returns {{1}}.

% \paragraph*{Entanglement and Anti-Particles.}
% A dual view of the operation of {{eta*}} and {{eps*}} is inspired by
% quantum mechanics. The {{eta*}} is a site of fission that creates an
% information particle and its anti-particle. Both flow in the same
% direction in time. The produce particles are entangles in that actions
% on one (such as transformation by application of an isomorphism)
% affects the other. Further the fission site does not create a particle
% in any specific state, but in a superposition of all possible states
% as dictated by its type. In each possible world that the particle
% exists, it takes on a specific value inhibiting its type and
% correspondingly its entangled anti-particle takes the dual of the
% specific value.

% This also leads to the exciting point of view that entangled
% particle/anti-particle pairs act as functions in a physical
% first-class sense. We can ask the function to be applied to a value by
% unifying the anti-particle with a particle in a well-known state and
% examining is pair. We shall use this point of view in the development
% our SAT solver (see Sec. \ref{sec:sat-solver}).

% \paragraph*{Entanglement and Anti-Particles.}
% \paragraph*{Annihilation.}
% As with {{(+, 0)}} we can construct a {{trace*}} operation. Unlike the
% additive {{trace+}}, {{trace*}} gives us the ability to find the
% fixpoint of the constraint (expressed with combinators) that is
% traced. If the constraint has not satisfying values, there is not
% fixpoint possible -- we use the term `annihilation' to describe the
% corresponding undefined state of the program. Annihilation and a
% non-termination both describe undefined program execution -- while
% non-termination characterizes an iteration that fails to terminate,
% annihilation characterizes a constraint that has no fixpoint. 
%% An analogy inspired by quantum mechanics is also applicable here.  The
%% operator {{eta*}} is a site of fission that creates \emph{an entangled
%%   particle and anti-particle}. These particles can be thought to be in
%% a \emph{superposition} of states determined by their type. Both flow
%% in the same direction in time.  In each possible world that the
%% particle exists, it takes on a specific value inhibiting its type and
%% correspondingly its entangled anti-particle takes the dual of the
%% specific value. Since the particles are entangled, actions on one
%% (such as transformation by application of an isomorphism) affects the
%% other in much the same sense as \emph{action at a distance}. The
%% `{{a-o*b}}' functions mentioned in the introduction, are first-class
%% values and they correspond to these entagled pairs. Finally {{eps*}}
%% operations are sites where corresponding particle and anti-particles
%% annihilate each other. While appealing as an analogy, this description
%% does not imply (or preclude) any formal connection with Physics.
% \paragraph*{Zero Information.}
% As with {{eta+}}, we argue that {{eta*}} constructs no new
% information, instead it gives us the ability to explore as many worlds
% as permitted by the type {{b}}. So situate ourselves in any specific
% world, we have to supply additional information in the form of
% unifying the particle or anti-particle with a particle representing
% known information. Hence only by supplying information to a an
% entangled pair can we read information from it -- and always exactly
% as much information as we supplied.

%% We can ask the function to be applied to a value by
%% unifying the anti-particle with a particle in a well-known state and
%% examining is pair. We shall use this point of view in the development
%% our SAT solver (see Sec. \ref{sec:sat-solver}).

%%  This also leads to the exciting point of view that
%% entangled such particle/anti-particle pairs, which are first class
%% values, act as functions in a physical first-class sense.

%% Further the fission site does not create a particle in any specific
%% state, but in a superposition of all possible states as dictated by
%% its type.

% Since such pairs are first-class entities, they correspond to 

%% \emph{Coherence for Compact Closure with Multiplicative Duality.}  As
%% before, the triangle axiom which is the coherence condition on compact
%% closed categories is applicable and is expressed below in terms of
%% {{eta*}}/{{eps*}} and the multiplicative monoid.

\paragraph*{Coherence.}
The following coherence condition is satisfied as before.
% ADD MATH. 

% %%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Conventional Higher-Order Functions}
% \label{sec:hof}

\emph{Annihilation.}  
With {{eta*}} and {{eps*}}, we can construct a {{trace*}} operation
which gives us the ability to find the fixpoint of a constraint.  If
the constraint has no satisfying values, there is no fixpoint possible
-- we use the term \emph{annihilation} to describe the corresponding
undefined state of the program.
% In a conventional language, higher-order functions capture scope and can be
% represented as first-class values. In {{langRevEE}}, we have two notions of
% functions both of which share similarities with conventional higher-order
% functions.

Annihilation and non-termination describe undefined program
execution. While non-termination characterizes an iteration that fails
to terminate, annihilation characterizes a constraint that has no
fixpoint. The SAT-solver of Sec. \ref{sec:sat-solver} works by
annihilating only the program states that do not match the boolean
satisfiability constraint.
% \paragraph*{Unification.}
% A computational interpretation of this is that {{langRevEE}} acts like
% a logic programming system and {{eta*}} acts as the site of creation
% of a new typed logic variable of type {{b}} and its corresponding dual
% of type {{1/b}}. These two values are free to move independently
% through a circuit, but any operations on value affect what operations
% are possible on the other since they are both related by an underlying
% logic variable. Dually, {{eps*}} acts as a unification site for values
% flowing along both branches. If the values are not exactly duals, then
% the system fails to unify and hence the program is stuck. If the
% values are exactly duals, then unification succeeds and {{eta*}}
% returns {{1}}.
% \begin{itemize}
% \item 
% Additive functions are not really values, but are possible paths of
% execution. However, they can be encoded as values as follows: Consider
% two functions {{f:b1 -o+ b2}} and {{g:b1-o+b2}}. A value of type
% {{bool}} can discriminate them and hence a value of type {{bool}} acts
% as the address of the function and is a first class value
% representing one of the {{b1 -o+ b2}} functions.  %% This is similar to
% %% how first classes functions a'la \lcal are represented by their
% %% addresses when compiled to conventional hardware.

% \paragraph*{Entanglement and Anti-Particles.}
% A dual view of the operation of {{eta*}} and {{eps*}} is inspired by
% quantum mechanics. The {{eta*}} is a site of fission that creates an
% information particle and its anti-particle. Both flow in the same
% direction in time. The produce particles are entangles in that actions
% on one (such as transformation by application of an isomorphism)
% affects the other. Further the fission site does not create a particle
% in any specific state, but in a superposition of all possible states
% as dictated by its type. In each possible world that the particle
% exists, it takes on a specific value inhibiting its type and
% correspondingly its entangled anti-particle takes the dual of the
% specific value.
% \item
% Multiplicative functions, of the form {{b1 -o* b2}}, on the other hand
% are indeed first-class values. However these functions can only be
% linearly used, i.e., they represent a constraint that can be satisfied
% at most once.
% %%  in one `computational world'. It may be possible that to
% %% repeatedly invoke multiplicative functions with an appropriate
% %% recursive type whose unfoldings allow for repeated unification.
% \end{itemize}

% This also leads to the exciting point of view that entangled
% particle/anti-particle pairs act as functions in a physical
% first-class sense. We can ask the function to be applied to a value by
% unifying the anti-particle with a particle in a well-known state and
% examining is pair. We shall use this point of view in the development
% our SAT solver (see Sec. \ref{sec:sat-solver}).
% The exact encoding on HOF in {{langRevEE}} should shed light on
% several problems, including the connections to continuations discussed
% in Sec. \ref{sec:related}. They should also allow for a natural
% encoding of HOFs in the translations of \lcal to {{langRevT}} and
% indicate a monadic encapsulation of of information effects which are
% currrently structured as an arrow.

% \paragraph*{Annihilation.}
% As with {{(+, 0)}} we can construct a {{trace*}} operation. Unlike the
% additive {{trace+}}, {{trace*}} gives us the ability to find the
% fixpoint of the constraint (expressed with combinators) that is
% traced. If the constraint has not satisfying values, there is not
% fixpoint possible -- we use the term `annihilation' to describe the
% corresponding undefined state of the program. Annihilation and a
% non-termination both describe undefined program execution -- while
% non-termination characterizes an iteration that fails to terminate,
% annihilation characterizes a constraint that has no fixpoint. 

% \paragraph*{Zero Information.}
% As with {{eta+}}, we argue that {{eta*}} constructs no new
% information, instead it gives us the ability to explore as many worlds
% as permitted by the type {{b}}. So situate ourselves in any specific
% world, we have to supply additional information in the form of
% unifying the particle or anti-particle with a particle representing
% known information. Hence only by supplying information to a an
% entangled pair can we read information from it -- and always exactly
% as much information as we supplied.

% ADD MATH. 

% %%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Conventional Higher-Order Functions}
% \label{sec:hof}

% In a conventional language, higher-order functions capture scope and can be
% represented as first-class values. In {{langRevEE}}, we have two notions of
% functions both of which share similarities with conventional higher-order
% functions.

% \begin{itemize}
% \item 
% Additive functions are not really values, but are possible paths of
% execution. However, they can be encoded as values as follows: Consider
% two functions {{f:b1 -o+ b2}} and {{g:b1-o+b2}}. A value of type
% {{bool}} can discriminate them and hence a value of type {{bool}} acts
% as the address of the function and is a first class value
% representing one of the {{b1 -o+ b2}} functions.  %% This is similar to
% %% how first classes functions a'la \lcal are represented by their
% %% addresses when compiled to conventional hardware.

\emph{Constructions.} The constructions for functions, involution, and trace are identical,
replacing {{+}} by {{*}}, \textit{mutatis mutandis}.

%% \begin{enumerate}
%% \item 
%% \emph{Trace}
%% % \item
%% % Multiplicative functions, of the form {{b1 -o* b2}}, on the other hand
%% % are indeed first-class values. However these functions can only be
%% % linearly used, i.e., they represent a constraint that can be satisfied
%% % at most once.
%% % %%  in one `computational world'. It may be possible that to
%% % %% repeatedly invoke multiplicative functions with an appropriate
%% % %% recursive type whose unfoldings allow for repeated unification.
%% % \end{itemize}

% The exact encoding on HOF in {{langRevEE}} should shed light on
% several problems, including the connections to continuations discussed
% in Sec. \ref{sec:related}. They should also allow for a natural
% encoding of HOFs in the translations of \lcal to {{langRevT}} and
% indicate a monadic encapsulation of of information effects which are
% currrently structured as an arrow.

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Constructions specific to Negatives and Fractions}
\label{sec:specific-constructions}

While the constructions in Sec \ref{sec:neg} apply to both the
{{(0,+)}} and {{(1,*)}} monoids, each construction presented here
applies to a specific monoid.

\roshan{Each of these isomorphisms has baffled us -- either we
  believed they were not constructible. Or we have struggled to
  comprehend what they mean. Hence it is worth saying a sentence or
  two about each beyond how they are constructed.}

\begin{enumerate}

%% \item 
%% \emph{ {{eps_{fst} }} }

%% {{(

\item 
\emph{Lifting negation out of {{*}}. }

{{(-b1) * b2 <-> -(b1 * b2) <-> b1 * (-b2)}}

To build these isomorphisms, we first build an intermeidate
construction which we call {{eps_{fst} : (-b1)*b2 + b1*b2 <-> 0}}. 
\begin{center}
\scalebox{0.8}{
  \includegraphics{diagrams/eps_fst.pdf}
}
\end{center}

The isomorphism {{(-b1) * b2 <-> -(b1 * b2)}} can be constructed in
terms of {{eps_{fst} }} as shown below. 

\begin{center}
\scalebox{0.9}{
  \includegraphics{diagrams/mult_neg.pdf}
}
\end{center}  

The second isomorphism can be built in the same way by merely swapping
the arguments. 

\item
\emph{Multiplying Negatives}

{{b1 * b2 <-> (-b1)*(-b2)}}

This isomorphism can be built using \emph{involution} constructed in
Sec \ref{sec:neg} and the isomorphisms above, and correspond to the
algebraic manipulation:

%subcode{opsem} include main
%! columnStyle = rc
%   & b1 * b2
% = & -(-(b1*b2)) 
% = & -((-b1)*b2)
% = & (-b1)*(-b2)

%% \emph{Distributing fractions over {{*}} }
%% {{1/(b1*b2) <-> (1/b1) * (1/b2)}}


%% \emph{Lifting Computation intro fractions}
%% {{b1 <-> b2 ==> 1/b1 <-> 1/b2}}

\item 
\emph{Multiplying Fractions}

{{b1/b2 * b3/b4 <-> (b1*b3)/(b2*b4)}}


\item 
\emph{Adding the numerator}

{{b1/b + b2/b <-> (b1+b2)/b}}

\item 
\emph{Fraction Addition}

{{b1/b2 + b3/b4 <-> (b1*b4+b3*b2)/(b2*b4) }}

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Computing in the Field of Rationals : {{langRevEE}} }
\label{sec:rat}


In this section we develop the semantics for {{langRev}} extended with
negative and fractional types, {{langRevEE}}.  We start with the
semantics for {{langRev}} defined in Sec \ref{sec:pi}, which we will
systematically rewrite until we have the desired {{langRevEE}}
sematics. Broadly this construction takes two steps 

\begin{enumerate}
\item We rewrite the semantics in Sec \ref{sec:pi} using unification
  of the values instead of direct pattern matching. This gives us the
  neccessary infrastructure for fractional types, as we will see. 

\item We write a reverse interpreter by reversing the direction of the
  `{{|-->}}' reductions. This gives us the neccessary infrastructure for
  negative types.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%
\subsection{Unification and Logic Programming}

\roshan{Can we cite away unification, reificatio, handling of
  substitutions etc?}

%%%%%%%%%%%%%%%%%%%%%
\subsection{Extending {{langRev}} Semantics}


\paragraph{Primitive Isomorphsims.}
Previously the reductions of the primitive isomorphisms were specified
in the following form.

{{ iso v_{input pattern} |--> v_{output pattern} }}

\noindent
In the precense of unification, we rewrite them to accept the incoming
substitution and extend it with the appropriate unification.

{{ iso s v' |--> (v_{output pattern}, s[v' <><> v_{input pattern}]) }}


\roshan{Talk about non-determinism, determinism and our handling of
  choices. Also, in what sense are we logically reversible?}

\paragraph{Extending the evaulator with unification.} 
We have rewriten the semantics of Sec. \ref{sec:pi} with
unification. The behaviour of the intepreter has not changed.

%subcode{opsem} include main
%! columnStyle = rclr
%! fwd = \triangleright
% <iso, v, C, s>_{fwd} &|-->& {[iso, v', C, s']}_{fwd}
% & & where iso s v |--> (v', s')
% <c1(;)c2, v, C, s>_{fwd} &|-->& <c1, v, Fst C c2, s>_{fwd}
% {[c1, v, Fst C c2, s]}_{fwd} &|-->& <c2, v, Snd c1 C, s>_{fwd}
% {[c2, v, Snd c1 C, s]}_{fwd} &|-->& {[ c1(;)c2, v, C,s ]}_{fwd}
% <c1(+)c2, v', C, s>_{fwd} &|-->& <c1, v, LeftP C c2, s[v' <><> left v]>_{fwd}
% {[ c1, v, LeftP C c2, s ]}_{fwd} &|-->& {[c1 (+) c2, left v, C,s ]}_{fwd}
% <c1(+)c2, v', C, s>_{fwd} &|-->& <c2, v, RightP c1 C, s[v' <><> right v]>_{fwd}
% {[ c2, v, RightP c1 C,s ]}_{fwd} &|-->& {[c1 (+) c2, right v, C,s ]}_{fwd}
% <c1(*)c2, v', C, s>_{fwd} &|-->& <c1, v1, LeftT C c2 v2, s'>_{fwd}
% & & where s' = s[v <><> (v1, v2)]
% {[ c1, v1, LeftT C c2 v2,s ]}_{fwd} &|-->& <c2, v2, RightT c1 v1 C, s>_{fwd}
% {[ c2, v2, RightT c1 v1 C,s ]}_{fwd} &|-->& {[ c1 (*) c2, (v1, v2), C,s ]}_{fwd}

\paragraph{Backward evaluator.}
Here we have written a ``backward'' evaluator in the same style, 
i.e., evaluation {{c v}} in this evaluator is equivalent to evaluating
{{C^{dagger} v}} in the forward evaluator.

%subcode{opsem} include main
%! columnStyle = rclr
%! bck = \triangleleft
% {[iso, v, C, s]}_{bck} &|-->& <iso, v', C, s'>_{bck}
% & & where iso^{dagger} s v |--> (v', s')
% <c1, v, Fst C c2, s>_{bck} &|-->& <c1(;)c2, v, C, s>_{bck}
% <c2, v, Snd c1 C, s>_{bck} &|-->& {[c1, v, Fst C c2, s]}_{bck}
% {[ c1(;)c2, v, C, s ]}_{bck} &|-->& {[c2, v, Snd c1 C, s]}_{bck}
% <c1, v, LeftP C c2,s >_{bck} &|-->& <c1(+)c2,left v, C>_{bck}
% {[c1 (+) c2, v', C, s ]}_{bck} &|-->& {[ c1, v, LeftP C c2, s[v' <><> left v] ]}_{bck}
% <c2, v, RightP c1 C, s>_{bck} &|-->& <c1(+)c2, right v, C>_{bck}
% {[c1 (+) c2, v', C,s ]}_{bck} &|-->& {[ c2, v, RightP c1 C, s[v' <><> right v] ]}_{bck}
% <c1, v1, LeftT C c2 v2, s>_{bck} &|-->& <c1(*)c2, (v1, v2), C, s>_{bck}
% <c2, v2, RightT c1 v1 C, s>_{bck} &|-->& {[ c1, v1, LeftT C c2 v2, s ]}_{bck}
% {[ c1 (*) c2, v, C, s ]}_{bck} &|-->& {[ c2, v2, RightT c1 v1 C, S' ]}_{bck}
% && where s' = s[v <><> (v1, v2)]

%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Semantics of {{langRevEE}} }

The previous section has defined all the pieces needed to define the
semantics of {{langRevEE}} and we piece them together here. 

\begin{enumerate}
\item 
To add multiplicative duality, we add the following rules to the set
of primitive isomorphisms:

\begin{enumerate}
\item {{eta* s () |--> ((1/v, v), s)}}
  where {{v}} is a fresh logic variable. 

\item {{eps* s v |--> (1, s[v <><> (inv(v'),v') ])}}
  where {{v'}} is a fresh logic variable. 

\end{enumerate}

\item To add negative types we add the following rules to the
  reductions above: 

  \begin{enumerate}
  \item 

%subcode{opsem} include main
%! columnStyle = rclr
%! fwd = \triangleright
%! bck = \triangleleft
% <eps+, v, C, s>_{fwd} |--> <eps+, left (-v'), C, s[v <><> right v]>_{bck}
% <eps+, v, C, s>_{fwd} |--> <eps+, right v', C, s[v <><> left (-v')]>_{bck}

    \item
%subcode{opsem} include main
%! columnStyle = rclr
%! fwd = \triangleright
%! bck = \triangleleft
% <eta+, v, C, s>_{bck} |--> <eta+, left (-v'), C, s[v <><> right v]>_{fwd}
% <eta+, v, C, s>_{bck} |--> <eta+, right v', C, s[v <><> left (-v')]>_{fwd}

  \end{enumerate}


\end{enumerate}

%% %%%%%%%%%%%%%%%%%%%%%%
%% \subsection{Constructions in {{langRevEE}} }
%% \label{sec:monoidal-constructions}


\paragraph*{Zero Information.}
As with {{eta+}}, we argue that {{eta*}} constructs no new
information, instead it gives us the ability to explore as many worlds
as permitted by the type {{b}}. So situate ourselves in any specific
world, we have to supply additional information in the form of
unifying the particle or anti-particle with a particle representing
known information. Hence only by supplying information to a an
entangled pair can we read information from it -- and always exactly
as much information as we supplied.

ADD MATH. 

\emph{Zero Information.}  The intuitive explanation that {{eta+}} and
hence {{eps+}} introduces no information effects is that {{-b+b}} as a
type whose value is observable only if you `pay' {{b}} amount of
information to the {{-b}} branch. When {{b}} amount of information is
supplied {{(-b+b)+b=b}} amount of information is returned to
you. (TODO. more needs to be said here.)

For this reason, our treatment of negative values in the context of
{{langRev}} is particularly simple. We will have a type $0$ and an
isomorphism {{0 <-> a + (-a)}} that when used in the left-to-right direction
allows us to create a value and a corresponding debt out of nothing. Both the
value and its negative counterpart can flow anywhere in the system. Because
information is preserved, a closed program (which does not have any
``dangling'' negative values) will eventually have to match the negative
value with some corresponding value, effectively using the isomorphism in the
right-to-left direction. 



\paragraph*{Observability.} 
Execution of program is defined by {{c : b1 <-> b2}} when evaluated on input
{{v1 : b1}} gives us a value {{v2 : b2}} on termination. Execution is well
defined only if {{b1}} and {{b2}} are entirely positive types. This is
similar to the constraint that Zeilberger imposes to explain intuitionistic
polarity and delimited control~\cite{10.1109/LICS.2010.23}.

Consider the program that

(TODO: So this is a really bad explanation. But this section is really
important and we need to explain carefully. Maybe an analogy can be
drawn to Cayley diagrams for groups to point out that the notion of
`i' is entirely artificial, but it is essential to the discourse.)

\begin{center}
  \includegraphics{diagrams/shuffle.pdf}
\end{center}

We define observables to be only positive types. The outputs of
programs that output mixed positive and negative types are not
observable.  Also, programs that input mixed positive and negatives
types are not executable.

It is not fair to say that negative types flow backwards. The
following circuits are valid in {{langRevEE}}. It is however proper to
say that for any type {{b}} that flows in a direction, the type {{-b}}
flows in the reverse direction.

Several things we don't know how to do, or are sloppy about. Some of
these are deep questions to which there is no immeidate answer.

\begin{itemize}

\item Is the language without recursive types always terminating? For
  the right symmetry with annihilation, it is desirable to have
  non-termination.

\item What is the correct lemma for logical reversibility now?

\item We should say things about information preservation. At least
  more so than what is being said now.


\item Multiplicative duality gives us a means of talking about
  equivalence classes of values. This should correspond to all sorts
  of other things -- groupoids, Lawvere theories etc. We barely even
  understand these connections.

\item How does this connect to QC? Is there any strong connection such
  that the SAT solver here is an alternate QC algorithm to Shor's?

\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Advanced Example: SAT Solver in {{langRevEE}} }
\label{sec:prog}
\label{sec:sat-solver}

% Additive traces correspond to iteration and multiplicative traces
% correspond to fixpoint of constraints. Addtive traces have been
% studied in the past \cite{infeffects} and here we focus on the
% multiplicative trace.

% The construction presented here is a novel SAT-solver that relies on
% {{trace*}} for its solution. It varies from previous classical SAT solvers
% \amr{must cite ???} in that there is no explicit search operation on the
% boolean space. It also varies from previous quantum SAT solvers \amr{must
%   cite ???} in that it does uses a {{trace}} operation to compute a fixpoint
% which is the solution following which an isomorphic clone operation allows us
% to examine the result.  Though we focus on SAT here, this construction can be
% generalized to any constraint satisfaction problem whose search space can be
% represented by a recursive type and for which an isomorphic clone operation
% can be constructed. 
% can be constructed. Before we start, let us recall two constructions:

We illustrate the expressive power of first-class constraints represented by
fractional types

%%%%%%%%%%%%%%%%%%%
\subsection{Constructions with Booleans}

Before we start, let us recall a construction of conditionals from
prior work \cite{infeffects}.  Given any combinator {{c : b <-> b}} we
can construct a combinator called {{if_c : bool*b <->bool*b}} in terms
of {{c}}, where {{if_c}} behaves like a one-armed
$\mathit{if}$-expression. If the supplied boolean is {{true}} then the
combinator {{c}} is used to transform the value of type~{{b}}. If the
boolean is {{false}}, then the value of type {{b}} remains
unchanged. We can write down the combinator for {{if_c}} in terms of
{{c}} as {{ dist (;) ((id (*) c) (+) id) (;) factor }}.

% Let us look at the combinator pictorially as if it were a circuit and values
% are like particles that flow through this circuit.  

The diagram below shows the input value of type {{(1+1)*b}} processed
by the distribute operator {{dist}}, which converts it into a value of
type {{(1*b)+(1*b)}}. In the {{left}} branch, which corresponds to the
case when the boolean is {{true}} (i.e. the value was {{left ()}}),
the combinator~{{c}} is applied to the value of type~{{b}}. The right
branch which corresponds to the boolean being {{false}} passes along
the value of type {{b}} unchanged.


\begin{center}
\scalebox{1.0}{
%%subcode-line{pdfimage}[diagrams/if_c.pdf]
\includegraphics{diagrams/thesis/cnot.pdf}
}
\end{center}

% We will be seeing many more such wiring diagrams in this paper and it is
% useful to note some conventions about them. Wires indicate a
% value that can exist in the program. Each wire, whenever possible, is
% annotated with its type and sometimes additional information to help clarify
% its role. When multiple wires run in parallel, it means that those values
% exist in the system at the same time, indicating pair types. When there is a
% disjunction, we put a {{+}} between the wires. 
% Combinators for distribution {{dist}} and factoring {{factor}}
% are represented as triangles with their operator symbols in them. Other
% triangles may be used and, in each case, types or labels will be used to
% clarify their roles. Finally, we don't draw boxes for combinators such as
% {{id}}, commutativity, and associativity, but instead just shuffle the wires
% as appropriate.

\emph{Boolean Operation based on {{if_c}}.}
The combinator {{if_{not} }} has type {{bool*bool<->bool*bool}} and
negates its second argument if the first argument is {{true}}. This
gate {{if_{not} }} is often referred to as the {{cnot}} gate. An
equivalent construction that is useful is {{else_{not} }} where we
negate the second argument only is the first is {{false}}. 

Similarly, we can iterate the construction of {{if_c}} to check
several bits. The gate {{if_{cnot} }}, which we may also write as
{{if^2_{not} }}, checks two booleans and negates the result wire only
if they are both {{true}}. The gate {{if^2_{not} }} is well known as
the universal reversible primitive gate called the `Toffoli gate'
\cite{Toffoli:1980}.  We can generalize this construction to
     {{if^n_{not} }} which checks {{n}} bits and negates the result
     wire only if they are all {{true}}.

\emph{Cloning Bits.} The gate {{else_{not} }} has the interesting
property that as long as the second argument is fixed to be {{true}},
output of the second wire is always the same as teh value of the first
wire. In other words {{else_{not} }} can be thought of as cloning the
value of the first wire. A circuit of {{n}} parallel {{else_{not} }}
gates can hence clone {{n}} bits. They also consume {{n}} {{true}}
inputs in the process. Let us call this construction 
{{clone^n_{bool} }}.

DIAGRAM clone

DIAGRAM clone n bits


\subsection{Construction of the Solver}
The key insight underlying the construction comes from the fact that
we can build \emph{annihilation circuits}.  Consider the circuit below
that has no observable output, i.e., for all inputs it is
\emph{A Total Annihilation Circuit.}  Consider the circuit below that
has no observable output, i.e., for all inputs it is
annihilating. This happens because it constructs a boolean and its
dual, negates one of them and attempts to collapse them.

\begin{center}
\scalebox{1.2}{
  \includegraphics{diagrams/not_trace.pdf}
}
\end{center}


\emph{Controlled Annihilation.}  We leverage the basic idea of the
above circuit, to collapse all values that are not solutions to the
specified isomorphic constraint {{f}}.  The contraint {{f}} checks the
inputs to see if they match a constraint -- in the case of the SAT
solver, {{f}} would be the circuit expressing the boolean expression
to be satisfied.  While the output of {{f}} may contain many
components, its must contain a boolean indicating if the input satisfy
{{f}}. The operation {{f}} acts on some inputs and its adjoint,
{{f^{dagger} }}, reconstructs the inputs from the outputs.  The
`trick' is the following: the special output wire is used to `control'
the negation of a boolean `control wire'.  If the inputs do not
satisfy {{f}}, the control wire is negated.

\begin{center}
\scalebox{1.2}{
  \includegraphics{diagrams/sat1.pdf}
}
\end{center}  

If this construction is traced using {{trace*}}, all the values that
do not satisfy {{f}} get annihilated because the control wire acts
like the closed-loop `not' in the previous construction. 

\emph{Controlling the heap.}  The final detail we have to be concerned
with is the handling of the heap and garbage of {{f}}. Any boolean
expression {{bool^n -> bool}} can be compiled into the isomorphism
{{f:bool^h*bool^n<->bool^g*bool}} where the extra bits {{bool^h}} and
{{bool^g}} are the heap and garbage. Constructing such an isomorphism
has been detailed before \cite{Toffoli:1980,infeffects}.  However the
computation of {{f}} is meaningful {{iff}} the heap {{bool^h}} is
initialized appropriately. We can ensure that the heap has the
appropriate initial value by checking the heap and negating a second
control wire, if the values do not match, i.e., the dotted part in the
diagram below and is {{if^h_{not} }} followed by a {{not}} operator on
the control wire.
control wire, if the values do not match, i.e., the dotted part in
the diagram below and is {{if^h_{not} }} followed by a {{not}} on the
`heap control wire'.

\begin{center}
\scalebox{1.2}{
  \includegraphics{diagrams/sat2.pdf}
}
\end{center}  

% \roshan{Explain how the dotted part is implemented.}

\emph{Cloning the Solution.}.  Let us call the above construction
which maps inputs, heap and control wires to input, heap and control
wires as {{sat_f}}. The SAT-solver is is completed by tracing the
{{sat_f}} and cloning the `inputs' using {{clone^n_{bool} }}.

\begin{center}
\scalebox{1.5}{
  \includegraphics{diagrams/sat3.pdf}
}
\end{center}  

When the solver is fed inputs initialized to {{true}}, the {{clone}}
will clone any inputs to {{sat_f}} that satisfies the constraints
imposed by {{f}} and the constraints on the {{heap}}. In the case of
unique-SAT the solver will produce exactly 1 or 0 solutions. In the
case of general SAT the solver will produce solutions as determined by
the top-level (Sec. \ref{sec:rat}).

%% %%%%%%%%%%%%%%%%%%%
%% \subsection{GoI machine}

%% We can now encode the GoI machine of
%% Mackie~\cite{Mackie2011,DBLP:conf/popl/Mackie95}. \amr{The machine uses bang
%%   which allows arbitrary duplication. Can we really do that???}






%% the final equivalence is valid and can be used to actually
%% construct an isomorphism between $x^7$ the type of seven binary
%% trees and $x$ the type of binary trees.

%% Here is one way to view computations involving irrationals that might
%% shed some insight into how we should view recursive data types
%% also. For finite datatypes say of size {{n}} we can say that an
%% element specifies a choice of {{1/n}} and hence contains as much
%% information. For finite things, each value is computationally equal to
%% any other value. Any permutation is possible among the values. Hence
%% there is no metric possible on the values.  Recursive data types, such
%% as binary trees, have an infinite number of elements and hence the
%% argument that says that value specifies {{1/n}} units of information
%% is not longer valid. Instead to talk meaningfully about information
%% content, we should talk about a probability distribution on the values
%% of the type. However what shall we base this distribution on? Is there
%% a natural order beyond the unfolding order of the values of a type? 

%% In this context, let us go back to the thought that recursive such as
%% {{nat=nat+1}} have no meaningful solutions, whereas trees do. 


%% Real numbers in that sense really are a series of bits of unknown
%% information theoretic interpretation -- a sort of {{top}}. Numbers
%% such as {{sqrt(2) }} are very different in the sense that even though
%% they contain an infinite sequence of bits, there is a finite program
%% that generates those bits and its only the cost of computing bits that
%% is infinite.  The consistency of arithmetic isn't affected by
%% extending the rational field by adding any specific computable
%% irrational. So it might be possible to design a model of computing
%% where we can indeed deal with them. Such a model will have to explicit
%% thunks for the delayed computation represented by trees and never
%% equate and cancel two infinite computations unless the represent
%% exactly the same infinite computation.

%% TODO.

%% Square root and imaginary types have also appeared in the literature: we
%% relegate the connections to Sec.~\ref{sec:related} and proceed with a simple
%% explanation. We have so far extended the set of types to be the rational
%% numbers. Now we will push this and extend the set of types to algebraic
%% numbers. In other words, we will allow datatypes defined by arbitrary
%% polynomials and allow the roots of such polynomials to be types. 

%% Consider first an example in which we want to compute with the sides of a
%% rectangle whose area is 91 and whose length is longer than its width by 6
%% units. One can solve the quadratic equation to determine that the sides are 7
%% and 13 and proceed. This however prematurely forces us to globally solve the
%% constraint. Instead we can let the two sides of the rectangle be $x$ and
%% $x+6$ and use the following equation to capture the desired constraint:
%% \[
%% x^2 + 6x - 91 = 0
%% \]
%% The equation introduces an isomorphism between the type $x^2 + 6x - 91$ and
%% the type $0$. We can now proceed to compute with the unknown $x$, being
%% assured that in a closed program, our computation will eventually be
%% consistent with the solution of the algebraic equation. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work} 
\label{sec:related}

The idea of ``negative types'' has appeared many times in the literature and
has often been related to some form of continuations. Fractional types are
less common but have also appeared in relation to parsing natural
languages. Although each of these previous occurrences of negative and
fractional types is somewhat related to our work, our results are
subtantially different. To clarify this point, we start by reviewing the
salient point of the major pieces of related work and conclude this section
with a summary contrasting our approach to previous work.

\paragraph*{Declarative Continuations.} 
In his Masters thesis~\cite{Filinski:1989:DCI:648332.755574}, Filinski
proposes that continuations are a \emph{declarative} concept. He,
furthermore, introduces a symmetric extension of the $\lambda$-calculus in
which call-by-value is dual to call-by-name and values are dual to
continuations. In more detail, the symmetric calculus contains a ``value''
fragment and a ``continuation'' fragment which are mirror images. Pairs and
sums are treated as duals in the sense that the ``value'' fragment includes
pairs whose mirror image in the ``continuation'' fragment are sums. In
contrast, our language includes pairs and sums in the value fragment and two
symmetries: one that maps the pairs to fractions and another that maps the
sums to subtractions.

\paragraph*{The Duality of Computation.}
The duality between call-by-name and call-by-value was further investigated
by Selinger using control
categories~\cite{Selinger:2001:CCD:966910.966911}. Curien and
Herbelin~\cite{Curien:2000} also introduce a calculus that exhibits
symmetries between values and continuations and between call-by-value and
call-by-name. The calculus includes the type $A-B$ which is the dual of
implication, i.e., a value of type $A-B$ is a context expecting a function of
type $A \rightarrow B$. Alternatively a value of type $A-B$ is also explained
as a \emph{pair} consisting of a value of type $A$ and a continuation of type
$B$. This is to be contrasted with our interpretation of a value of that type
as \emph{either} a value of $A$ or a demand for a value of type $B$. This
calculus was further analyzed and extended by
Wadler~\cite{Wadler:2003,DBLP:conf/rta/Wadler05}. The extension gives no
interpretation to the subtraction connective and like the original symmetric
calculus of Filinksi, introduces a duality that relates sums to products and
vice-versa.

\paragraph*{Subtractive Logic.} 
Rauszer~\cite{springerlink:10.1007/BF02120864,rauszer,rauszer2} introduced a
logic which contains a dual to implication. Her work has been distilled in
the form of \emph{subtractive logic}~\cite{Crolard01} which has recently been
related to coroutines~\cite{Crolard01082004} and delimited
continuations~\cite{Ariola:2009:TFD:1743339.1743381}.  In more detail,
Crolard explains the type $A-B$ as the type of \emph{coroutines} with a local
environment of type $A$ and a continuation of type $B$. The description is
complicated by what is essentially the desire to enforce linearity
constraints so that coroutines cannot access the local environment of other
coroutines. 

\paragraph*{Negation in Classical Linear Logic} 
Filinski~\cite{Filinski92} uses the negative types of linear logic to model
continuations. Reddy~\cite{Reddy91} generalizes this idea by interpreting the
negative types of linear logic as \emph{acceptors}, which are like
continuations in the sense that they take an input and return no
output. Acceptors however are also similar in flavor to logic variables:
they can be created and instantiated later once their context of use is
determined. Although a formal connection is lacking, it is clear that, at an
intuitive level, acceptors are entities that combine elements of our negative
and fractional types.

\paragraph*{The Lambek-Grishin Calculus.} The ``parsing-as-deduction'' style
of linguistic analysis uses the Lambek-Grishin calculus with the following
types: product, left division, right division, sum, right difference, and
left difference~\cite{Bernardi:2010:CSL:1749618.1749689}. The division and
difference types are similar to our types but because the calculus lacks
commutativity and associativity and only has limited notions of
distributivity, each connective needs a left and right version. The
Lambek-Grishin exhibits two notions of symmetry but they are unrelated to our
notions. In particular, the first notion of symmetry expresses commutativity
and the second relates products to sums and divisions to subtractions. In
contrast, our two symmetries relate sums to subtractions and products to
divisions.

\paragraph*{Our Approach.} The salient aspects of our approach are the
following:
\begin{itemize}
\item Negative and fractional types have an elementary and familiar
  interpretation borrowed from the algebra of rational numbers. One can write
  any algebraic identity that is valid for the rational numbers and interpret
  it as an isomorphism with a clear computational interpretation: negative
  values flow backwards and fractional values represent constraints on the
  context. None of the systems above has such a natural interpretation of
  negative and fractional types.
\item Because we are \emph{not} in the context of the full
  $\lambda$-calculus, which allows arbitrary duplication and erasure of
  information, values of negative and fractional types are first-class values
  that can flow anywhere. The information-preserving computational
  infrastructure guarantees that, in a complete program, every negative
  demand will be satisfied exactly once, and every constraint imposed by a
  fractional value will also be satisfied exactly once. This property is
  shared with systems that are based on linear logic; other systems must
  impose ad hoc constraints to ensure negative and fractional values are used
  exactly once.
\item In contrast to all the work that takes continuations as primitive
  entities of negative types, we view continuations as a derived notion that
  combines a demand for a value with constraints on how this value will be
  used to proceed with the evaluation (to the closest delimiter or to the end
  of the program). In other words, we view a continuation as a non-elementary
  notion that combines the negative types to demand a value and the
  fractional types to explain how this value will be used to continue the
  evaluation. As a consequence, the previously observed duality between
  values and continuations can be teased into two dualities: a duality
  between values flowing in one direction or the other and a duality between
  aggregate values composing and decomposing into smaller values. Arguably
  each of the dualities is more natural than a duality that maps regular
  values to a conflated notion of negative and fractional types, and hence
  requires notions like ``additive pairs'' and ``multiplicative sums.''
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future work and Conclusion}
\label{sec:conc}

We have extended the language {{langRev}} that expressed computation
in the commutative semiring of whole numbers to {{langRevEE}} that
expresses computation in the fields of rationals.  Every algebraic
identity that holds for the rational numbers corresponds to a type
isomorphism with a computational interpretation in our model. We have
examined the two function spaces that arise in this model and
developed non-trivial constructions such as a SAT-solver that relies
on a multiplicative trace.

In another sense however, this paper is about the nature of duality in
computation. The concept of duality is deep and significant and in
many cases -- probably most famously in the divide between classical
and intuinionistic mathematics -- contain non-trivial assumptions
about our world-view.  The problem of duality in computation and
logic, has many facets of which `continuation' and `higher order
functions' are only one.  We have contributed an elegant and natural
notion of duality that arises from the monoidal structure of sums and
pairs. This notion of duality has a crisp semantics, clear
computational interpretations and an information theoretic basis.
Before we conclude, we briefly mention some closely related ideas that
warrant further study.

%% \paragraph*{Int Construction} How are we preserving two compact
%% closed structures -- what does this imply about the Int
%% construction.

\paragraph*{Geometry of Interaction.}
GoI was developed by Girard \cite{girard1989geometry}as basis for the
proof theory of Linear Logic.  GoI however has been developed into a
reversible model of computing
\cite{Mackie2011,DBLP:conf/popl/Mackie95,Abramsky:1994:NFG:184662.184664}
in which values traveling along wires, much like {{langRevEE}}.
Preliminary investigations suggest that many GoI machine constructions
can be simulated in {{langRevEE}} by treating Mackie's bi-directional
wires as pairs of wires in {{langRevEE}} and replacing the machine's
global state with a typed value on the wire that captures the
appropriate state.  However a concrete connection between GoI machines
and GoI in general requires further work. This connection is exciting
because when viewed through a Curry-Howard lens it suggests that the
logical interpretation of {{langRevEE}} should a logic much like LL
with a strong notion of resource preservation, but also with a strong
computational basis.

\paragraph*{Computing in the Field of Algebraic Numbers.}
\label{sec:algebraic-field}
The field of algebraic numbers consist of the rationals extended with
solutions to polynomials. In a limited sense {{langRevEE}} extended
with recursive types already include polynomials such as the
definition of binary tree {{@@x.(1+x*x)}} implies the equality
{{x=1+x*x}} and hence {{x^2-x+1=0}}. 

The famous example of isomorphism on algebraics is the embedding of
seven binary trees in one. In Figure 1 of Fiore's paper
\cite{Fiore:2004}, he demonstrates this isomorphism by the
corrsponding algebraic manipulation of the types.  The algebraic
manipulation can be viewed as a {{langRevEE}} combinator and it indeed
does constructively witness the isomorphism, i.e., you can run it and
view one data structure encoded by the other. In this context,
consider the following algebraically valid proof of the same
isomorphism:
\[\begin{array}{rclclclcl}
x^3 &=& x^2 x &=& (x-1) x &=& x^2 - x &=& -1 \\
x^6 &=& 1 \\
x^7 &=& x^6 x &=& x
\end{array}\]
Fiore poses the question of why such algebraic manipulation would make
sense type theoretically, even though some of the intermediate steps
make no sense.  Stated in terms of {{langRevEE}} computations, this
manipulation has no meaningful computational content -- it is always
non-terminating. It suggests that some algebraic manipulations are
somehow more `constructive' than others or dually that {{langRevEE}}
with recursive types somehow lacks the ability to express all
computations that are otherwise algebraically meaningful.

\begin{itemize}
\item Maybe the problem is that there is cancellation in the second
  case and cancellation with recursive types is problematic. However
  not all cancellation is. So what gives?

\item One idea is that irrationals correspond to an infinite amount of
  computational work. If what we are cancelling is not the same
  infinity, then things go wrong. 

\item Chaitin has already shown that reals have inifite amount of
  information which is paradoxical and problematic. This is a
  different issue from that of the rational being infinite
  computations.

\item Finally, not all meaningful types are meaningful polynomials --
  ex. nat. So this is not the whole story. 

\end{itemize}

\paragraph*{Quantum Computing} 
A \emph{time traveling} intuition is also applicable here.  In a
normal cicruit, as computational steps are taken values flow from left
to right, i.e. computational time progresses from left to right.  The
action of {{eps+}} causes values, aka information particles, to flow
from right to left i.e. backwards in computational time.  The
interesting thing about this interpretation is that particles that
travel backwards in time get to see and interact with the history of
particles that they coexist with. This gives us an intuitive
interpretation of the isomorphism {{(-b1)*b2<->-(b1*b2)}} (see
Sec. \ref{sec:specific-constructions}) where it would otherwise seem
that {{-b1}} and {{b2}} move in opposite directions. The backward flow
of the {{-b1}} allows it to `see the past' of {{b2}} and is thus
equivalent to both particles moving backward in time.

An analogy inspired by quantum mechanics is also applicable here.  The
operator {{eta*}} is a site of fission that creates \emph{an entangled
  particle and anti-particle}. These particles can be thought to be in
a \emph{superposition} of states determined by their type. Both flow
in the same direction in time.  In each possible world that the
particle exists, it takes on a specific value inhibiting its type and
correspondingly its entangled anti-particle takes the dual of the
specific value. Since the particles are entangled, actions on one
(such as transformation by application of an isomorphism) affects the
other in much the same sense as \emph{action at a distance}. The
`{{a-o*b}}' functions mentioned in the introduction, are first-class
values and they correspond to these entagled pairs. Finally {{eps*}}
operations are sites where corresponding particle and anti-particles
annihilate each other. While appealing as an analogy, this description
does not imply (or preclude) any formal connection with Physics.

Are constructions are very similar to those developed in the context
of QC. Is this how nature computes? Are the analogies to time
traveling particles and anti-particles more concrete in some way? 



\begin{comment}
  
--- END -- 


%% \begin{center}
%%   \includegraphics{diagrams/dispatch.pdf}
%% \end{center}

% \subsection{Other}

%% To summarize negative, fractional, square root, and imaginary types all make
%% sense. What they help you accomplish as a programmer is to disassociate
%% global invariants into local ones that can be satisfied independently by
%% subcomputations with no synchronization or communication. A computation
%% producing something of type $a/b$ does not need to concern itself with who is
%% going to supply the missing $b$: it just does its part. Conversely faced with
%% a complicated task, a computation might decide to break it into pieces and
%% demand these pieces using negative types. 

%% It is no surprise that these types are closely related to quantum mechanics
%% and that they give us the feel that this is how nature computes. This is
%% speculation however.

%% In any case, in a framework where information can be copied and deleted, none
%% of this makes much sense. It is critical that these constraints and demands
%% can neither be duplicated nor erased.  This gives us the maximum
%% ``parallelism'' possible.


%% Say we have not considered recursion in this paper.

%% The simplest way to connect the In a conventional computational model, one
%% might realize this situation by simply writing the identity function: the
%% buyer hands the money to the seller to finish the transaction. The above
%% sequence of isomorphisms implements this identity function in a much more
%% interesting way, however.  of the above series of

%% On the producer side, the debt is paid for by the money computation with the
%% identity function. the producer and consumer must somehow share an explicit
%% dependency that allows the value. However in our model, the presence of
%% negative types allows the produced value to satisfy the demand without the
%% producer or consumer even knowing about each other. As is explained in detail
%% in

%% Furthermore, we illustrate their true appeal and expressiveness is brought
%% forth by viewing them in the context of an information-preserving
%% computational model.

%% In addition, we show how these types enrich our computational model, they
%% obey the same laws as the rational numbers. 


%% Specifically, isomorphisms enrich our computational model with have an
%% interesting computational interpretation

In particular, linear logic~\cite{Girard87tcs}, among other contributions,
exposed an additive/multiplicative distinction in logical connectives and
rules. In particular, linear logic includes additive disjunctions $\oplus$
and conjunctions $\with$ as well as multiplicative disjunctions $\parr$ and
conjuctions $\otimes$. Duality is also prominent in linear logic: it relates
the additive connectives to each other (the dual of $\oplus$ is $\with$ and
vice-versa) and the multiplicative connectives to each other (the dual of
$\otimes$ is $\parr$ and vice-versa).

%% We furthermore demonstrate that, in our model,
%% programming with these negative and fractional types, is a new ``revolution''
%% breaking dependencies...

Since Filinski, we've had the
idea that values and continuations are like mirror images. In a conventional
language, the negative (continuation) side is implicit and we introduce
information effects on the positive. Trying to recover the duality from this
distorted positive side has always been messy. Now it looks clean because we
have kept the positive side pure.

Continuations made their introduction to the world of programming language
semantics as a mathematical device to model first-class labels and
jumps~\cite{springerlink:10.1023/A:1010026413531}. In a remarkable
development, Filinski~\cite{Filinski:1989:DCI:648332.755574} observed that
--- with the right perspective --- this highly imperative concept was
actually the symmetric dual of values. The heart of the observation is that
values represent entities that flow from producers to consumers while
continuations represent \emph{demands} for such entities, that flow from
consumers to producers. In more detail, Filinski describes continuations as
representing ``the \emph{lack} or \emph{absence} of a value, just as having a
negative amount of money represents a debt.'' He then proceeds to construct a
language where values and continuations are treated truly symmetrically. To
that end, he abandons the $\lambda$-calculus amalgamation of functions and
values and distinguishes between three different syntactic classes:
functions, values, and continuations, with the property that any function can
be used either as a value transformer or as a continuation transformer.

This highly intuitive and appealing idea was further explored and refined by
many authors~\cite{Griffin:1989:FNC:96709.96714, Curien:2000,
  Wadler:2003, DBLP:conf/rta/Wadler05}. Yet, despite its appeal, the duality
between values and continuations 

%% Computationally, symmetry exhibits itself as a duality between two concepts.

%% Symmetry is pervasive in both natural and man-made environments. 

In 1989, Filinski~\cite{Filinski:1989:DCI:648332.755574} observed that values
and continuations are dual notions. This observation was followed by numerous

%% We introduced this thesis that computation should be based on isomorphisms
%% that preserve information~\cite{infeffects}. Since Filinski, we've had the
%% idea that values and continuations are like mirror images. In a conventional
%% language, the negative (continuation) side is implicit and we introduce
%% information effects on the positive. Trying to recover the duality from this
%% distorted positive side has always been messy. Now it looks clean because we
%% have kept the positive side pure.

%% In a technical sense, this paper extends the language of isomorphisms
%% {{langRev}}, with duality. Unlike Linear logic \cite{Girard87tcs} and
%% other systems which have one notion of duality over additive and
%% multiplicative components, {{langRevEE}} has two notions of duality
%% -- an additive duality over the monoid {{(0, +)}} and a multiplicative
%% duality over the monoid {{(1, *)}}. Each axis of duality also give us
%% a function space and hence {{langRevEE}} has an additive function
%% space corresponding to a notion of control or backtracking and a
%% multiplicative function space corresponding to a notion of unification
%% or constraint satisfaction.

The world of computation we are describing has:
\begin{itemize}
\item suppliers, 
\item consumers, and
\item bi-directional transformations
\end{itemize}
This is the same world described by the papers on the duality of computation
but that work only scratched the surface! We have the following features:
\begin{itemize}
\item we can start from the supplier and push the values towards the
  consumer (call-by-value in the duality of computation papers)
\item we can start from the consumer and pull the values from the suppliers
  (call-by-name in the duality of computation papers)
\item we can combine the pushing and pulling and values using eta/epsilon for
  sum types; these allow us to at any point in the middle of the computation
  create out of nothing a value to send to the consumers and a demand to send
  to the suppliers.
\item we can break a big data structure into fragments described by
  fractional types; the suppliers and consumers can produce and consume the
  pieces completely independently of each other. Eventually the pieces will
  fit together at the consumer to produce the desired output.
\item we can break a bi-directional transformation into pieces using square
  roots
\item we can take into account that values have phase (complex numbers),
  i.e., it is not that they flow towards the consumer or just towards the
  suppliers; they can be flowing in direction that ``30 degrees'' towards the
  consumer for example.
\end{itemize}

%% So it is all about breaking dependencies in some sense to allow for maximum
%% autonomy (parallelism) of subcomputations. It is probably the case that to
%% make full use of square root types and imaginary types, we have to move to a
%% vector space. If that's the case, we should probably leave this stuff out and
%% focus on negative and fractional types and only have a short discussion of
%% the polynomials restricted to seven trees in one and similar issues.

%% The conventional idea is to divide the world into a ``real'' one and a
%% ``virtual'' one. In the ``real'' world, we can define datatypes like
%% \verb|t=t^2+1| but we don't have additive inverses so it makes no sense to
%% talk of negative types and we can't rearrange the terms in the datatype
%% definition. However the observation is that we can map these datatypes to a
%% virtual world that has more structure (a ring that provides additive inverses
%% or a field that also provides multiplicative inverses) and then perform
%% computations in the ring/field. If we perform computations in the ring, then
%% some of these will use additive inverses in ways that cannot be mapped back
%% to the ``real'' world. Much of current research attempts to characterize
%% which computations done in the ring are valid isomorphisms between datatypes
%% in the ``real'' world. This is nice but is not what I am after. In fact I am
%% not interested in the ring or the semiring at all. I am interested in the
%% field and I want this field to be \textbf{the real world.} This is partly
%% motivated by the fact that Quantum Mechanics seems to demand an underlying
%% field and more generally that the field provides the maximum generality in
%% slicing and dicing computations. So assuming I live in a field and that the
%% negative, fractional, square root, and imaginary types are all ``real,'' how
%% do I compute in this field? Clearly there will be constraints on
%% ``measurement'' in the sense that a full program cannot produce any of the
%% crazy types but that's done outside the formalism in some sense just as in
%% Quantum Computing. The main question I am after is how to compute in this
%% field with first-class negative, fractional, etc. types. As I mentioned in my
%% previous email, we can produce programs that have types \verb|t^3 <-> -1| and
%% they ``run'' (but only to give infinite loops). 

%% So when a programmer writes the datatype declaration \verb|t = t^2+1|,
%% if we allow negative etc. then this is effectively writing
%% \verb|t = cubicroot{-1}|. If we are in the field then computations
%% that manipulate these trees can be sliced and diced even at interfaces
%% that expose the cubic root and the imaginary types.

%% Future work: develop a type system for a ``normal language'' that has
%% negative, fractional, etc. types as first-class types. More long term,
%% instead of adding one polynomial at a time, we can go to an algebraically
%% closed field. The complex numbers is an obvious choice but I would rather go
%% to something computable like the field of algebraic numbers. Is the adele
%% ring or the p-adics relevant here?


We show a deep symmetry between functions and delimited continuations, values
and continuations that arises in {{langRev}} in a manner that is reminiscent
of Filinksi's Symmetric \lcal ~\cite{Filinski:1989:DCI:648332.755574}. The
symmetry arises by extending {{langRev}} with a notion of additive duality
over the monoid {{(+, 0)}} by including {{eta+}} and {{eps+}} operators of
Compact Closed Categories. The resulting dual types, which we denote {{-b}},
have a time traveling ``backward information flow'' interpretation and allow
for the encoding of higher-order function and iteration via the construction
of trace operators, thereby making the extended language {{langRevEE}} a
Turing-complete reversible programming language with higher-order functions
and first-class delimited continuations.

%% We introduced this thesis that computation should be based on isomorphisms
%% that preserve information~\cite{infeffects}. Since Filinski, we've had the
%% idea that values and continuations are like mirror images. In a conventional
%% language, the negative (continuation) side is implicit and we introduce
%% information effects on the positive. Trying to recover the duality from this
%% distorted positive side has always been messy. Now it looks clean because we
%% have kept the positive side pure.

%% The way to think about something of type $A$ is that it is a value we have
%% produced. The way to think about something of type $-A$ is that is a value we
%% have already consumed. 

Other interpretations of the types of think about. The first one is
arithmetic obviously. Another one is languages consisting of sets of
string. The type 0 is the empty set, the type 1 is the set containing the
empty word, the $+$ constructor corresponds to union, and the $*$ constructor
corresponds to concatenation. The constructor $-$ would not correspond to set
difference however. It would correspond to marking the elements in the set as
``consumed'' so that if we take the union and a ``consumed'' element appears
in the other set, the two cancel. This makes it clear that concatenating a
produced $a$ and a consumed $b$ is not the same as concatenating a consumed
$a$ and a produced $b$. They really need to be kept separate. Incidentally,
division would be defined as follows:
\[
L_1 / L_2 = \{ x ~|~ xy \in L_1 \mbox{~for~some~} y \in L_2 \}
\]

Filinski proposes that continuations are a \emph{declarative} concept. He,
furthermore, introduces a symmetric extension of the $\lambda$-calculus in
which values and continuations are treated as opposites. This is essentially
what we are proposing with one fundamental difference: our underlying
language is not the $\lambda$-calculus but a language of pure isomorphisms in
which information is preserved. This shift of perspective enables us to
distill and generalize the duality of values and continuations: in
particular, in the conventional $\lambda$-calculus setting values and
continuations can be erased and duplicated which makes it difficult to
maintain the correspondence between a value and its negative counterpart.

The idea of using negative types to model information flowing backwards,
demand for values, continuations, etc. goes back to at Filinski's thesis. We
recall these connections below but we first note that all these systems are
complicated because in all these systems information can be ignored,
destroyed, or duplicated. Clearly the possibility of erasure of information
would mean that our credit card transaction is incorrect. In our work,
information is maintained and hence we have a guarantee that, in a closed
program, the debt must be accounted and paid for.

Much of previous work builds on the idea that there is one duality in
computation between values and continuations which manifests itself as a
duality between the call-by-value and call-by-name parameter-passing
mechanisms. The former mechanism focuses on evaluating expressions to values
even if these values are not demanded by the context; the latter focuses on
evaluating expressions to continuations even if these continuations might be
aborted. The idea that a continuation is dual to a value is intuitive but
then one would expect that the sum of two values naturally corresponds to the
sum of two continuations and that the product of two values naturally
corresponds to the product of two continuations.

check and say that our work teases the continuations into a negative part
(which simply demands a value) and a fractional part (which imposes
constraints on how this value will be used). So something like $-1/c$ is
needed to express a conventional continuation. Having two dualities makes the
whole calculus natural and symmetric.

Consider a continuation that takes $x$ and $y$ and swaps them. It can't be
expressed using two conventional continuations because the demand and the way
it is used are entangled together.

In accounts that are linear, the value and continuation that comprise the
subtractive type need to be constrained to ``stay together.'' This can be
achieved by various restrictions. In this work we have no such constraints,
the negative value can flow anywhere. The entire system guarantees that any
closed program would have to account for it. We don't have to introduce
special constraints to achieve that. Zeilberger in the paper on polarity and
the logic of delimited continuations uses polarized logic: he shows that if
positive and negative values are completely symmetric except that answer
types are positive, then the framework accommodates delimited
continuations. But he interprets negative values are control operators, or as
values defined by the shape of their continuations. We simply interpret
values of negative type as values flowing in the ``other'' direction.

This is essentially what we are proposing with one fundamental difference:
our underlying language is not the $\lambda$-calculus but a language of pure
isomorphisms in which information is preserved. This shift of perspective
enables us to distill and generalize the duality of values and continuations:
in particular, in the conventional $\lambda$-calculus setting values and
continuations can be erased and duplicated which makes it difficult to
maintain the correspondence between a value and its negative counterpart. In
contrast, in our setting, one can start from the empty type $0$, introduce a
positive value and its negative counterpart, and let each of these flow in
arbitrary ways. The entire framework guarantees that neither the value nor
its negative counterpart will be deleted or duplicated and hence that, in any
closed program, the ``debt'' corresponding to the negative value is paid off
exactly once. The forward and backward executions in our framework correspond
to call-by-value and call-by-name. This duality was observed by Filinski and
others following him but it is particularly clean in our framework.

\paragraph*{Logic Programming and Backtracking.} 
This is a constrained form of backtracking and a constrained form of
logic programming.

\paragraph*{Linear Logic and GoI.} 
Say something. 

\paragraph*{Int Construction.}
For a traced monoidal category {{C}} the Int construction produces a Compact
Closed Category called Int {{C}} \cite{joyal1996traced}.  Further we know
that the target of the Int construction is isomorphic to the target of \G
construction of Abramsky \cite{Abramsky96:0} from Haghverdi. However, note
that the {{langRevEE}} is not the same as the image of the Int construction
on {{langRevT}}, since the later lacks a multiplicative tensor that
distributes over the additive tensor in Int {{langRevT}}.

once we combine the two structures, we seem to retain the monoidal
structure. How!?

\end{comment}

%% Jacques Carette, Michael Adams, Khaled, Will Byrd, Yin Wang

\acks This project was partially funded by Indiana University's Office
of the Vice President for Research and the Office of the Vice Provost
for Research through its Faculty Research Support Program.  We also
acknowledge support from Indiana University's Institute for Advanced
Study.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{small}
\bibliographystyle{abbrvnat}
\bibliography{cites}
\end{small}


\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
