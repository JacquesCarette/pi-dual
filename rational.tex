\documentclass[preprint]{sigplanconf}

\usepackage{graphicx}
\usepackage{longtable}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{mdwlist}
\usepackage{txfonts}
\usepackage{xspace}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{proof}
\usepackage{multicol}
\usepackage[nodayofweek]{datetime}
\usepackage{etex}
\usepackage[all, cmtip]{xy}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{multicol}
\usepackage{bm}
\usepackage{cmll}

%% \newtheorem{theorem}{Theorem}[section]
%% \newtheorem{lemma}[theorem]{Lemma}
%% \newtheorem{proposition}[theorem]{Proposition}
%% \newtheorem{corollary}[theorem]{Corollary}

\newcommand{\xcomment}[2]{\textbf{#1:~\textsl{#2}}}
\newcommand{\amr}[1]{\xcomment{Amr}{#1}}
\newcommand{\roshan}[1]{\xcomment{Roshan}{#1}}

\newcommand{\ie}{\textit{i.e.}\xspace}
\newcommand{\eg}{\textit{e.g.}\xspace}

\newcommand{\lcal}{\ensuremath{\lambda}-calculus}
\newcommand{\G}{\ensuremath{\mathcal{G}}\xspace}

\newcommand{\code}[1]{\lstinline[basicstyle=\small]{#1}\xspace}
\newcommand{\name}[1]{\code{#1}}

\def\newblock{}

\newenvironment{floatrule}
    {\hrule width \hsize height .33pt \vspace{.5pc}}
    {\par\addvspace{.5pc}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newenvironment{proof}[1][Proof.]{\begin{trivlist}\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\arrow}[1]{\mathtt{#1}}

%subcode-inline{bnf-inline} name langRev
%! swap+ = \mathit{swap}^+
%! swap* = \mathit{swap}^*
%! dagger =  ^{\dagger}
%! assocl+ = \mathit{assocl}^+
%! assocr+ = \mathit{assocr}^+
%! assocl* = \mathit{assocl}^*
%! assocr* = \mathit{assocr}^*
%! identr* = \mathit{uniti}
%! identl* = \mathit{unite}
%! dist = \mathit{distrib}
%! factor = \mathit{factor}
%! eta+ = \eta^+
%! eps+ = \epsilon^+
%! eta* = \eta^*
%! ^^^ = ^{-1}
%! eps* = \epsilon^*
%! (o) = \fatsemi
%! (;) = \fatsemi
%! (*) = \times
%! (+) = +

%subcode-inline{bnf-inline} regex \{\{(((\}[^\}])|[^\}])*)\}\} name main include langRev
%! Gx = \Gamma^{\times}
%! G = \Gamma
%! [] = \Box
%! |-->* = \mapsto^{*}
%! |-->> = \mapsto_{\ggg}
%! |--> = \mapsto
%! |- = \vdash
%! ==> = \Longrightarrow
%! <== = \Longleftarrow
%! <=> = \Longleftrightarrow
%! <-> = \leftrightarrow
%! ~> = \leadsto
%! -o = \multimap
%! ::= = &::=&
%! /= = \neq
%! @@ = \mu
%! forall = \forall
%! exists = \exists
%! empty = \epsilon
%! langRev = \Pi
%! langRevT = \Pi^{o}
%! langRevEE = \Pi^{\eta\epsilon}
%! theseus = Theseus
%! * = \times

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\conferenceinfo{ICFP'12}{}
\CopyrightYear{}
\copyrightdata{}
\titlebanner{}
\preprintfooter{}

\title{The Two Dualities of Computation: Negative and Fractional Types}
\authorinfo{Roshan P. James}
           {Indiana University}
           {rpjames@indiana.edu}
\authorinfo{Amr Sabry}
           {Indiana University}
           {sabry@indiana.edu}
\maketitle

\begin{abstract}
  Every functional programmer knows about sum and product types, {{a+b}} and
  {{a*b}}. Negative and fractional types, $a-b$ and $a/b$ respectively, are
  much less known and their computational interpretation is unfamiliar and
  often complicated. We show that in a programming model in which information
  is preserved (such as the model introduced in our recent paper on
  \emph{Information Effects}), these types have particularly natural
  computational interpretations. Intuitively, values of negative types are
  values that flow ``backwards'' to satisfy demands and values of fractional
  types are values that impose constraints on their context.  The combination
  of these negative and fractional types enables greater flexibility in
  programming by breaking global invariants into local ones that can be
  autonomously satisfied by a subcomputation. Furthermore, these types and
  their properties suggest that the previously observed duality of
  computation conflated two orthogonal notions: an additive duality that
  corresponds to backtracking and a multiplicative duality that corresponds
  to unification.
\end{abstract}

\category{D.3.1}{Formal Definitions and Theory}{}
\category{F.3.2}{Semantics of Programming Languages}{}
\category{F.3.3}{Studies of Program Constructs}{Type structure}

\terms
Languages, Theory

\keywords continuations, information flow, linear logic, logic programming,
quantum computing, reversible logic, symmetric monoidal categories, compact
closed categories.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

In 1989, Filinski~\cite{Filinski:1989:DCI:648332.755574} observed that values
and continuations are dual notions. This observation was followed by numerous
papers~\cite{Griffin:1989:FNC:96709.96714, curien00duality, Wadler:2003,
  DBLP:conf/rta/Wadler05} which explored and refined this duality. Even
earlier, Rauszer~\cite{springerlink:10.1007/BF02120864,rauszer,rauszer2}
introduced a logic which contains a dual to implication. Her work has been
distilled in the form of \emph{subtractive logic}~\cite{Crolard01} which has
recently been related to coroutines~\cite{Crolard01082004} and delimited
continuations~\cite{Ariola:2009:TFD:1743339.1743381}.

In a seemingly unrelated development, Girard in 1987 introduced linear
logic~\cite{Girard87tcs} which, among other contributions, exposed an
additive/multiplicative distinction in logical connectives and rules. In
particular, linear logic includes additive disjunctions $\oplus$ and
conjunctions $\with$ as well as multiplicative disjunctions $\parr$ and
conjuctions $\otimes$. Duality is also prominent in linear logic: it relates
the additive connectives to each other (the dual of $\oplus$ is $\with$ and
vice-versa) and the multiplicative connectives to each other (the dual of
$\otimes$ is $\parr$ and vice-versa).

We introduced this thesis that computation should be based on isomorphisms
that preserve information~\cite{infeffects}. Since Filinski, we've had the
idea that values and continuations are like mirror images. In a conventional
language, the negative (continuation) side is implicit and we introduce
information effects on the positive. Trying to recover the duality from this
distorted positive side has always been messy. Now it looks clean because we
have kept the positive side pure.








In a technical sense, this paper extends the language of isomorphisms
{{langRev}}, with duality. Unlike Linear logic \cite{Girard87tcs} and
other systems which have one notion of duality over additive and
multiplicative components, {{langRevEE}} has two notions of duality
-- an additive duality over the monoid {{(0, +)}} and a multiplicative
duality over the monoid {{(1, *)}}. Each axis of duality also give us
a function space and hence {{langRevEE}} has an additive function
space corresponding to a notion of control or backtracking and a
multiplicative function space corresponding to a notion of unification
or constraint satisfaction.


The world of computation we are describing has:
\begin{itemize}
\item suppliers, 
\item consumers, and
\item bi-directional transformations
\end{itemize}
This is the same world described by the papers on the duality of computation
but that work only scratched the surface! We have the following features:
\begin{itemize}
\item we can start from the supplier and push the values towards the
  consumer (call-by-value in the duality of computation papers)
\item we can start from the consumer and pull the values from the suppliers
  (call-by-name in the duality of computation papers)
\item we can combine the pushing and pulling and values using eta/epsilon for
  sum types; these allow us to at any point in the middle of the computation
  create out of nothing a value to send to the consumers and a demand to send
  to the suppliers.
\item we can break a big data structure into fragments described by
  fractional types; the suppliers and consumers can produce and consume the
  pieces completely independently of each other. Eventually the pieces will
  fit together at the consumer to produce the desired output.
\item we can break a bi-directional transformation into pieces using square
  roots
\item we can take into account that values have phase (complex numbers),
  i.e., it is not that they flow towards the consumer or just towards the
  suppliers; they can be flowing in direction that ``30 degrees'' towards the
  consumer for example.
\end{itemize}
So it is all about breaking dependencies in some sense to allow for maximum
autonomy (parallelism) of subcomputations. It is probably the case that to
make full use of square root types and imaginary types, we have to move to a
vector space. If that's the case, we should probably leave this stuff out and
focus on negative and fractional types and only have a short discussion of
the polynomials restricted to seven trees in one and similar issues.

The conventional idea is to divide the world into a ``real'' one and a
``virtual'' one. In the ``real'' world, we can define datatypes like
\verb|t=t^2+1| but we don't have additive inverses so it makes no sense to
talk of negative types and we can't rearrange the terms in the datatype
definition. However the observation is that we can map these datatypes to a
virtual world that has more structure (a ring that provides additive inverses
or a field that also provides multiplicative inverses) and then perform
computations in the ring/field. If we perform computations in the ring, then
some of these will use additive inverses in ways that cannot be mapped back
to the ``real'' world. Much of current research attempts to characterize
which computations done in the ring are valid isomorphisms between datatypes
in the ``real'' world. This is nice but is not what I am after. In fact I am
not interested in the ring or the semiring at all. I am interested in the
field and I want this field to be \textbf{the real world.} This is partly
motivated by the fact that Quantum Mechanics seems to demand an underlying
field and more generally that the field provides the maximum generality in
slicing and dicing computations. So assuming I live in a field and that the
negative, fractional, square root, and imaginary types are all ``real,'' how
do I compute in this field? Clearly there will be constraints on
``measurement'' in the sense that a full program cannot produce any of the
crazy types but that's done outside the formalism in some sense just as in
Quantum Computing. The main question I am after is how to compute in this
field with first-class negative, fractional, etc. types. As I mentioned in my
previous email, we can produce programs that have types \verb|t^3 <-> -1| and
they ``run'' (but only to give infinite loops). 

So when a programmer writes the datatype declaration \verb|t = t^2+1|, if we
allow negative etc. then this is effectively writing 
\verb|t = cubicroot{-1}|. If we are in the field then computations that 
manipulate these trees can be sliced and diced even at interfaces that
expose the cubic root and the imaginary types. 

Future work: develop a type system for a ``normal language'' that has
negative, fractional, etc. types as first-class types. More long term,
instead of adding one polynomial at a time, we can go to an algebraically
closed field. The complex numbers is an obvious choice but I would rather go
to something computable like the field of algebraic numbers. Is the adele
ring or the p-adics relevant here?

We show a deep symmetry between functions and delimited continuations,
values and continuations that arises in {{langRev}} in a manner that
is reminiscent of Filinksi's Symmetric \lcal
~\cite{Filinski:1989:DCI:648332.755574}. The symmetry arises by
extending {{langRev}} with a notion of additive duality over the
monoid {{(+, 0)}} by including {{eta}} and {{eps}} operators of
Compact Closed Categories. The resulting dual types, which we denote
{{-b}}, have a time traveling ``backward information flow''
interpretation and allow for the encoding of higher-order function and
iteration via the construction of {{trace}} operators, thereby making
the extended language {{langRevEE}} a Turing-complete reversible
programming language with higher-order functions and first-class
delimited continuations.

We introduced this thesis that computation should be based on isomorphisms
that preserve information~\cite{infeffects}. Since Filinski, we've had the
idea that values and continuations are like mirror images. In a conventional
language, the negative (continuation) side is implicit and we introduce
information effects on the positive. Trying to recover the duality from this
distorted positive side has always been messy. Now it looks clean because we
have kept the positive side pure.

The way to think about something of type $A$ is that it is a value we have
produced. The way to think about something of type $-A$ is that is a value we
have already consumed. 

Other interpretations of the types of think about. The first one is
arithmetic obviously. Another one is languages consisting of sets of
string. The type 0 is the empty set, the type 1 is the set containing the
empty word, the $+$ constructor corresponds to union, and the $*$ constructor
corresponds to concatenation. The constructor $-$ would not correspond to set
difference however. It would correspond to marking the elements in the set as
``consumed'' so that if we take the union and a ``consumed'' element appears
in the other set, the two cancel. This makes it clear that concatenating a
produced $a$ and a consumed $b$ is not the same as concatenating a consumed
$a$ and a produced $b$. They really need to be kept separate. Incidentally,
division would be defined as follows:
\[
L_1 / L_2 = \{ x ~|~ xy \in L_1 \mbox{~for~some~} y \in L_2 \}
\]

Connections to type logical grammars and Lambek calculus

Some background on {{langRev}} because it is mentioned in the next section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Field of Rational Types: Intuitions} 

%%%%%%%%%%%%%%%%%%
\subsection{Negative Types}

Consider the following two ways of purchasing an item that costs \$20.00:
\begin{enumerate}
\item You give the seller a \$20.00 bill.
\item You use a credit card to pay the seller.
\end{enumerate}
In both cases, the seller receives the money immediately but there is a
subtle difference. In the first transaction, the money received by the seller
corresponds to a value that has been produced earlier in the computation. In
the second transaction, the money may or may not exist yet: computationally,
a \emph{debt} is generated to compensate for the money received by the seller
and this debt travels ``backwards'' towards the bank where it is (hopefully)
reconciled.

The example suggests that the ability to consider values flowing backwards
enriches our computational model. This observation goes back to Filinski's
Masters thesis where continuations are identified with these negative values.
We discuss the connections to Filinski's work and others in more detail in
Sec.~\ref{sec:related}. For now, we note that in a conventional programming
language in which values can be copied and deleted at will, extreme care is
needed to keep track of negative values. Indeed, in the example above, if it
were possible to simply delete the variable corresponding to the debt, we
would have produced money out of nothing. 

For this reason, our treatment of negative values in the context of
{{langRev}} is particularly simple. We will have a type $0$ and an
isomorphism {{0 <-> a + (-a)}} that when used in the left-to-right direction
allows us to create a value and a corresponding debt out of nothing. Both the
value and its negative counterpart can flow anywhere in the system. Because
information is preserved, a closed program (which does not have any
``dangling'' negative values) will eventually have to match the negative
value with some corresponding value, effectively using the isomorphism in the
right-to-left direction. In more detail, we can model the credit card
transaction above using the following program (shown diagrammatically):

FIGURE

CLEAN UP the following based on the figure. In contrast, in our setting, one
can start from the empty type $0$, introduce a positive value and its
negative counterpart, and let each of these flow in arbitrary ways. The
entire framework guarantees that neither the value nor its negative
counterpart will be deleted or duplicated and hence that, in any closed
program, the ``debt'' corresponding to the negative value is paid off exactly
once. Computationally we model the first transaction using essentially the
identity function which receives a \$20.00 bill as input from the buyer and
propagates it on its output to the seller. The second transaction is more
complicated. We model it as shown in the circuit below: There are two ways to
understand this circuit that are both quite instructive. Let's first examine
the type structure of each of the combinators that comprise the circuit. The
first combinator on the right outputs \$20.00 to the seller. This \$20.00 is
produced from nothing so to speak by generating an equivalent debt that
travels backwards. COMPLETE BASED ON THE FIGURE. The other way is to follow
the execution. It goes forward in time so to speak, comes back, and then goes
forwards again.

It is critical that the framework in which the negative types are introduced
is a framework in which all information is preserved, with no duplication or
erasure. This guarantees that the generated debts are paid once and exactly
once.

%%%%%%%%%%%%%%%%%%%%%%
\section{Fractional Types}

The type $a/b$ is prominent in the Lambek-Grishin calculus which is
extensively used in computational linguistics. In the common interpretation,
a value of type $a/b$ is a value of some type $c$ such that when put in a
context of type $b$, the result is a value of type $a$. For example, assuming
contexts are represented as functions, a value of type $\texttt{bool} /
(\texttt{int} \rightarrow \texttt{bool})$ is simply a value of type
$\texttt{int}$. Indeed in this case, putting a value of type $\texttt{int}$
in the context $(\texttt{int} \rightarrow \texttt{bool})$ produces a value of
type $\texttt{bool}$. 

In our case, the situation is simpler. If the goal is to produce a value of
type {{a * b}} and a subcomputation can only produce the {{a}}-part, it would
have type {{(a * b) / b}}. In most settings, this type would be equivalent to
{{a}}. However in our setting, the constraint that this value must eventually
fit in a larger value that supplies the {{b}} is explicitly recorded and must
be resolved. In more detail, we have an isomorphism {{ 1 <-> a * (1/a) }}
which when used in the left-to-right direction allows the creation of a value
and a corresponding constraint on the context.  Both the value and the
constraint can flow in arbitrary ways during the computation but eventually
in a closed program with no ``dangling'' constraints, the isomorphism should
be used in the right-to-left direction to resolve the constraints.  To
understand the computational interpretation, consider the following example.

The goal is to produce a value of type $(\texttt{bool} \times
\texttt{int})$. One part of the program, $e_1$, knows how to produce the
value of type $\texttt{int}$ (say {{3}}) and another part, $e_2$, knows how
to produce the value of type $\texttt{bool}$ (say {{true}}). Computationally,
we model this as follows. The first subcomputation $e_1$ produces {{((),3)}}
and uses the isomorphism to convert {{()}} to {{(alpha,1/alpha)}} where
{{alpha}} is a yet-unknown boolean value. Reshuffling the components, $e_1$
produces {{((alpha,3), 1/alpha)}}. Similarly, $e_2$ can independently produce
{{((true,beta),1/beta)}} where {{beta}} is a yet-unknown integer value.
When the two values meet, we can group the components as follows:
\[
{{(alpha,beta)}}  {{(3,1/beta)}}  {{(true,1/alpha)}}
\]
Using the isomorphism in the right-to-left direction on the last two pairs,
forces {{alpha}} to be resolved to {{true}} and forces {{beta}} to be
resolved to {{3}}. Both of these pairs then become {{()}} and be absorbed. We
are left with the pair {{(true,3)}} as desired. 

To summarize, one can think of a value of type $a/b$ as a value that imposes
a constraint on its context of use: it is a value that requires its context
to be of type $b$ and only if that condition is satisfied, can the value be
considered as a value of type $a$. In the simplest case, the type $1/b$ just
constraints the context to be of type $b$. By allowing the fractional types
to be first-class, we can separate the generation of constraints from their
use. Both the value and the constraint can flow in arbitrary ways during the
computation but eventually in a closed program with no ``dangling''
constraints, the isomorphism should be used in the right-to-left direction to
resolve the constraints.

NEED to work out the example in detail (or perhaps another example that's
better. 

Again it is critical that the framework in which the fractional types are
introduced is a framework in which all information is preserved, with no
duplication or erasure. This guarantees that the generated constraints are
satisfied once and exactly once. 

%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary}

To summarize negative, fractional, square root, and imaginary types all make
sense. What they help you accomplish as a programmer is to disassociate
global invariants into local ones that can be satisfied independently by
subcomputations with no synchronization or communication. A computation
producing something of type $a/b$ does not need to concern itself with who is
going to supply the missing $b$: it just does its part. Conversely faced with
a complicated task, a computation might decide to break it into pieces and
demand these pieces using negative types. 

It is no surprise that these types are closely related to quantum mechanics
and that they give us the feel that this is how nature computes. This is
speculation however.

In any case, in a framework where information can be copied and deleted, none
of this makes much sense. It is critical that these constraints and demands
can neither be duplicated nor erased.  This gives us the maximum
``parallelism'' possible.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ {{langRev}} }

The presentation here differs from \cite{infeffects} in the sense that
we have introduced the {{0}} type here, thereby completing the monoid
{{(0,+)}} and we present a small-step operational semantics. 

%subcode{bnf} include main
% value types, b ::= 0 | 1 | b + b | b * b 
% values, v ::= () | left v | right v | (v, v)

%subcode{bnf} include main
%! columnStyle = r@{\hspace{-0.5pt}}c@{\hspace{-0.5pt}}l
%zeroe :&  0 + b <-> b &: zeroi
%swap+ :&  b1 + b2 <-> b2 + b1 &: swap+
%assocl+ :&  b1 + (b2 + b3) <-> (b1 + b2) + b3 &: assocr+
%identl* :&  1 * b <-> b &: identr*
%swap* :&  b1 * b2 <-> b2 * b1 &: swap*
%assocl* :&  b1 * (b2 * b3) <-> (b1 * b2) * b3 &: assocr*
%dist0 :& 0 * b <-> 0 &: factor0
%dist :&~ (b1 + b2) * b3 <-> (b1 * b3) + (b2 * b3)~ &: factor

%subcode{proof} include main
%@  ~
%@@ id : b <-> b 
%
%@ c : b1 <-> b2
%@@ sym c : b2 <-> b1
%
%@ c1 : b1 <-> b2
%@ c2 : b2 <-> b3
%@@ c1(;)c2 : b1 <-> b3
%---
%@ c1 : b1 <-> b3
%@ c2 : b2 <-> b4
%@@ c1 (+) c2 : b1 + b2 <-> b3 + b4
%
%@ c1 : b1 <-> b3
%@ c2 : b2 <-> b4
%@@ c1 (*) c2 : b1 * b2 <-> b3 * b4

\begin{definition}{(Syntax of {{langRev}})}
\label{def:langRev}
We collect our types, values, and combinators, to get the full language
definition.
%subcode{bnf} include main
% value types, b ::= 1 | b+b | b*b 
% values, v ::= () | left v | right v | (v,v) 
%
% comb.~types, t ::= b <-> b
% iso ::= swap+ | assocl+ | assocr+ 
%     &|& identl* | identr* 
%     &|& swap* | assocl* | assocr* 
%     &|& dist | factor 
% comb., c ::= iso | id | sym c | c (;) c | c (+) c | c (*) c 
\end{definition}

%%%%%%%%%%%%%%%%%%%
\subsection{Semantics}

%subcode{bnf} include main
% Combinators, c = iso | c (;) c | c (*) c | c (+) c
% Combinator Contexts, cc = [] | Fst cc c | Snd c cc 
%                  &|& LeftTimes cc c v | RightTimes c v cc 
%                  &|& LeftPlus cc c | RightPlus c cc 
% Values, v = () | (v, v) | L v | R v
%
% Machine states = <c, v, cc> | {[c, v, cc]}
% Start state = <c, v, []> 
% Stop State = {[c, v, []]}

\begin{scriptsize}
%subcode{opsem} include main
%! columnStyle = rlcl
% swap+ & (left v) &|-->& right v
% swap+ & (right v) &|-->& left v 
% assocl+ & (left v1) &|-->& left (left v1)
% assocl+ & (right (left v2)) &|-->& left (right v2)
% assocl+ & (right (right v3)) &|-->& right v3 
% assocr+ & (left (left v1)) &|-->& left v1
% assocr+ & (left (right v2)) &|-->& right (left v2)
% assocr+ & (right v3) &|-->& right (right v3)
% identl* & ((), v) &|-->& v 
% identr* & v &|-->& ((), v) 
% swap* & (v1, v2) &|-->& (v2, v1) 
% assocl* & (v1, (v2, v3)) &|-->& ((v1, v2), v3) 
% assocr* & ((v1, v2), v3) &|-->& (v1, (v2, v3)) 
% dist & (left v1, v3) &|-->& left (v1, v3)
% dist & (right v2, v3) &|-->& right (v2, v3)
% factor & (left (v1, v3)) &|-->& (left v1, v3) 
% factor & (right (v2, v3)) &|-->& (right v2, v3)   
\end{scriptsize}

%subcode{opsem} include main
%! columnStyle = rclr
% <iso, v, cc> &|-->& {[iso, iso(v), cc]}
% <c1(;)c2, v, cc> &|-->& <c1, v, Fst cc c2>
% {[c1, v, Fst cc c2]} &|-->& <c2, v, Snd c1 cc>
% {[c2, v, Snd c1 cc]} &|-->& {[ c1(;)c2, v, cc ]}
% <c1(+)c2, L v, cc> &|-->& <c1, v, LeftPlus cc c2>
% {[ c1, v, LeftPlus cc c2 ]} &|-->& {[c1 (+) c2, L v, cc ]}
% <c1(+)c2, R v, cc> &|-->& <c2, v, RightPlus c1 cc>
% {[ c2, v, RightPlus c1 cc ]} &|-->& {[c1 (+) c2, R v, cc ]}
% <c1(*)c2, (v1, v2), cc> &|-->& <c1, v1, LeftTimes cc c2 v2>
% {[ c1, v1, LeftTimes cc c2 v2 ]} &|-->& <c2, v2, RightTimes c1 v1 cc>
% {[ c2, v2, RightTimes c1 v1 cc ]} &|-->& {[ c1 (*) c2, (v1, v2), cc ]}

\begin{proposition}[Logical Reversibility]
\label{prop:logrev}
{{c v |--> v'}} iff {{c{dagger} v' |--> v}}.
\end{proposition}

\begin{proposition}[Groupoid]
\label{prop:groupoid}
{{langRev}} is a groupoid. 
\end{proposition}

\begin{proposition}
\label{prop:groupoid}
{{langRev}} is a dagger symmetric monoidal category. 
\end{proposition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ {{langRevEE}} }

\paragraph*{Observability.} 
Execution of program is defined by {{c : b1 <-> b2}} when evaluated on
input {{v1 : b1}} gives us a value {{v2 : b2}} on
termination. Execution is well defined only if {{b1}} and {{b2}} are
entirely positive types. Consider the program that

(TODO: So this is a really bad explanation. But this section is really
important and we need to explain carefully. Maybe an analogy can be
drawn to Cayley diagrams for groups to point out that the notion of
`i' is entirely artificial, but it is essential to the discourse.)

\begin{center}
  \includegraphics{diagrams/shuffle.pdf}
\end{center}

We define observables to be only positive types. The outputs of
programs that output mixed positive and negative types are not
observable.  Also, programs that input mixed positive and negatives
types are not executable.

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Additive Duality in {{langRev}} }

Additive duality is added to {{langRev}} by addition of negative types
denoted {{-b}} and the isomorphisms {{eta+}} and {{eps+}}. Hence we
extend the language as follows:

%subcode{bnf} include main
% Value Types, b = ... | -b
% Values, v = ... | -v
%
% Isomorphisms, iso &=& ... | eta+ | eps+

\noindent
with the types judgments

%subcode{opsem} include main
% eta+ &: 0 <-> (-b) + b :& eps+

%subcode{proof} include main
%@ |- v : b
%@@ |- -v : -b

We visually present {{eta+}} and {{eps+}} as `U'-shaped connectors. On
the left below is {{eta+}} showing the map from {{0}} to {{-b+b}} and
dually on the right is {{eps+}} showing the map from {{-b+b}} to
0. Even though the diagrams below show a {{0}} wire for completeness,
later diagrams will always drop them in contexts where we can
implicitly introduce/eliminate then using {{zeroi}}/{{zeroe}}.

\begin{multicols}{2}
\begin{center}
  \includegraphics{diagrams/eta.pdf}
\end{center}
  
\begin{center}
  \includegraphics{diagrams/eps.pdf}
\end{center}
\end{multicols}

The usual interpretation of the type {{b1+b2}} is that we either have
an appropriately tagged value of type {{b1}} or an appropriately
tagged value of type {{b2}}. This interpretation is maintained in the
presence of {{eta+}} and {{eps+}} in the following sense: a value of
type {{R v : -b + b}} flowing into an {{eps+}} on the lower wire
switches into a value {{L (-v): -b + b}} that flows backwards on the
upper wire. The inversion of direction is captured by the negative
sign on the value.

\paragraph*{Negative Information Flow.} 
Hence, the intuitive way to read the diagrams is that they reverse the
flow of information -- in normal {{langRev}} circuits information
flows from left-to-right and {{eps+}} operations cause information to
flow from right-to-left.  The interpretation of {{eta+}} is precisely
the opposite: it turns a right-to-left flow of information in a
circuit into a left-to-right flow.

\begin{multicols}{2}
\begin{center}
\scalebox{1.5}{
  \includegraphics{diagrams/eps_plus1.pdf}
}
\end{center}
  
\begin{center}
\scalebox{1.5}{
  \includegraphics{diagrams/eps_plus2.pdf}
}
\end{center}  
\end{multicols}

\paragraph*{Time Traveling.}
A time traveling intuition is also applicable here: if view the
left-to-right direction of information flow as the canonical forward
direction of information, then the action of {{eps+}} allows values,
aka information particles, to flow backwards in time. Particles that
flow backward in time get to see and interact with the history of
particles that they coexist with i.e. values that they exist
multiplicatively with. Hence the isomorphism {{(-b1)*b2<->-(b1*b2)}}
(whose construction is shown in Section \ref{sec:neg-constructions}).

\paragraph*{Coherence.}
The appropriate coherence condition for Compact Closed Categories,
expressed as the triangle axiom, is satisfied by {{eta+}}/{{eps+}} and
may be visualized as explained by Selinger
\cite{springerlink:10.1007/978-3-642-12821-94} as follows:

\begin{center}
  \includegraphics{diagrams/coherence.pdf}
\end{center}

\paragraph*{Non-termination.}
As is well known in category theory, Compact Closed Categories contain
trace operations.  In the context of {{langRev}} this implies that
{{eta+}}/{{eps+}} allow the construction of a {{trace+}} operator
(detailed in Section \ref{sec:monoidal-constructions}) hence
introducing \emph{non-termination} into the language
\cite{infeffects}.

\paragraph*{Zero Information.}
The fact that {{eta+}} (and hence {{eps+}}) introduces no information
can be understood is by viewing {{-b+b}} as a type whose value is
observable only if you `pay' {{b}} amount of information to the {{-b}}
branch. When {{b}} amount of information is supplied {{(-b+b)+b=b}}
amount of information is returned to you. (TODO. more needs to be said
here.)

%%%%%%%%%%%%%%%%%%%%%
\subsection{Multiplicative Duality in {{langRev}} }

%subcode{bnf} include main
% Value Types, b = ... | b^^^
% Values, v = ... | v^^^
%
% Isomorphisms, iso &=& ... | eta* | eps*

\noindent
with the types judgments

%subcode{opsem} include main
% eta* &: 1 <-> b^^^ * b :& eps*

%subcode{proof} include main
%@ |- v : b
%@@ |- v^^^ : b^^^

Note that we will freely write {{b^^^}} as {{1/b}} and {{a*b^^^}} as
{{a/b}}.

\begin{multicols}{2}
\begin{center}
  \includegraphics{diagrams/eta_times.pdf}
\end{center}
  
\begin{center}
  \includegraphics{diagrams/eps_times.pdf}
\end{center}
\end{multicols}


\paragraph*{Unification.}

test test test
\begin{multicols}{2}
\begin{center}
\scalebox{1.5}{
  \includegraphics{diagrams/eta_times1.pdf}
}
\end{center}

\begin{center}
\scalebox{1.5}{
  \includegraphics{diagrams/eps_times1.pdf}
}
\end{center}  
\end{multicols}


\paragraph*{Entanglement.}

\paragraph*{Coherence.}

\paragraph*{Annihilation.}

\paragraph*{Zero Information.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Semantics}

%%%%%%%%%%%%%%%%%%
\subsection{Syntax}

%subcode{bnf} include main
% Values, v = () | (v, v) | L v | R v
% Combinators, c &=& iso | c (;) c | c(+)c | c (*) c 
%
% Sequential Contexts, P, F = [] | P:c
% Parallel Contexts, D = [] 
%                     &|& D:(P| [](+)c| F)
%                     &|& D:(P| c(+)[]| F)
%                     &|& D:(P| [] (*) c v| F)
%                     &|& D:(P| [] (*) v c| F)
%                     &|& D:(P| c v (*)  []| F)
%                     &|& D:(P| v c (*) []| F)
%
%
% Machine States  = D[P| c v| F] | D[P| v c| F]
% Start State  = [][[]| v c| []]
% Stop State  = [][P| c v| []]

Combinator reconstruction, {{P[c]}}:
%subcode{opsem} include main
% [](c) '= c
% P:c'(c) '= P(c'(;)c)

%%%%%%%%%%%%%%%%%%%%%
\subsection{Operational Semantics}

The small step semantics present for {{langRev}} below work
symmetrically for forward and backward evaluation.

\begin{itemize}
\item 
Basic reduction of an isomorphism. Note that the evaluation leaves the
adjoint of the combinator behind. This will become important when we
reverse the direction of computation.
%% %subcode{opsem} include main
%% % <P; F; iso v; D>      &|-->& <P; F; v'~ iso{dagger}; D>

%subcode{opsem} include main
% D[P|v iso| F]      &|-->& D[P| iso{dagger} v| F]

\item
Sequencing involves pushing and popping from the Future and Past
continuations:
%subcode{opsem} include main
% D[P|v (c1(;)c2)| F]  &|-->& D[P| v c1| F:c2]
% D[P| c1 v| F:c2] &|-->& D[P:c1| v c2| F]


\item
Parallel composition, captures the current Future and Past and extends
the parallel context.
%subcode{opsem} include main
% D[P|(L v) c1 (+) c2| F] &|-->& D:(P|[](+)c2|F)[ []| v c1| [] ]
% D[P| (R v) c1 (+) c2| F] &|-->& D:(P|c1(+)[]|F)[ []| v c2| [] ]
% D:(P|[](+)c2|F)[P'|c1 v| [] ]  &|-->& D[P|(P'(c1)(+)c2{dagger}) (L v)| F] 
% D:(P|c1(+)[]|F)[P'| c2 v| [] ] &|-->& D[P|(c1{dagger}(+)P'(c2)) (R v)| F]


\item
Similarly for products:
%subcode{opsem} include main
% D[P| (v1, v2) c1 (*) c2| F] &|-->& D:(P| [] (*) v2 c2|F)[ []| v1 c1| [] ]
% D:(P|[] (*) v2 c2|F)[P'| c1 v1| [] ] &|-->& D:(P|P'(c1) v1 (*) []|F)[ []| v2 c2| [] ]
% D:(P|c1 v1(*) []|F)[P'|c2 v2| [] ]  &|-->& D[P|c1(*)P'(c2) (v1, v2)| F]


and symmetrical rules for evaluation along the second branch. 
%subcode{opsem} include main
% D:(P|v1 c1 (*) []|F)[P'| c2 v2| [] ] &|-->& D:(P,[] (*) P'(c2) v2,F)[ []| v1 c1| [] ]
% D:(P|[] (*) c2 v2|F)[P'|c1 v1| [] ] &|-->& D[P |P'(c1) (*) c2 (v1, v2)| F]

The later two rules will be relevant only for reverse execution. 

\end{itemize}

\begin{proposition}[Logical Reversibility]
\label{prop:logrev}
{{c v |--> v'}} iff {{c{dagger} v' |--> v}}.
\end{proposition}

\begin{proposition}[Groupoid]
\label{prop:groupoid}
{{langRevEE}} is a groupoid. 
\end{proposition}

\begin{proposition}
\label{prop:groupoid}
{{langRev}} is a Dagger Symmetric Monoidal Closed Category.
\end{proposition}

Note: We use `Monoidal Closed' in the sense of Compact Closed. While
the category has an multiplicative tensor that distributes over an
additive tensor, it does not have injections or projections and hence
does not have categorical sums and products.

%%%%%%%%%%%%%%%%%%%%
\subsection{Rules for {{eta}} and {{eps}} }

The operation {{eps}} reverses the direction of a particle by
reversing the world. 

Note that we deviate from the categorical definition of {{eta}} and
{{eps}} slightly in that they swap the order of {{-b}} and {{b}} in
choice of {{eta}}. This however does not affect us, because we deal
with a symmetric category.

\begin{itemize}
\item Grammar
%subcode{bnf} include main
% Values, v = () | (v, v) | L v | R v | -v
% Isomorphisms, iso &=& ... | eta | eps
% Combinators, c &=& iso | c (;) c | c(+)c | c (*) c 

\item
Type judgment.
%subcode{opsem} include main
% eta &: 0 <-> (-b) + b :& eps

%subcode{proof} include main
%@ |- v : b
%@@ |- -v : -b

\item
Operational Semantics.
%subcode{opsem} include main
% D[P| (R v) eps| F]      &|-->&  D{dagger}[F | eps (L (-v))| P]
% D[P| (L (-v)) eps| F]      &|-->&  D{dagger}[F | eps (R v)| P]

Note: there is NO reduction rule for {{eta}}. 

\item
The adjoint of a parallel context is defined to be:
%subcode{opsem} include main
% []{dagger} '= []
% (D:(P, [](+)c, F)){dagger} '= D{dagger}:(F, [](+)c{dagger}, P)
% (D:(P, c(+)[], F)){dagger} '= D{dagger}:(F, c{dagger}(+)[], P)
% (D:(P, [](*)c v, F)){dagger} '= D{dagger}:(F, [](*)v c, P)
% (D:(P, [](*)v c, F)){dagger} '= D{dagger}:(F, [](*)c v, P)
% (D:(P, c v(*)[], F)){dagger} '= D{dagger}:(F, v c(*)[], P)
% (D:(P, v c(*)[], F)){dagger} '= D{dagger}:(F, c v(*)[], P)

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Constructions in {{langRevEE}} }
\label{sec:monoidal-constructions}

These constructions are applicable to the additive and the
multiplicative monoid.

\paragraph*{Functions}

\begin{center}
  \includegraphics{diagrams/function.pdf}
\end{center}

Let us use the shorthand {{b1 -o b2 = -b1 + b2}}

\paragraph*{Function application}

\begin{center}
  \includegraphics{diagrams/apply1.pdf}
\end{center}

\begin{center}
  \includegraphics{diagrams/apply2.pdf}
\end{center}

\paragraph*{Function composition}

\begin{center}
  \includegraphics{diagrams/compose1.pdf}
\end{center}

\begin{center}
  \includegraphics{diagrams/compose.pdf}
\end{center}

This is also equivalent to sequencing both the computation blocks. 

\begin{center}
  \includegraphics{diagrams/compose2.pdf}
\end{center}


\paragraph*{Delimited continuation}

\begin{center}
  \includegraphics{diagrams/delimc.pdf}
\end{center}

\paragraph*{Trace }

%subcode{proof} include main
%@ c : b2 + b1 <-> b2 + b3
%@@ trace c : b1 <-> b3

\begin{center}
  \includegraphics{diagrams/trace.pdf}
\end{center}

\paragraph*{Principium Contradictiones}

{{b <-> -(-b)}}

\begin{center}
  \includegraphics{diagrams/double_neg.pdf}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%
\subsection{Constructions Specific to Negation}
\label{sec:neg-constructions}

\paragraph*{Negation distributes over {{+}}. }

{{-(b1 + b2) <-> (b1) + (-b2)}}

\begin{center}
  \includegraphics{diagrams/dist_neg_plus.pdf}
\end{center}

\paragraph*{ {{eps_{fst} }} }

{{(-b1)*b2 + b1*b2 <-> 0}}

\begin{center}
  \includegraphics{diagrams/eps_fst.pdf}
\end{center}

\paragraph*{Lifting negation out of {{*}}. }

{{(-b1) * b2 <-> -(b1 * b2) <-> b1 * (-b2)}}

\begin{center}
  \includegraphics{diagrams/mult_neg.pdf}
\end{center}

The following isomorphism can be constructed similarly 

{{b1 * b2 <-> (-b1)*(-b2)}}


\paragraph*{Lifting a operation of positive types to negated types}

Given {{c : b1 <-> b2}}

\begin{center}
  \includegraphics{diagrams/neg_lift.pdf}
\end{center}


%%%%%%%%%%%%%%%%%%%%%%
\subsection{Constructions Specific to Multiplication}


%%%%%%%%%%%%%%%%%%%%%%
\subsection{To think about}

\begin{itemize}

\item If we built an effect over {{langRevEE}}, say {{create}} and
  {{erase}}, are effects structured by an arrow or a monad now?

\item Which operations can be lifted to work on negative types?

\item Can the operational semantics for {{langRevEE}} be an
  interpreter that is implemented in {{langRevT}} (similar to how the
  the tree traversal interpreter was implemented). This would imply
  the existence of a more powerful construction than Int, wherein the
  products would also be preserved. This is possibly worth a paper in
  itself.

\item These functions aren't really values. There is no value one can
  produce that denotes a function. These functions are the ability to
  transform a value - the possibility of transforming a value.  In the
  product encoding of environments, there is no value one can assign
  to a variable such that it denotes a function.


Actually this is possible. Consider two functions {{f : b1 <-> b2}}
and {{g : b1 <-> b2}}. A value of type {{bool}} is sufficient to
discriminate them. Hence the {{boo}} is the first class representation
of the functions and can be thought of as the address of the function. 

\begin{center}
  \includegraphics{diagrams/dispatch.pdf}
\end{center}


\item It is not fair to say that negative types flow backwards. The
  following circuits are valid in {{langRevEE}}. It is however proper
  to say that for any type {{b}} that flows in a direction, the type
  {{-b}} flows in the reverse direction. 

\begin{center}
  \includegraphics{diagrams/neg_circuit1.pdf}
\end{center}

\begin{center}
  \includegraphics{diagrams/neg_circuit2.pdf}
\end{center}

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Advanced Programming Examples}

%%%%%%%%%%%%%%%%%%%
\subsection{SAT solver}

%%%%%%%%%%%%%%%%%%%
\subsection{GoI machine}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work} 
\label{sec:related}

(NOTE: Lets use this section here to establish a long of ideas and
intuition. None of these connections have to be formal, but they can
be concrete.)

Filinski proposes that continuations are a \emph{declarative} concept. He,
furthermore, introduces a symmetric extension of the $\lambda$-calculus in
which values and continuations are treated as opposites. This is essentially
what we are proposing with one fundamental difference: our underlying
language is not the $\lambda$-calculus but a language of pure isomorphisms in
which information is preserved. This shift of perspective enables us to
distill and generalize the duality of values and continuations: in
particular, in the conventional $\lambda$-calculus setting values and
continuations can be erased and duplicated which makes it difficult to
maintain the correspondence between a value and its negative counterpart. 

\paragraph*{Continuations.} The idea of using negative types to model
information flowing backwards, demand for values, continuations, etc. goes
back to at Filinski's thesis. We recall these connections below but we first
note that all these systems are complicated because in all these systems
information can be ignored, destroyed, or duplicated. Clearly the possibility
of erasure of information would mean that our credit card transaction is
incorrect. In our work, information is maintained and hence we have a
guarantee that, in a closed program, the debt must be accounted and paid for.

\paragraph*{Filinski~\cite{Filinski:1989:DCI:648332.755574}.}
In his Masters thesis, Filinski proposes that continuations are a
\emph{declarative} concept. He, furthermore, introduces a symmetric extension
of the $\lambda$-calculus in which values and continuations are treated as
opposites. This is essentially what we are proposing with one fundamental
difference: our underlying language is not the $\lambda$-calculus but a
language of pure isomorphisms in which information is preserved. This shift
of perspective enables us to distill and generalize the duality of values and
continuations: in particular, in the conventional $\lambda$-calculus setting
values and continuations can be erased and duplicated which makes it
difficult to maintain the correspondence between a value and its negative
counterpart. In contrast, in our setting, one can start from the empty type
$0$, introduce a positive value and its negative counterpart, and let each of
these flow in arbitrary ways. The entire framework guarantees that neither
the value nor its negative counterpart will be deleted or duplicated and
hence that, in any closed program, the ``debt'' corresponding to the negative
value is paid off exactly once. The forward and backward executions in our
framework correspond to call-by-value and call-by-name. This duality was
observed by Filinski and others following him but it is particularly clean in
our framework.

\paragraph*{Subtraction} Wadler the reloaded paper, does not consider the
subtraction type because its ``computational interpretation is not
familiar.'' Curien and Herbelin study duality in classical logic, show that
it exchanges call-by-value and call-by-name. They extend classical natural
deduction with subtraction but give it no computational meaning. Crolard (in
the formulae-as-types interpretation of subtractive logic) address the
computational interpretation of subtraction. He explains the type $A-B$ as
the of \emph{coroutines} with a local environment of type $A$ and a
continuation of type $B$. The description is complicated by what is
essentially the desire to enforce linearity constraints so that coroutines
cannot access the local environment of other coroutines. Must cite Selinger
control categories in this context of duality but I am not sure what to say:
it assumes cartesian closed categories for one thing and not symmetric
monoidal ones. Ariola, Herbelin, and Sabry also use subtractive types to
explain delimited continuations. 

\paragraph*{Linear Logic} In accounts that are linear, the value and
continuation that comprise the subtractive type need to be constrained to
``stay together.'' This can be achieved by various restrictions. In this work
we have no such constraints, the negative value can flow anywhere. The entire
system guarantees that any closed program would have to account for it. We
don't have to introduce special constraints to achieve that. Zeilberger in
the paper on polarity and the logic of delimited continuations uses polarized
logic: he shows that if positive and negative values are completely symmetric
except that answer types are positive, then the framework accommodates
delimited continuations. But he interprets negative values are control
operators, or as values defined by the shape of their continuations. We
simply interpret values of negative type as values flowing in the ``other''
direction.

\paragraph*{Int Construction.}
For a traced monoidal category {{C}} the Int construction produces a
Compact Closed Category called Int {{C}} \cite{joyal1996traced}.
Further we know that the target of the Int construction is isomorphic
to the target of \G construction of Abramsky \cite{Abramsky96:0} from
Haghverdi. However, note that the {{langRevEE}} is not the same as the
image of the Int construction on {{langRevT}}, since the the later
lacks a multiplicative tensor that distributes over the additive tensor
in Int {{langRevT}}.

\paragraph*{Recursion.} In our previous work we introduce recursive types and
trace operators. This is dangerous here because infinite loops allow us to
prolong paying the debt for as long as we want.

\paragraph*{GoI machines} 
We can now encode the GoI machine of Mackie \cite{Mackie2011,DBLP:conf/popl/Mackie95}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Field of Algebraic Numbers} 

The field of algebraic numbers consist of the rationals extended with
solutions to polynomials. In a limited sense {{langRevEE}} with
recursive types already include polynomials such as the definition of
binary tree {{@@x.(1+x*x)}} implies the equality {{t = 1 + t*t}} and
hence {{t^2-t+1=0}}. While this equation does indeed have an
irrational imaginary root, as we will see -- {{langRevEE}} does not
have the means of expressing meaningful computation that uses the
root.

Here is one way to view computations involving irrationals that might
shed some insight into how we should view recursive data types
also. For finite datatypes say of size {{n}} we can say that an
element specifies a choice of {{1/n}} and hence contains as much
information. For finite things, each value is computationally equal to
any other value. Any permutation is possible among the values. Hence
there is no metric possible on the values.  Recursive data types, such
as binary trees, have an infinite number of elements and hence the
argument that says that value specifies {{1/n}} units of information
is not longer valid. Instead to talk meaningfully about information
content, we should talk about a probability distribution on the values
of the type. However what shall we base this distribution on? Is there
a natural order beyond the unfolding order of the values of a type? 

In this context, let us go back to the thought that recursive such as
{{nat=nat+1}} have no meaningful solutions, whereas trees do. But the
interesting solution of trees is precisely that they involve
irrational numbers. Our usual notation for irrationals such as
{{2^{1/2} }} can actually be viewed as a thunk representing the
infinite computation {{1.4142..}}. Irrationals in this sense have a
very different information theoretic interpretation from full real
numbers. With Chaitin's Omega numbers he postulates the existence of
real numbers that have an infinite amount of information -- ones for
example are the solution to the question about the halting properties
of every Turning machine. 

Real numbers in that sense really are a series of bits of unknown
information theoretic interpretation -- a sort of {{top}}. Numbers
such as {{2^{1/2} }} are very different in the sense that even though
they contain an infinite sequence of bits, there is a finite program
that generates those bits and its only the cost of computing bits that
is infinite.  The consistency of arithmetic isn't affected by
extending the rational field by adding any specific computable
irrational. So it might be possible to design a model of computing
where we can indeed deal with them. Such a model will have to explicit
thunks for the delayed computation represented by trees and never
equate and cancel two infinite computations unless the represent
exactly the same infinite computation.

TODO.

Square root and imaginary types have also appeared in the literature: we
relegate the connections to Sec.~\ref{related} and proceed with a simple
explanation. We have so far extended the set of types to be the rational
numbers. Now we will push this and extend the set of types to algebraic
numbers. In other words, we will allow datatypes defined by arbitrary
polynomials and allow the roots of such polynomials to be types. 

Consider first an example in which we want to compute with the sides of a
rectangle whose area is 91 and whose length is longer than its width by 6
units. One can solve the quadratic equation to determine that the sides are 7
and 13 and proceed. This however prematurely forces us to globally solve the
constraint. Instead we can let the two sides of the rectangle be $x$ and
$x+6$ and use the following equation to capture the desired constraint:
\[
x^2 + 6x - 91 = 0
\]
The equation introduces an isomorphism between the type $x^2 + 6x - 91$ and
the type $0$. We can now proceed to compute with the unknown $x$, being
assured that in a closed program, our computation will eventually be
consistent with the solution of the algebraic equation. 

Previously the most famous example of a similar nature is the puzzling
isomorphism that one can establish between seven binary trees and one.
A binary tree is defined by the datatype
\[
x = 1 + x * x 
\]
which can be rearranged to the polynomial equation $x^2 - x + 1 = 0$. By
algebraic manipulation we can reason as follows:
\[\begin{array}{rclclclcl}
x^3 &=& x^2 x &=& (x-1) x &=& x^2 - x &=& -1 \\
x^6 &=& 1 \\
x^7 &=& x^6 x &=& x
\end{array}\]
Fiore poses the question of why such algebraic manipulation would make sense
type theoretically but states that even though some of the intermediate steps
make no sense, the final equivalence is valid and can be used to actually
construct an isomorphism between $x^7$ the type of seven binary trees 
and $x$ the type of binary trees.

Discussion of possible polynomials:
\begin{itemize}
\item $b=1+1$. Boring.
\item $b=b+1$. That introduces the natural numbers. No solution for this
  polynomial over the algebraic numbers. We must extend the numbers with
  $\omega$ to get a solution. We reject this in this paper and prefer to
  stick to algebraic numbers. The advantage is that all the isomorphisms are
  valid numerically. With the above type we could subtract $b$ from both
  sides to show that $0=1$ which is nonsense. We can however have infinite
  types as long as they have algebraic solutions.
\item $b^2=2$ or $b = \pm \sqrt{2}$. We have introduced the square root of a
  boolean! If we had superpositions, we could then write a function that
  performs the square root of negation in the sense that applying it twice
  would be equivalent to boolean negation.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

%% Jacques

\acks This project was partially funded by Indiana University's Office
of the Vice President for Research and the Office of the Vice Provost
for Research through its Faculty Research Support Program.  We also
acknowledge support from Indiana University's Institute for Advanced
Study.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{small}
\bibliographystyle{abbrvnat}
\bibliography{cites}
\end{small}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
