\documentclass{article}
\usepackage{fullpage}
\usepackage{url}
\usepackage{comment}

\title{Embracing the Laws of Physics: \\ Reversible Models of Computation}
\author{Jacques Carette, Roshan P. James, Amr Sabry}

\newcommand{\amr}[1]{\fbox{Amr says:} \textbf{#1}}
\newcommand{\jc}[1]{\fbox{Jacques says:} \textbf{#1}}
\newcommand{\roshan}[1]{\fbox{Roshan says:} \textbf{#1}}

\newcommand{\lcal}{\ensuremath{\lambda}-calculus}

\begin{document}
\maketitle

% * Reversibility intro / motivation as we  have now more or less

% * Thesis statement: programs are reversible deformations on data /
%    spaces, almost necessarily by definition

% * Focus now is on data

% * If data is plain finite sets, programs become permutations (deformations on finite sets).

% * If data is structured trees, we get Pi. 

% * Explain Pi with examples.

% * If data is itself deformations-on-finite-sets, then the
%   deformations between them become something quite
%   interesting. Explain Pi level 2 with examples.

% * Conclude with thoughts regarding other kinds of data that can be
%   plugged in into that story.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction : Reversibility, the Missing Principle}

To fully appreciate one of the principles is missing in conventional computing,
one must go back to an old thought experiment by J. C. Maxwell. This is
codified in a letter that Maxwell wrote to P. G Tait in 1867 --
the letter whose ideas are now known as `Maxwell's
Demon'. Maxwell's Demon is a thought experiment that seems to
indicate that intelligent beings can somehow violate the second law of
thermodynamics, thereby violating physics itself.

Many resolutions were offered for this conundrum (for a compilation, see
Maxwell's Demon~\cite{leff1990}), but none withstood careful
scrutiny until the establishment of `Landauer's Principle' in 1961 -- a
principle whose experimental validation
happened recently in 2012~\cite{berut2012experimental}.

Maxwell's demon appears to violate the second law of thermodynamics by
having a tiny `intelligence' observing the movement of individual
particles of a gas and separating fast moving particles from slow
moving ones, thereby reducing the total entropy of the
system. Landauer's resolution of the demon relied on two ideas that
took root only a few decades earlier -- the formal notion of
computation (through the work of Turing and Church, 1936) and the
formal notion of information (through the work of Claude Shannon,
1948). Landauer reasoned that the computation done by the finite brain
of the demon, involves getting information about the movement of
molecules, acting on that information and then overwriting it to make
room for the next computation.  Landauer reasoned, and this is the
important part, that the computation that is manipulating information
in the demon's brain \textit{must be thermodynamic work}, thereby
bringing the demon back into the fold of physics.

This is a strange and wonderful idea: Information,
physics and computation are inextricably linked. Furthermore, this
implies that ideas in each field have consequences for the other.
(Cite Bennett and the various `thermodynamics of computation' papers here.)

When the early models computation, the Turing machine, and the
$\lambda$-calculus, were developed, there was no compelling reason to the
take the information content of computations into
consideration -- in fact, at that time there was no quantifiable
notion of information. These models followed in the footsteps of logic
where conventional gates such as AND and OR happily erase their
inputs. These models were already radical enough, by being purely
constructive, in an era where classical mathematics, with its
pervasive use of excluded middle and frequent use of the axiom of
choice, did not overly worry about effective representation.

\begin{quote}
Toffoli 1980: Mathematical models of computation are abstract
constructions, by their nature unfettered by physical laws. However,
if these models are to give indications that are relevant to concrete
computing, they must somehow capture, albeit in a selective and
stylized way, certain general physical restrictions to which all
concrete computing processes are subjected.
\end{quote}

In our current age, both Landauer's Principle and recent investigations
of computational models well-suited to quantum mechanics, push us to
revisit our current models of computation. Information, on top of
having a physical manifestation, appears to be \emph{conserved},
along with other quantities like energy, mass and momentum.

But what does it mean for a computation to preserve information?
What is the missing principle that will guide the creation of a new
model of computation that ``conserve information''?
It is nature to look again at physics for inspiration --
in our current understanding of physics all of the primitive rules
are reversible. Conventional computation is not. Can
we embrace this simple principle as the building block of a model of
computation? What would computation look like when viewed from this
vantage point? Can non-trivial computation be performed in a
reversible manner? And, if so, does it have applications?

Traditional models of computation are all unityped, generally
operating on unstructured sequences of symbols. But recent
investigations, in the intersection between mathematics and
computer science (particularly in Homotopy Type Theory~\cite{HoTT-book})
reveal that not only can mathematics be typed, but that doing so
reveals further structure -- in both mathematics and in computation.

And that brings us to the principal objective of this survey: explore the
domain of \emph{typed reversible computation}.

But first, we need to better understand our current models of
computation, as a prelude to understand where they lead us astray.

\subsection{Origins}

Aspects of computation are
quite old. M\"{u}ller (1786) first conceived of the idea of a
``difference machine'', which Babbage (1819--1822) was able to
construct. There are other computer precursors as well -- the first
stored programs were actually for looms, most notably those of
Bouchon (1725) which operated on a paper tape, and Jacquard (1804)
which operated by chains of punched cards.

But it was Alan Turing's seminal work in 1936 which established the idea
that computation has a formal interpretation and that all
computability can be captured within a formal system. Implicit in this
achievement however is the idea that abstract models of computation
are just that -- \emph{abstractions of computation realized in the
physical world.}  In fact, one of the major achievements of Computer
Science has been the development of abstract models of computation
that shield the discipline from the underlying technology. As
effective as these models have been, one must note, however, that they
\emph{embody several implicit physical assumptions}.  As Tommaso
Toffoli explains in his influential 1980 paper:

{\begin{quote} Mathematical models of computation are abstract
  constructions, by their nature unfettered by physical laws. However,
  if these models are to give indications that are relevant to
  concrete computing, they must somehow capture, albeit in a selective
  and stylized way, certain general physical restrictions to which all
  concrete computing processes are subjected~\cite{Toffoli:1980}.
\end{quote}}

Our logic of programs and our hardware are based on Boolean Logic
-- which really
ought to be called \emph{Piercean logic}, as Boole had a rather different
conception of logic which is rather far from our current models.
Nevertheless, going back
to Boole's 1853 book entitled \emph{An Investigation of the Laws of Thought,
  on which are Founded the Mathematical Theories of Logic and
  Probabilities}. The opening sentence of Ch.~1 is:
\begin{quote}
  The design of the following treatise is to investigate the fundamental laws
  of those operations of the mind by which reasoning is performed; \ldots
\end{quote}
A few chapters later, we find:
\begin{quote}
  \textbf{Proposition IV.}  That axiom of metaphysicians which is termed the
  principle of contradiction, and which affirms that it is impossible for any
  being to possess a quality, and at the same time not to possess it, is a
  consequence of the fundamental law of thought, whose expression is $x^2 =
  x$.
\end{quote}
A detailed historical analysis of Boole's ideas are beyond our scope.
The above quotes, however, should convey the idea that our notions of
computation generally date back to ideas that were thought reasonable
before \textasciitilde 1900.

It is conventional to base
the theory of computation and complexity on the Turing Machine.
Going back to Turing's 1936 article \emph{On Computable Numbers, with
  an Application to the Entscheidungsproblem,}, the opening sentence of
Sec. 1 is:
\begin{quote}
  We have said that the computable numbers are those whose decimals are
  calculable by finite means \ldots the justification lies in the fact that
  the human memory is necessarily limited.
\end{quote}
In Sec. 9, we find:
\begin{quote}
I think it is reasonable to suppose that they can only be squares
whose distance from the closest of the immediately previously observed
squares does not exceed a certain fixed amount.
\end{quote}
A detailed historical account is again beyond our scope. The quotes above
should convey the ideas that our theories
of computation and complexity are based on some assumptions that
Turing found reasonable in 1936. Though it is worth noting that these assumptions
are both physical (on distances) and metaphysical (on restrictions of the
mind).  If we take the human mind to be a physical ``machine'' which does
computation, then when both of the above assumptions are translated to the
language of physics, they embody what is known as the ``Bekenstein bound:''
which is an upper limit on the amount of information that can be contained
within a given finite region of space.

One can examine more and more cases but the general story should be
clear. Our abstractions were made up based on a certain understanding of the
laws of physics. In particular, our understanding of physics has evolved
tremendously since 1900!  Thus it is time to revisit these abstractions,
especially with respect to quantum mechanics.
In the words of Girard:
\begin{quote}
  In other terms, what is so good in logic that quantum physics should obey?
  Can't we imagine that our conceptions about logic are wrong, so wrong that
  they are unable to cope with the quantum miracle?
  \\
  Instead of teaching logic to nature, it is more reasonable to learn
  from her. Instead of interpreting quantum into logic, we shall
  interpret logic into quantum (Girard 2007).
\end{quote}

\begin{comment}
\jc{The reason I commented this out is that it is under-justified. The
reader will simply not understand what these next few lines are really
saying.}
Indeed one should take the physical principles underlying quantum mechanics,
the most successful physical theory known to us and adapt computation to
``learn'' from these principles. To illustrate the depth of our crisis, Scott
Aaronson, Umesh Vazirani, and others have proposed the following puzzle.

One of these wild claims must be true!:
\begin{itemize}
\item the extended Church-Turing thesis is false, or
\item quantum physics is false, or
\item there is an efficient classical algorithm for factoring
\end{itemize}
Indeed, if quantum physics is correct then there is an efficient quantum
algorithm for factoring (Shor). If there is no efficient classical algorithm
for factoring then the extended Church-Turing thesis is false.
\end{comment}

\subsection{From classical to quantum}

Conventional classical models of computation, including boolean
circuits, the Turing machine, and the $\lambda$-calculus, are founded on
primitives which correspond to \emph{irreversible} physical processes which
gratuitously erase information.  For example, a \emph{nand} gate is an
irreversible logical operation in the sense that its inputs cannot generally
be recovered from observing its output, and so is the operation of overriding
a cell on a Turing machine tape with a new symbol, and so is a
$\beta$-reduction which typically erases or duplicates values in a way that
is destructive and irreversible.

These irreversible abstractions chosen as the basis of our
models of computing are at odds with the underlying
reversible physical reality. The information theoretic underpinnings
of physics is best understood through \emph{Landauer's
principle}~\cite{Landauer:1961}. As mentioned previously in the
introduction, Landauer, through his analysis of
Maxwell's Demon, established the idea that \emph{information is a
physical quantity} in much the sense that one would think of an
electron as a physical quantity. More precisely, his principle implies
that a certain minimum amount of
thermodynamic work has to be carried out to erase one bit of
information. Experimental verification of the Landauer principle
happened in recent years~\cite{berut2012experimental}. If information
has a physical significance, then it is subject to conservation laws
just as mass and energy are.

\subsection{conservation of information}

To make this idea concrete and to provide a taste of the applications
it opens, consider a tiny 2-bit password = \verb|"10"|. The password
checker looks like:

% verbatim for now, will go to something decent later
\begin{verbatim}
check-password (guess) =
  if guess == "10"
  then True
  else False
\end{verbatim}

One can ask how much information is leaked by this program assuming
the attacker has no prior knowledge except that the password is 2
bits, i.e., the four possible 2-bits are equally likely. If the
attacker guesses \verb|"10"| (with probability $1/4$) the password (2
bits) is leaked. If the attacker guesses one of the other choices
(with probability $3/4$) the number of possibilities is reduced from 4
to 3, i.e., the attacker learns $\log{4} - \log{3}$ bits of
information. So in general the attacker learns\footnote{If the
  password is 8 restricted ASCII characters (6 bits), the attacker
  learns 0.00001 bits in the first probe.}:
\[\begin{array}{ll}
   &  1/4 * 2 + 3/4 (\log{4} - \log{3}) \\
  =&  1/4 \log{4} + 3/4 \log{4/3} \\
  =&  - 1/4 \log{1/4} - 3/4 \log{3/4} \\
  \sim& 0.8 \mbox{~bits~in~the~first~probe}
\end{array}\]

One can alternatively look at the situation by viewing the input as a
random variable with 4 possibilities and a uniform distribution (i.e.,
with 2 bits of information). The output is another random variable
with 4 possibilities but with the distribution
$\{ (True, 1/4), (False, 3/4) \}$ which contains 0.8 bits of
information. Thus 2 input bits of information were given to the
function and only 0.8 were produced. Where did the 1.2 bits of
information go? Once we accept the thesis that information is a
physical entity this question cannot be ignored.

The Landauer Principle states that erasing information requires
energy. In other words, the password checking function must have
erased 1.2 bits during its calculation, which dissipate as heat in any
physical realization of the function. In conventional models of
computation, this erasure is \emph{implicit}, i.e., it occurs as a
side effect of calculation. We wish to look at models where
erasure is simply not allowed.

There are, in fact, many different principles of physics which are
at odds with our current models of computation. Even within quantum
mechanics itself, there are other ideas (such as superposition or
entanglement) which are not present in current models. We will however
restrict ourselves to exploring a single dimension in which our
foundations should be revised: \textbf{conservation of information}.
We will follow its consequences, which will turn out to be far reaching.

\begin{quote}
  The laws of physics are essentially algorithms for calculation. These
  algorithms are significant only to the extent that they are executable in
  our real physical world. Our usual laws of physics depend on the
  mathematician's real number system. With that comes the presumption that
  given any accuracy requirement, there exist a number of calculations steps,
  which if executed, will satisfy that accuracy requirement. But the real
  world is unlikely to supply us with unlimited memory of unlimited Turing
  machine tapes, Therefore, continuum mathematics is not executable, and
  physical laws which invoke that can not really be satisfactory. They are
  references to illusionary procedures. Rolf Landauer, The physical nature of
  information, Physics Letters A 217 (1996) pp. 188-193:
\end{quote}

A high level approach to conservation of information would proceed
as follows:
\begin{enumerate}
\item Create a means to measure information.
\item Show that this measure is sound with respect to our current understanding
of what information ought to be.
\item Assert that whatever ``quantity'' of information a system has, must
remain invariant throughout the lifetime of the system.
\end{enumerate}

Certainly, whatever our notion of information, it can neither be created,
duplicated, nor erased. Of course, if we make our notion of information
too rigid, it will be impossible to even \textbf{modify} it! For computation
to occur, modification must be possible.  But of what kind?

\jc{wavefront}

Example : $2+5 = 5+2$. Green and Red apples versus apples. What
can we observe? If we can't observe something, then we call them
the same (Leibniz).

All programs, proofs, deductions are equivalences, isomorphisms,
reversible

Several pages on reversible logic; reversible family of level 1 and
level 2 languages (Pi);

We should also have a quick survey of other works in reversible
computation? Most of the other work tries to start from well known
models of computation or well known programming languages, and then
adapts them to be reversible. Pi is different in that it starts from
the semantic implications of reversibility. Then, because it adds
types, rather than control-flow (or even data-flow) as its next layer
of ``understanding'', this leads to equivalences, isomorphisms, etc.
This then leads, quite naturally, to finding that the ``proof
language'' of semirings (and Rig Groupoids at level 2) is actually a
programming language. And it is Pi. This is a neat twist on
Curry-Howard because CH is about \textbf{inhabitation} only. But with
``conservation of information'' as the basis, a different kind of
correspondance arises; in fact, this one may well be an actual
isomorphism.

\subsection{Equalities as Isomorphisms}

Translated to the computational world, the essence of our thesis is this:
\emph{to speak of equality is to speak of computation}. This view is partly
familiar from the study of various systems of equalities, called
\emph{calculi}, as models of computation. The \lcal, for instance, is a
system of equalities over a set of syntactic $\lambda$-terms such that they
result in a model of computation. We distill this idea to its most primitive
form. We show that there is computational content not just in certain
specific calculi, such as the \lcal, but even in our most primitive notion of
equality -- isomorphisms of finite sets.

The equalities we speak of are not \emph{a priori} extensional
notions, but are intensional specifications. To talk about two things
being equal, one has to show \emph{how they are equal} wherein this
\emph{how} is an intensional definition of a translation from
witnesses of the first to witnesses of the second and vice versa,
i.e. they are programs. Equivalences that do not have operational
content do not exist for us.

One may ask, what is the point of such a rarefied spartan view of
computation? How does this compare with more conventional models of
computation?  \emph{What is the point?}

\begin{enumerate}

\item The result is computation in its purest form: Programs written
  in this model are witnesses of equalities. In the process of
  computing these programs neither gain nor lose information. We get a
  crisp notion of quantitative information content of programs and
  show that \emph{computation preserves information}.

\item From the perspective of Quantum Physics, the physical world is
  one where every fundamental interaction is essentially reversible
  and various quantities such as energy, mass, angular momentum are
  conserved. Computation is no longer at odds with
  physics. Conservation of information and logical reversibility are
  intrinsic properties of computation as well. Further, various
  category theoretic models such as dagger symmetric traced monoidal
  categories studied for Quantum Physics are also the categorical
  models for computation based on isomorphisms.

\item Irreversibility is a computational effect: By embracing
  irreversible physical primitives, models of computation have also
  implicitly included a class of computational effects which we call
  \emph{information effects}. A consequence of our careful treatment
  of information, is that we effectively capture the gap between
  reversible computation and irreversible computation using a
  type-and-effect system.

\end{enumerate}

What do we mean when we talk about intentional specification of
equality leading to an information preserving model of computation?
The equalities we talk of are processes to transform one thing to the
other. For example, if one were to believe that apples and oranges
were equal then one would have to show how any given apple can be
transformed to an orange i.e. witness of apple-hood get transformed to
witnesses of orange-hood (and vice versa). This is inherently
different from an extensional notion of equality such as
\emph{knowledge is power} and have a more operational sense. Equality
in this sense acts more like the the more general notion of
\emph{isomorphism} as probably best explained in the paper
``\emph{When is one thing equal to some other
  thing?}''~\cite{mazur2008one}.

In our formalism, we are not concerned with fruit but with very simple
things, namely numbers and sets. When we talk of a set of five apples,
we are not concerned about the individuality of the apples that
constitute the set. Rather, each apple in the set has the same status
as any other apple and we care about the individual apples only to the
extent that we can tell them apart to count five distinct ones. We
care about the abstract idea of fiveness and its equality with any
other abstract idea of fiveness.  When used as types, sets take the
role of possibility spaces -- we are no longer talking about a set of
five apples, but talking of one apple out of a possible five. This is
also where the switch from sets to information happens: When one apple
out of five is identified, it corresponds to the information to
discriminate between five choices. Clearly a choice of one out of ten
presents more information in the sense that it is a more
discriminating choice. When a computation promises an output of a
certain type, the resultant value is a witness of one choice out of
several. This is the sense in which we are concerned with equalities
of sets.

Programs in our model are built from descriptions of equalities or
sets/numbers. These are the familiar laws of arithmetic or logic:
\emph{Commutativity.} The generalized notion of swapping,
i.e. $a*b=b*a$ and $a+b=b+a$.
\emph{Associativity.} The
generalized notion of grouping, i.e. $a*(b*c)=(a*b)*c$ and
$a+(b+c)=(a+b)+c$.
\emph{Identity.} Multiplication by 1 and addition with 0,
  i.e. $a*1=a$ and $a+0=a$.
\emph{Distributivity.} Multiplication distributes over addition
  and cancels at zero, i.e. $a*(b+c)=a*b+a*c$ and $a*0=0$.
\emph{Cancellation.} Cancellation is the idea that equal things
  on both sides of a equality can be canceled out, i.e. $a+b=a+c$
  implies $b=c$, and is referred to as a \emph{trace} in category
  theory~\cite{joyal1996traced}.

How, one may ask, does this form a computational model? Even in very
simple computational models such as the SK combinatory basis, the
combinator K provides the basis of choice/conditional by deleting an
argument and the combinator S provides the basis for iteration by
duplicating an argument. As we will see, distributivity gives us
conditionals and trace gives us iteration, albeit in a logically
reversible manner.  In Chs.~\ref{ch:pi} and~\ref{ch:pi0} we work out
the details of the computational model, show that it is Turing
complete and develop programs in it including several simple numeric
operations, algebraic manipulations of trees and a meta-circular
interpreter (see Ch.~\ref{ch:interpreters}).

More formally, we develop an information preserving model of
computation, wherein the process of computing does not gain or lose
information. Our model arises from a computational interpretation of
type isomorphisms with iso-recursive types and trace. This can
equivalently be described as computing a commutative semi-ring with
cancellation or as a traced dagger symmetric bimonoidal category. The
category has a groupoid structure and all computations are logically
reversible.

To summarize, the most primitive notion of equality that is available
to us -- that of isomorphisms -- already contains the notion of
computation. The resulting computational model exposes the fine
structure of how computation handles information. We believe that such
a model of computing is foundational and we are only beginning to
understand its applications and full potential. In conclusion, let us
quote Paul Blain Levy's fine advocacy slogan: Once the fine structure
has been exposed, why ignore it?

%%%
\subsection{Information}

We can now describe information preservation in a precise sense.  Let
`$b$' be a (not necessarily finite) type whose values are labeled
$b^1, b^2, \ldots$. Let $\xi$ be a random variable of type $b$ that is
equal to $b^i$ with probability $p_i$. The entropy of $\xi$ is defined
as $-\sum p_i \log{p_i}$~\cite{Shannon1948}.  Consider a function
$f : b_1 \rightarrow b_2$ where $b_2$ is a (not necessarily finite)
type whose values are labeled $b_2^1, b_2^2, \ldots$. The output
entropy of the function is given by $- \sum q_j \log{q_j}$ where $q_j$
indicates the probability of the output of the function to have value
${b_2}^j$. We say a function is \emph{information-preserving} if its
output entropy is equal to the entropy of its input. See
Ch.~\ref{ch-metalang} for details.

Operations that change the information content of programs have a
special status. We call them \emph{information effects} and they are
encapsulated using an arrow meta-language in much the same way that
one would encapsulate other computational effects in the \lcal\ using a
monadic metalanguage.  The treatment of information effects is
analogous to open and closed systems in physics. Closed physical
systems conserve mass and energy and are the basic unit of study in
physics. Pure computations that do not have information effects are
like closed physical systems. They describe equalities.  Open systems
interact with their environment, possibly exchanging mass or
energy. These interactions may be thought of as \emph{effects} that
modify the conservation properties of the system. Computations with
information effects are much like open systems and they can be
converted into pure computations by making explicit the surrounding
information environment that they interact with.

Three things follow: (1) We capture the gap between reversible and
irreversible computation with a type-and-effect system. (2)
Categorically speaking, information effects turn monoidal categories
into ones that have a cartesian structure corresponding to
conventional computation.  (3) We show how conventional irreversible
computation such as the \lcal\ can be embedded into this model, such
that the embedding makes the implicit information effects of \lcal\
explicit.  As a consequence of this approach, many applications in
which information manipulation is computationally significant are put
within the reach of our conceptual model of computation. Such
applications include quantitative information-flow
security~\cite{myerssab}, differential
privacy~\cite{dwork:differential}, energy-aware
computing~\cite{1324180,605411}, VLSI
design~\cite{Macii:1996:ECE:874066.875828}, and biochemical models of
computation~\cite{bio}.

Ch.~\ref{ch:piee} focuses on work in progress. It deals with the
duality of computation in the setting of information
preservation. Andrzej Filinski's work on the symmetric
\lcal\ established the fascinating connection that values are dual to
continuations, functions are dual to delimited continuations and
established the duality of call-by-name and call-by-value evaluation
strategies~\cite{Filinski:1989:DCI:648332.755574}. We explore
Filinski-style duality in the setting of our information preserving
model of computing. Here duality does not appear as one De
Morgan-style dualizing $\neg$ operator, but as two distinct dualizing
operations -- negation and division. This results in a crisp semantics
for negative and fractional types and the resulting system has a
equational formalism in zero-totalized fields or
meadows~\cite{DBLP:journals/tcs/BergstraHT09}. Computational analogues
of physical phenomena such as superposition and entanglement appear in
the resulting semantics.

Talk about conservation of information in biology, computer science,
evolutionary computing, and search. (See
\url{http://www.evolutionnews.org/2012/08/conservation_of063671.html}.

%%%
\subsection{Technical Overview}

The technical development derives from ideas which are implicit in the
works of \cite{Toffoli:1980, Zuliani:2001:LR,
  malacaria2007assessing,ClarkHM07,Ghica:2007:GSS:1190216.1190269}.
(1) Functions whose output entropy is equal to their input entropy are
information preserving functions.  (2) A function
$f : b_1 \rightarrow b_2$ is \emph{logically reversible} if there
exists an inverse function $f^{\dagger}$ such that for any input
$v_1:b_1$ if there exists $v_2:b_2$ such that $f(v_1)=v_2$ \emph{iff}
$f^{\dagger}(v_2)=v_1$.  (3) Logically reversible functions are
information preserving.

To summarize:
\begin{enumerate}
\item Based on isomorphisms of sum and product
  types~\cite{Fiore:2004}, we develop a strong normalizing model of
  computing, which we call $\Pi$, that is logically reversible and
  information preserving. Every computation expressible in $\Pi$
  is an isomorphism.

\item We extend $\Pi$ with isorecursive type and trace operators from
  category theory to obtain $\Pi$. Every computation expressible in
  $\Pi$ is a partial isomorphism --- i.e. the system admits
  non-termination. $\Pi$ is Turing complete while being information
  preserving and logically reversible.

\item We develop the categorical semantics of these
  models~\cite{rc2011}. \amr{Mention n-categories.} We also develop
  a graphical notation for programming in these models that
  reminiscent of Penrose diagrams for
  categories~\cite{selinger-graphical}, Geometry of Interaction (GoI)
  machines and Proof
  Nets~\cite{Mackie2011,DBLP:conf/popl/Mackie95}. In the graphical
  notation, computation is modeled by the flow of particles in a
  circuit.

\item Since these new models are substantially different from \lcal\
  or familiar high-level languages, a point of concern is how one can
  effectively develop programs in them.  We address this by presenting
  a straightforward technique for deriving $\Pi$ programs by
  systematically translating logically reversible small-step abstract
  machines~\cite{isoint}. Complex programs can be devised in this way;
  we demonstrate the derivation of a meta-circular interpreter for
  $\Pi$.

\item We develop an arrow meta-language to encapsulate information
  effects. This metalanguage serves to encapsulate effects, in much
  the same way that traditional arrows or monads serve to encapsulate
  effects over the \lcal.

\item We show how a conventional irreversible model can be expressed
  in our model. We compile the first-order fragment of typed \lcal\
  extended with sums, products and loops to $\Pi$. This compilation is
  interesting for two reasons: (1) it exposes the implicit information
  effects of \lcal\ in the compilation to $\Pi$ and (2) the
  compilation of $\Pi$ to $\Pi$ shows that information effects can be
  erased by treating them as interactions with an explicit information
  environment.

\item We develop the notion of `duality' in $\Pi$ which gives us not
  one duality (like in linear logic or Filinski's symmetric
  \lcal~\cite{Filinski:89}), but two notions of duality -- an additive
  duality and a multiplicative duality. This gives a crisp semantics
  for negative and fractional types in the context of $\Pi$.

\end{enumerate}

Several intriguing areas of exploration remain and the book concludes
with discussion of some of these, along with other possible
applications (see Ch.~\ref{ch:discussion}).  Of these, the field of
reversible and Quantum computing is closely related and we survey
several connections in depth.  The exact connection between $\Pi$ and
linear logic~\cite{Girard87tcs} is not fully understood though it is
clear that both systems capture related but different notions of
resource usage. A closely related area is the connection with duality
of computation \cite{Filinski:89, DBLP:conf/icfp/CurienH00,
  10.1109/LICS.2010.23,Wadler:2003} --- i.e., the quest for a unifying
framework between `computations', values, continuations and their
logical counterparts. The investigation of duality gives rise to
negative and fractional types which have an intuitive semantics and
are reminiscent of negative information flow, superposition and
entanglement from Quantum Physics~\cite{piee}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Programs as Reversible Deformations} 

All programs/proofs/deductions are  
isomorphisms/equivalences/deformations.  
 
Take it as a fundamental principle; avoid detour via irreversible
functions and current models of computations.

Starting from a physical perspective, programs naturally emerge as
reversible transformations on data. In some sense, there is nothing
more to say about programs per se. The focus is on the data. Depending
on the space we use to represent the data, the corresponding notion of
deformation on these spaces will define the appropriate notion of
programs. In the following three sections, we consider three natural
classes of data and explore the corresponding notion of reversible
programs.

In the quantum case, this is clear. Say something about topological
quantum computing.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data I: Finite Sets}

Most programming languages provide a rich collection of primitive
data like booleans, characters, strings, and numbers that are
naturally modeled as sets. Infinite sets embody a notion of recursion
which is subtle and is discussed briefly in the concluding
section. For finite sets, a reversible process between them is simply
a permutation. In other words, reversible programming with finite sets
is all about writing permutations. 

We therefore start with a small language for writing permutations as a
product of transpositions:

\begin{verbatim}
p ::= id | swap i j | p;p
\end{verbatim}

Permutations on finite sets of size $n$ can be inductively defined as
follows:
\begin{verbatim}
perms(0)   = { id }
perms(n+1) = { (swap 0 i).p | p <- perms(n); i <- [1..n] }  
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data II: Structured Finite Types}

Pi level 1

sound and complete isomorphisms for finite types

name the isomorphisms

\begin{verbatim}
examples: 
  (1 + 1) x ((1 + 1) x b) = (b+b) + (b+b)
  conditionals
  toffoli
  fredkin
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data III: Reversible Programs between Structured Finite Types}

Pi level 2

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data IV: Recursive Types}

Add recursion to structured finite types, trace, feedback, loop...

Natural numbers: fold/unfold

Partial isos

meta-circular interpreter
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data V: Exclusive Disjunctions}

???

Quantum over finite field 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

Discuss recursion, or add a section of infinite sets???

CS abstractions are based on ``old'' physics

computer applications are more and more ``physical''

physical principles such as conservation of information should be part
of our foundational abstractions

quest for other principles; other data models; graphs; HoTT

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgements} many students and colleagues 

\bibliographystyle{plainnat}
%\pagestyle{headings}
\bibliography{cites}
\end{document}

