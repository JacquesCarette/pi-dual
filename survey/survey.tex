\documentclass{article}
\usepackage{fullpage}
\usepackage{url}
\usepackage{comment}

\title{Embracing the Laws of Physics: \\ Reversible Models of Computation}
\author{Jacques Carette, Roshan P. James, Amr Sabry}

\newcommand{\amr}[1]{\fbox{Amr says:} \textbf{#1}}
\newcommand{\jc}[1]{\fbox{Jacques says:} \textbf{#1}}
\newcommand{\roshan}[1]{\fbox{Roshan says:} \textbf{#1}}

\newcommand{\lcal}{\ensuremath{\lambda}-calculus}

\begin{document}
\maketitle

% * Reversibility intro / motivation as we  have now more or less

% * Thesis statement: programs are reversible deformations on data /
%    spaces, almost necessarily by definition

% * Focus now is on data

% * If data is plain finite sets, programs become permutations (deformations on finite sets).

% * If data is structured trees, we get Pi. 

% * Explain Pi with examples.

% * If data is itself deformations-on-finite-sets, then the
%   deformations between them become something quite
%   interesting. Explain Pi level 2 with examples.

% * Conclude with thoughts regarding other kinds of data that can be
%   plugged in into that story.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction : Reversibility, the Missing Principle}

To fully appreciate one of the principles is missing in conventional computing,
one must go back to an old thought experiment by J. C. Maxwell. This is
codified in a letter that Maxwell wrote to P. G Tait in 1867 --
the letter whose ideas are now known as `Maxwell's
Demon'. Maxwell's Demon is a thought experiment that seems to
indicate that intelligent beings can somehow violate the second law of
thermodynamics, thereby violating physics itself.

Many resolutions were offered for this conundrum (for a compilation, see
Maxwell's Demon~\cite{leff1990}), but none withstood careful
scrutiny until the establishment of `Landauer's Principle' in 1961 -- a
principle whose experimental validation
happened recently in 2012~\cite{berut2012experimental}.

Maxwell's demon appears to violate the second law of thermodynamics by
having a tiny `intelligence' observing the movement of individual
particles of a gas and separating fast moving particles from slow
moving ones, thereby reducing the total entropy of the
system. Landauer's resolution of the demon relied on two ideas that
took root only a few decades earlier -- the formal notion of
computation (through the work of Turing and Church, 1936) and the
formal notion of information (through the work of Claude Shannon,
1948). Landauer reasoned that the computation done by the finite brain
of the demon, involves getting information about the movement of
molecules, acting on that information and then overwriting it to make
room for the next computation.  Landauer reasoned, and this is the
important part, that the computation that is manipulating information
in the demon's brain \textit{must be thermodynamic work}, thereby
bringing the demon back into the fold of physics.

This is a strange and wonderful idea: Information,
physics and computation are inextricably linked. Furthermore, this
implies that ideas in each field have consequences for the other.
(Cite Bennett and the various `thermodynamics of computation' papers here.)

When the early models computation, the Turing machine, and the
$\lambda$-calculus, were developed, there was no compelling reason to the
take the information content of computations into
consideration -- in fact, at that time there was no quantifiable
notion of information. These models followed in the footsteps of logic
where conventional gates such as AND and OR happily erase their
inputs. These models were already radical enough, by being purely
constructive, in an era where classical mathematics, with its
pervasive use of excluded middle and frequent use of the axiom of
choice, did not overly worry about effective representation.

\begin{quote}
Toffoli 1980: Mathematical models of computation are abstract
constructions, by their nature unfettered by physical laws. However,
if these models are to give indications that are relevant to concrete
computing, they must somehow capture, albeit in a selective and
stylized way, certain general physical restrictions to which all
concrete computing processes are subjected.
\end{quote}

In our current age, both Landauer's Principle and recent investigations
of computational models well-suited to quantum mechanics, push us to
revisit our current models of computation. Information, on top of
having a physical manifestation, appears to be \emph{conserved},
along with other quantities like energy, mass and momentum.

But what does it mean for a computation to preserve information?
What is the missing principle that will guide the creation of a new
model of computation that ``conserve information''?
It is nature to look again at physics for inspiration --
in our current understanding of physics all of the primitive rules
are reversible. Conventional computation is not. Can
we embrace this simple principle as the building block of a model of
computation? What would computation look like when viewed from this
vantage point? Can non-trivial computation be performed in a
reversible manner? And, if so, does it have applications?

Traditional models of computation are all unityped, generally
operating on unstructured sequences of symbols. But recent
investigations, in the intersection between mathematics and
computer science (particularly in Homotopy Type Theory~\cite{HoTT-book})
reveal that not only can mathematics be typed, but that doing so
reveals further structure -- in both mathematics and in computation.

And that brings us to the principal objective of this survey: explore the
domain of \emph{typed reversible computation}.

But first, we need to better understand our current models of
computation, as a prelude to understand where they lead us astray.

\subsection{Origins}

Aspects of computation are
quite old. M\"{u}ller (1786) first conceived of the idea of a
``difference machine'', which Babbage (1819--1822) was able to
construct. There are other computer precursors as well -- the first
stored programs were actually for looms, most notably those of
Bouchon (1725) which operated on a paper tape, and Jacquard (1804)
which operated by chains of punched cards.

But it was Alan Turing's seminal work in 1936 which established the idea
that computation has a formal interpretation and that all
computability can be captured within a formal system. Implicit in this
achievement however is the idea that abstract models of computation
are just that -- \emph{abstractions of computation realized in the
physical world.}  In fact, one of the major achievements of Computer
Science has been the development of abstract models of computation
that shield the discipline from the underlying technology. As
effective as these models have been, one must note, however, that they
\emph{embody several implicit physical assumptions}.  As Tommaso
Toffoli explains in his influential 1980 paper:

{\begin{quote} Mathematical models of computation are abstract
  constructions, by their nature unfettered by physical laws. However,
  if these models are to give indications that are relevant to
  concrete computing, they must somehow capture, albeit in a selective
  and stylized way, certain general physical restrictions to which all
  concrete computing processes are subjected~\cite{Toffoli:1980}.
\end{quote}}

Our logic of programs and our hardware are based on Boolean Logic
-- which really
ought to be called \emph{Piercean logic}, as Boole had a rather different
conception of logic which is rather far from our current models.
Nevertheless, going back
to Boole's 1853 book entitled \emph{An Investigation of the Laws of Thought,
  on which are Founded the Mathematical Theories of Logic and
  Probabilities}. The opening sentence of Ch.~1 is:
\begin{quote}
  The design of the following treatise is to investigate the fundamental laws
  of those operations of the mind by which reasoning is performed; \ldots
\end{quote}
A few chapters later, we find:
\begin{quote}
  \textbf{Proposition IV.}  That axiom of metaphysicians which is termed the
  principle of contradiction, and which affirms that it is impossible for any
  being to possess a quality, and at the same time not to possess it, is a
  consequence of the fundamental law of thought, whose expression is $x^2 =
  x$.
\end{quote}
A detailed historical analysis of Boole's ideas are beyond our scope.
The above quotes, however, should convey the idea that our notions of
computation generally date back to ideas that were thought reasonable
before \textasciitilde 1900.

It is conventional to base
the theory of computation and complexity on the Turing Machine.
Going back to Turing's 1936 article \emph{On Computable Numbers, with
  an Application to the Entscheidungsproblem,}, the opening sentence of
Sec. 1 is:
\begin{quote}
  We have said that the computable numbers are those whose decimals are
  calculable by finite means \ldots the justification lies in the fact that
  the human memory is necessarily limited.
\end{quote}
In Sec. 9, we find:
\begin{quote}
I think it is reasonable to suppose that they can only be squares
whose distance from the closest of the immediately previously observed
squares does not exceed a certain fixed amount.
\end{quote}
A detailed historical account is again beyond our scope. The quotes above
should convey the ideas that our theories
of computation and complexity are based on some assumptions that
Turing found reasonable in 1936. Though it is worth noting that these assumptions
are both physical (on distances) and metaphysical (on restrictions of the
mind).  If we take the human mind to be a physical ``machine'' which does
computation, then when both of the above assumptions are translated to the
language of physics, they embody what is known as the ``Bekenstein bound:''
which is an upper limit on the amount of information that can be contained
within a given finite region of space.

One can examine more and more cases but the general story should be
clear. Our abstractions were made up based on a certain understanding of the
laws of physics. In particular, our understanding of physics has evolved
tremendously since 1900!  Thus it is time to revisit these abstractions,
especially with respect to quantum mechanics.
In the words of Girard:
\begin{quote}
  In other terms, what is so good in logic that quantum physics should obey?
  Can't we imagine that our conceptions about logic are wrong, so wrong that
  they are unable to cope with the quantum miracle?
  \\
  Instead of teaching logic to nature, it is more reasonable to learn
  from her. Instead of interpreting quantum into logic, we shall
  interpret logic into quantum (Girard 2007).
\end{quote}

\begin{comment}
\jc{The reason I commented this out is that it is under-justified. The
reader will simply not understand what these next few lines are really
saying.}
Indeed one should take the physical principles underlying quantum mechanics,
the most successful physical theory known to us and adapt computation to
``learn'' from these principles. To illustrate the depth of our crisis, Scott
Aaronson, Umesh Vazirani, and others have proposed the following puzzle.

One of these wild claims must be true!:
\begin{itemize}
\item the extended Church-Turing thesis is false, or
\item quantum physics is false, or
\item there is an efficient classical algorithm for factoring
\end{itemize}
Indeed, if quantum physics is correct then there is an efficient quantum
algorithm for factoring (Shor). If there is no efficient classical algorithm
for factoring then the extended Church-Turing thesis is false.
\end{comment}

\subsection{From classical to quantum}

Conventional classical models of computation, including boolean
circuits, the Turing machine, and the $\lambda$-calculus, are founded on
primitives which correspond to \emph{irreversible} physical processes which
gratuitously erase information.  For example, a \emph{nand} gate is an
irreversible logical operation in the sense that its inputs cannot generally
be recovered from observing its output, and so is the operation of overriding
a cell on a Turing machine tape with a new symbol, and so is a
$\beta$-reduction which typically erases or duplicates values in a way that
is destructive and irreversible.

These irreversible abstractions chosen as the basis of our
models of computing are at odds with the underlying
reversible physical reality. The information theoretic underpinnings
of physics is best understood through \emph{Landauer's
principle}~\cite{Landauer:1961}. As mentioned previously in the
introduction, Landauer, through his analysis of
Maxwell's Demon, established the idea that \emph{information is a
physical quantity} in much the sense that one would think of an
electron as a physical quantity. More precisely, his principle implies
that a certain minimum amount of
thermodynamic work has to be carried out to erase one bit of
information. Experimental verification of the Landauer principle
happened in recent years~\cite{berut2012experimental}. If information
has a physical significance, then it is subject to conservation laws
just as mass and energy are.

\subsection{conservation of information}

To make this idea concrete and to provide a taste of the applications
it opens, consider a tiny 2-bit password = \verb|"10"|. The password
checker looks like:

% verbatim for now, will go to something decent later
\begin{verbatim}
check-password (guess) =
  if guess == "10"
  then True
  else False
\end{verbatim}

One can ask how much information is leaked by this program assuming
the attacker has no prior knowledge except that the password is 2
bits, i.e., the four possible 2-bits are equally likely. If the
attacker guesses \verb|"10"| (with probability $1/4$) the password (2
bits) is leaked. If the attacker guesses one of the other choices
(with probability $3/4$) the number of possibilities is reduced from 4
to 3, i.e., the attacker learns $\log{4} - \log{3}$ bits of
information. So in general the attacker learns\footnote{If the
  password is 8 restricted ASCII characters (6 bits), the attacker
  learns 0.00001 bits in the first probe.}:
\[\begin{array}{ll}
   &  1/4 * 2 + 3/4 (\log{4} - \log{3}) \\
  =&  1/4 \log{4} + 3/4 \log{4/3} \\
  =&  - 1/4 \log{1/4} - 3/4 \log{3/4} \\
  \sim& 0.8 \mbox{~bits~in~the~first~probe}
\end{array}\]

One can alternatively look at the situation by viewing the input as a
random variable with 4 possibilities and a uniform distribution (i.e.,
with 2 bits of information). The output is another random variable
with 4 possibilities but with the distribution
$\{ (True, 1/4), (False, 3/4) \}$ which contains 0.8 bits of
information. Thus 2 input bits of information were given to the
function and only 0.8 were produced. Where did the 1.2 bits of
information go? Once we accept the thesis that information is a
physical entity this question cannot be ignored.

The Landauer Principle states that erasing information requires
energy. In other words, the password checking function must have
erased 1.2 bits during its calculation, which dissipate as heat in any
physical realization of the function. In conventional models of
computation, this erasure is \emph{implicit}, i.e., it occurs as a
side effect of calculation. We wish to look at models where
erasure is simply not allowed.

There are, in fact, many different principles of physics which are
at odds with our current models of computation. Even within quantum
mechanics itself, there are other ideas (such as superposition or
entanglement) which are not present in current models. We will however
restrict ourselves to exploring a single dimension in which our
foundations should be revised: \textbf{conservation of information}.
We will follow its consequences, which will turn out to be far reaching.

\begin{quote}
  The laws of physics are essentially algorithms for calculation. These
  algorithms are significant only to the extent that they are executable in
  our real physical world. Our usual laws of physics depend on the
  mathematician's real number system. With that comes the presumption that
  given any accuracy requirement, there exist a number of calculations steps,
  which if executed, will satisfy that accuracy requirement. But the real
  world is unlikely to supply us with unlimited memory of unlimited Turing
  machine tapes, Therefore, continuum mathematics is not executable, and
  physical laws which invoke that can not really be satisfactory. They are
  references to illusionary procedures. Rolf Landauer, The physical nature of
  information, Physics Letters A 217 (1996) pp. 188-193:
\end{quote}

A high level approach to conservation of information would proceed
as follows:
\begin{enumerate}
\item Create a means to measure information.
\item Show that this measure is sound with respect to our current understanding
of what information ought to be.
\item Assert that whatever ``quantity'' of information a system has, must
remain invariant throughout the lifetime of the system.
\end{enumerate}

Certainly, whatever our notion of information, it can neither be created,
duplicated, nor erased. Of course, if we make our notion of information
too rigid, it will be impossible to even \textbf{modify} it! For computation
to occur, modification must be possible.  But of what kind?

\amr{We should also have a quick survey of other works in reversible
computation? Most of the other work tries to start from well known
models of computation or well known programming languages, and then
adapts them to be reversible. Pi is different in that it starts from
the semantic implications of reversibility. Then, because it adds
types, rather than control-flow (or even data-flow) as its next layer
of ``understanding'', this leads to equivalences, isomorphisms, etc.
This then leads, quite naturally, to finding that the ``proof
language'' of semirings (and Rig Groupoids at level 2) is actually a
programming language. And it is Pi. This is a neat twist on
Curry-Howard because CH is about \textbf{inhabitation} only. But with
``conservation of information'' as the basis, a different kind of
correspondance arises; in fact, this one may well be an actual
isomorphism.}

\subsection{Information}

We can now give a first precise definition of information preservation.
Let
`$b$' be a (not necessarily finite) type whose values are labeled
$b^1, b^2, \ldots$. Let $\xi$ be a random variable of type $b$ that is
equal to $b^i$ with probability $p_i$. The entropy of $\xi$ is defined
as $-\sum p_i \log{p_i}$~\cite{Shannon1948}.  Consider a function
$f : b_1 \rightarrow b_2$ where $b_2$ is a (not necessarily finite)
type whose values are labeled $b_2^1, b_2^2, \ldots$. The output
entropy of the function is given by $- \sum q_j \log{q_j}$ where $q_j$
indicates the probability of the output of the function to have value
${b_2}^j$. We say a function is \emph{information-preserving} if its
output entropy is equal to the entropy of its input. In later sections,
we will refine this view.

\amr{
Talk about conservation of information in biology, computer science,
evolutionary computing, and search. (See
\url{http://www.evolutionnews.org/2012/08/conservation_of063671.html}.
}

%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Programs as Reversible Deformations} 

All programs/proofs/deductions are  
isomorphisms/equivalences/deformations.  
 
Take it as a fundamental principle; avoid detour via irreversible
functions and current models of computations.

Starting from a physical perspective, programs naturally emerge as
reversible transformations on data. In some sense, there is nothing
more to say about programs per se. The focus is on the data. Depending
on the space we use to represent the data, the corresponding notion of
deformation on these spaces will define the appropriate notion of
programs. In the following three sections, we consider three natural
classes of data and explore the corresponding notion of reversible
programs.

In the quantum case, this is clear. Say something about topological
quantum computing.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data I: Finite Sets}

Most programming languages provide a rich collection of primitive
data like booleans, characters, strings, and numbers that are
naturally modeled as sets. Infinite sets embody a notion of recursion
which is subtle and is discussed briefly in the concluding
section. For finite sets, a reversible process between them is simply
a permutation. In other words, reversible programming with finite sets
is all about writing permutations. 

We therefore start with a small language for writing permutations as a
product of transpositions:

\begin{verbatim}
p ::= id | swap i j | p;p
\end{verbatim}

Permutations on finite sets of size $n$ can be inductively defined as
follows:
\begin{verbatim}
perms(0)   = { id }
perms(n+1) = { (swap 0 i).p | p <- perms(n); i <- [1..n] }  
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data II: Structured Finite Types}

Pi level 1

sound and complete isomorphisms for finite types

name the isomorphisms

\begin{verbatim}
examples: 
  (1 + 1) x ((1 + 1) x b) = (b+b) + (b+b)
  conditionals
  toffoli
  fredkin
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data III: Reversible Programs between Structured Finite Types}

Pi level 2

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data IV: Recursive Types}

Add recursion to structured finite types, trace, feedback, loop...

Natural numbers: fold/unfold

Partial isos

meta-circular interpreter
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data V: Exclusive Disjunctions}

???

Quantum over finite field 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

Discuss recursion, or add a section of infinite sets???

CS abstractions are based on ``old'' physics

computer applications are more and more ``physical''

physical principles such as conservation of information should be part
of our foundational abstractions

quest for other principles; other data models; graphs; HoTT

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgements} many students and colleagues 

\bibliographystyle{plainnat}
%\pagestyle{headings}
\bibliography{cites}
\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Below are pieces of text that are not immediately useful above, but
%% from which we might still be able to pilfer usefully.
\subsection{Technical Overview}

The technical development derives from ideas which are implicit in the
works of \cite{Toffoli:1980, Zuliani:2001:LR,
  malacaria2007assessing,ClarkHM07,Ghica:2007:GSS:1190216.1190269}.
(1) Functions whose output entropy is equal to their input entropy are
information preserving functions.  (2) A function
$f : b_1 \rightarrow b_2$ is \emph{logically reversible} if there
exists an inverse function $f^{\dagger}$ such that for any input
$v_1:b_1$ if there exists $v_2:b_2$ such that $f(v_1)=v_2$ \emph{iff}
$f^{\dagger}(v_2)=v_1$.  (3) Logically reversible functions are
information preserving.

To summarize:
\begin{enumerate}
\item Based on isomorphisms of sum and product
  types~\cite{Fiore:2004}, we develop a strong normalizing model of
  computing, which we call $\Pi$, that is logically reversible and
  information preserving. Every computation expressible in $\Pi$
  is an isomorphism.

\item We extend $\Pi$ with isorecursive type and trace operators from
  category theory to obtain $\Pi$. Every computation expressible in
  $\Pi$ is a partial isomorphism --- i.e. the system admits
  non-termination. $\Pi$ is Turing complete while being information
  preserving and logically reversible.

\item We develop the categorical semantics of these
  models~\cite{rc2011}. \amr{Mention n-categories.} We also develop
  a graphical notation for programming in these models that
  reminiscent of Penrose diagrams for
  categories~\cite{selinger-graphical}, Geometry of Interaction (GoI)
  machines and Proof
  Nets~\cite{Mackie2011,DBLP:conf/popl/Mackie95}. In the graphical
  notation, computation is modeled by the flow of particles in a
  circuit.

\item Since these new models are substantially different from \lcal\
  or familiar high-level languages, a point of concern is how one can
  effectively develop programs in them.  We address this by presenting
  a straightforward technique for deriving $\Pi$ programs by
  systematically translating logically reversible small-step abstract
  machines~\cite{isoint}. Complex programs can be devised in this way;
  we demonstrate the derivation of a meta-circular interpreter for
  $\Pi$.

\item We develop an arrow meta-language to encapsulate information
  effects. This metalanguage serves to encapsulate effects, in much
  the same way that traditional arrows or monads serve to encapsulate
  effects over the \lcal.

\item We show how a conventional irreversible model can be expressed
  in our model. We compile the first-order fragment of typed \lcal\
  extended with sums, products and loops to $\Pi$. This compilation is
  interesting for two reasons: (1) it exposes the implicit information
  effects of \lcal\ in the compilation to $\Pi$ and (2) the
  compilation of $\Pi$ to $\Pi$ shows that information effects can be
  erased by treating them as interactions with an explicit information
  environment.

\item We develop the notion of `duality' in $\Pi$ which gives us not
  one duality (like in linear logic or Filinski's symmetric
  \lcal~\cite{Filinski:89}), but two notions of duality -- an additive
  duality and a multiplicative duality. This gives a crisp semantics
  for negative and fractional types in the context of $\Pi$.

\end{enumerate}

Several intriguing areas of exploration remain and the book concludes
with discussion of some of these, along with other possible
applications (see Ch.~\ref{ch:discussion}).  Of these, the field of
reversible and Quantum computing is closely related and we survey
several connections in depth.  The exact connection between $\Pi$ and
linear logic~\cite{Girard87tcs} is not fully understood though it is
clear that both systems capture related but different notions of
resource usage. A closely related area is the connection with duality
of computation \cite{Filinski:89, DBLP:conf/icfp/CurienH00,
  10.1109/LICS.2010.23,Wadler:2003} --- i.e., the quest for a unifying
framework between `computations', values, continuations and their
logical counterparts. The investigation of duality gives rise to
negative and fractional types which have an intuitive semantics and
are reminiscent of negative information flow, superposition and
entanglement from Quantum Physics~\cite{piee}.


