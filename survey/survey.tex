\documentclass{article}
\usepackage{amssymb,amsthm,amsmath}
\usepackage{fullpage}
\usepackage{url}
\usepackage{comment}
\usepackage{tikz}
\usepackage{tikz-cd}
\usetikzlibrary{quotes}
\tikzset{>=latex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Macros

\newcommand{\nboxtimes}[2]{\,\,~{^{#1}\boxtimes^{#2}}~\,\,}
\newcommand{\mm}{\texttt{\textminus}}
\newcommand{\pp}{\texttt{+}}
\newcommand{\inl}[1]{\textsf{inl}~#1}
\newcommand{\inr}[1]{\textsf{inr}~#1}
\newcommand{\idv}[3]{#2 \xrightarrow{#1} #3}
\newcommand{\cp}[3]{#1\stackrel{#2}{\bullet}#3}
\newcommand{\idt}[3]{#2 \equiv_{#1} #3}
\newcommand{\idrt}[3]{#3 \equiv_{#1} #2}
\newcommand{\refl}[1]{\textsf{refl}~#1}
%% \newcommand{\lid}{\textsf{lid}}
\newcommand{\alt}{~|~}
%% \newcommand{\rid}{\textsf{rid}}
\newcommand{\linv}{l!}
\newcommand{\rinv}{r!}
\newcommand{\invinv}{!!}
\newcommand{\assoc}{\circ}
\newcommand{\identlp}{\mathit{unite}_+\mathit{l}}
\newcommand{\identrp}{\mathit{uniti}_+\mathit{l}}
\newcommand{\identlsp}{\mathit{unite}_+\mathit{r}}
\newcommand{\identrsp}{\mathit{uniti}_+\mathit{r}}
\newcommand{\swapp}{\mathit{swap}_+}
\newcommand{\assoclp}{\mathit{assocl}_+}
\newcommand{\assocrp}{\mathit{assocr}_+}
\newcommand{\identlt}{\mathit{unite}_*\mathit{l}}
\newcommand{\identrt}{\mathit{uniti}_*\mathit{l}}
\newcommand{\identlst}{\mathit{unite}_*\mathit{r}}
\newcommand{\identrst}{\mathit{uniti}_*\mathit{r}}
\newcommand{\swapt}{\mathit{swap}_*}
\newcommand{\assoclt}{\mathit{assocl}_*}
\newcommand{\assocrt}{\mathit{assocr}_*}
\newcommand{\absorbr}{\mathit{absorbr}}
\newcommand{\absorbl}{\mathit{absorbl}}
\newcommand{\factorzr}{\mathit{factorzr}}
\newcommand{\factorzl}{\mathit{factorzl}}
\newcommand{\dist}{\mathit{dist}}
\newcommand{\factor}{\mathit{factor}}
\newcommand{\distl}{\mathit{distl}}
\newcommand{\factorl}{\mathit{factorl}}
\newcommand{\distz}{\mathit{absorbr}}
\newcommand{\iso}{\leftrightarrow}
\newcommand{\proves}{\vdash}
\newcommand{\idc}{\mathit{id}\!\!\leftrightarrow}
\newcommand{\Rule}[4]{
\makebox{{\rm #1}
$\displaystyle
\frac{\begin{array}{l}#2 \\\end{array}}
{\begin{array}{l}#3      \\\end{array}}$
 #4}}
\newcommand{\jdg}[3]{#2 \proves_{#1} #3}


\title{Embracing the Laws of Physics: \\ Three Reversible Models of Computation}
\author{Jacques Carette \qquad\qquad Roshan P. James \qquad\qquad Amr Sabry \\
McMaster University \qquad\qquad Google \qquad\qquad Indiana University}

\newtheorem{defn}{Definition}[section]
\newtheorem{prop}{Proposition}[section]

\newcommand{\amr}[1]{\fbox{Amr says:} \textbf{#1}}
\newcommand{\jc}[1]{\fbox{Jacques says:} \textbf{#1}}
\newcommand{\roshan}[1]{\fbox{Roshan says:} \textbf{#1}}

\newcommand{\lcal}{\ensuremath{\lambda}-calculus}

\newcommand{\fin}[1]{\ensuremath{\left[#1\right]}}
\newcommand{\Nat}{\ensuremath{\mathbb{N}}}
\newcommand{\true}{\mathit{true}}
\newcommand{\false}{\mathit{false}}

\begin{document}
\maketitle

% * Reversibility intro / motivation as we  have now more or less

% * Thesis statement: programs are reversible deformations on data /
%    spaces, almost necessarily by definition

% * Focus now is on data

% * If data is plain finite sets, programs become permutations (deformations on finite sets).

% * If data is structured trees, we get Pi. 

% * Explain Pi with examples.

% * If data is itself deformations-on-finite-sets, then the
%   deformations between them become something quite
%   interesting. Explain Pi level 2 with examples.

% * Conclude with thoughts regarding other kinds of data that can be
%   plugged in into that story.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reversibility, the Missing Principle}

What kind of operations can computers perform? This question was
answered several times in the last hundred years, where each answer
proposes an abstract \emph{model of computation} that specifies
allowable operations and (usually) their cost. The emerging consensus,
reflected in both early models of computations such as the Turing
Machine and the RAM as well as in the early Von Neumann and in modern
computer architectures, is that basic computer operations include
logical operations like conjunction, disjunction, and negation, as
well as reading from and writing to a large (infinite) collection of
memory locations.

No doubt, this consensus has been successful. And no doubt, these
operations \emph{can} indeed be performed on a computer. Yet, today,
with a possible quantum computing revolution in sight and with
unprecedented explosion in embedded computers and cyber-physical
systems, there are reasons to re-think this foundational question
again. In fact, the calls to re-think this foundational question have
been proclaimed by physicists more than forty years ago as the
following two quotes testify:

\begin{quote}
  \textbf{Toffoli 1980~\cite{toffoli:1980}:} Mathematical models of computation are
  abstract constructions, by their nature unfettered by physical
  laws. However, if these models are to give indications that are
  relevant to concrete computing, they must somehow capture, albeit in
  a selective and stylized way, certain general physical restrictions
  to which all concrete computing processes are
  subjected.

  \textbf{Feynman 1982~\cite{springerlink:10.1007/bf02650179}:}
  Another thing that has been suggested early was that natural laws
  are reversible, but that computer rules are not. But this turned out
  to be false; the computer rules can be reversible, and it has been a
  very, very useful thing to notice and to discover that. This is a
  place where the relationship of physics and computation has turned
  itself the other way and told us something about the possibilities
  of computation. So this is an interesting subject because it tells
  us something about computer rules.
\end{quote}

\noindent These quotes by Toffoli and Feynman both highlight the
consequences of two obvious observations: (i) all the operations that
a computer performs reduce to basic physical operations; and (ii)
there is a mismatch between the logical operations of a typical model
of computation (which are logically irreversible) and the fundamental
laws of physics (which are reversible). One could certainly dismiss
the mismatch as irrelevant to the practice of computing but our thesis
is that the next computing revolution is likely to be founded on
revised models of computation that are designed to in harmony with the
laws of physics.

After a detailed introduction on the origins of ``logically
reversibile computer operations'' and an excursion into the origins of
``irreversible computer operations'', we will develop in detail three
reversible models of computation and discuss their potential
applications.

\begin{comment}
\begin{quote}
  Ed Fredkin pursued the idea that information must be finite in
  density. One day, he announced that things must be even more simple
  than that. He said that he was going to assume that information
  itself is conserved. “You’re out of you mind, Ed.” I
  pronounced. “That’s completely ridiculous. Nothing could happen in
  such a world. There couldn’t even be logical gates. No decisions
  could ever be made.” But when Fredkin gets one of his ideas, he’s
  quite immune to objections like that; indeed, they fuel him with
  energy. Soon he went on to assume that information processing must
  also be reversible — and invented what’s now called the Fredkin
  gate. (Minsky 1999)

  In other terms, what is so good in logic that quantum physics should
  obey?  Can't we imagine that our conceptions about logic are wrong,
  so wrong that they are unable to cope with the quantum miracle?
  [\ldots] Instead of teaching logic to nature, it is more reasonable
  to learn from her. Instead of interpreting quantum into logic, we
  shall interpret logic into quantum (Girard 2007).
\end{quote}
\end{comment}

%%%%%%%%%
\paragraph*{Maxwell's Demon.}
To fully appreciate the missing principle of ``reversibility'' in
conventional computing, we go back to an old thought experiment by
J. C. Maxwell. The details are codified in a letter that Maxwell wrote
to P. G Tait in 1867 -- the letter, whose ideas are now known as
\emph{Maxwell's Demon}, tells of a thought experiment that seems to
indicate that intelligent beings can somehow violate the second law of
thermodynamics, thereby violating physics itself. Many resolutions
were offered for this conundrum (for a compilation, see the book by
Leff and Rex~\cite{leff1990}), but none withstood careful scrutiny
until the establishment of \emph{Landauer's Principle} in 1961 -- a
principle whose experimental validation happened recently in
2012~\cite{berut2012experimental}.

Maxwell's demon appears to violate the second law of thermodynamics by
having a tiny `intelligence' observing the movement of individual
particles of a gas and separating fast moving particles from slow
moving ones, thereby reducing the total entropy of the
system. Landauer's resolution of the demon relied on two ideas that
took root only a few decades earlier: the formal notion of computation
(through the work of Turing and Church, 1936) and the formal notion of
information (through the work of Shannon, 1948). Landauer reasoned
that the computation done by the finite brain of the demon, involves
getting information about the movement of molecules, acting on that
information, and then overwriting it to make room for the next
computation.  In other words, the computation that is manipulating
information in the demon's brain \textit{must be thermodynamic work},
thereby bringing the demon back into the fold of physics.

%% (Cite Bennett and the various `thermodynamics of computation'  
%% papers here.)  
 
This is a strange and wonderful idea: information, physics, and
computation are inextricably linked. In contrast, when the early
models of computation were developed, there was no compelling reason
to the take the information content of computations into consideration
-- in fact, at that time there was no quantifiable notion of
information. These models followed in the footsteps of logic where,
following hundreds of years of tradition, conventional operations such
conjunction and disjunction are logically irreversible. These models
were already radical enough, by being purely \emph{constructive}, in
an era where classical mathematics, with its pervasive use of excluded
middle and frequent use of the axiom of choice, did not overly worry
about effective representation. Landauer's observation implied however
that ideas in each field have consequences for the
other~\cite{bennett:1973:lrc,bennett1985fundamental,bennett2010notes,bennett2003notes,baker:1992:nft,baez2011physics,dblp:conf/csfw/malacarias12}. To
really appreciate this fact, we delve deeper into the origin of our
computational models and argue that they are essentially reflections
of contemporary laws of physics. 

\amr{wavefront}

%%%%%%%%%
\paragraph*{Origins of Computational Models.}
Aspects of computation are
quite old. M\"{u}ller (1786) first conceived of the idea of a
``difference machine'', which Babbage (1819--1822) was able to
construct. There are other computer precursors as well -- the first
stored programs were actually for looms, most notably those of
Bouchon (1725) which operated on a paper tape, and Jacquard (1804)
which operated by chains of punched cards.

But it was Alan Turing's seminal work in 1936 which established the idea
that computation has a formal interpretation and that all
computability can be captured within a formal system. Implicit in this
achievement however is the idea that abstract models of computation
are just that -- \emph{abstractions of computation realized in the
physical world.}  In fact, one of the major achievements of Computer
Science has been the development of abstract models of computation
that shield the discipline from the underlying technology. As
effective as these models have been, one must note, however, that they
\emph{embody several implicit physical assumptions}.  As Tommaso
Toffoli explains in his influential 1980 paper:

{\begin{quote} Mathematical models of computation are abstract
  constructions, by their nature unfettered by physical laws. However,
  if these models are to give indications that are relevant to
  concrete computing, they must somehow capture, albeit in a selective
  and stylized way, certain general physical restrictions to which all
  concrete computing processes are subjected~\cite{toffoli:1980}.
\end{quote}}

Our logic of programs and our hardware are based on Boolean Logic
-- which really
ought to be called \emph{Piercean logic}, as Boole had a rather different
conception of logic which is rather far from our current models.
Nevertheless, going back
to Boole's 1853 book entitled \emph{An Investigation of the Laws of Thought,
  on which are Founded the Mathematical Theories of Logic and
  Probabilities}. The opening sentence of Ch.~1 is:
\begin{quote}
  The design of the following treatise is to investigate the fundamental laws
  of those operations of the mind by which reasoning is performed; \ldots
\end{quote}
A few chapters later, we find:
\begin{quote}
  \textbf{Proposition IV.}  That axiom of metaphysicians which is termed the
  principle of contradiction, and which affirms that it is impossible for any
  being to possess a quality, and at the same time not to possess it, is a
  consequence of the fundamental law of thought, whose expression is $x^2 =
  x$.
\end{quote}
A detailed historical analysis of Boole's ideas are beyond our scope.
The above quotes, however, should convey the idea that our notions of
computation generally date back to ideas that were thought reasonable
before \textasciitilde 1900.

It is conventional to base
the theory of computation and complexity on the Turing Machine.
Going back to Turing's 1936 article \emph{On Computable Numbers, with
  an Application to the Entscheidungsproblem,}, the opening sentence of
Sec. 1 is:
\begin{quote}
  We have said that the computable numbers are those whose decimals are
  calculable by finite means \ldots the justification lies in the fact that
  the human memory is necessarily limited.
\end{quote}
In Sec. 9, we find:
\begin{quote}
I think it is reasonable to suppose that they can only be squares
whose distance from the closest of the immediately previously observed
squares does not exceed a certain fixed amount.
\end{quote}
A detailed historical account is again beyond our scope. The quotes above
should convey the ideas that our theories
of computation and complexity are based on some assumptions that
Turing found reasonable in 1936. Though it is worth noting that these assumptions
are both physical (on distances) and metaphysical (on restrictions of the
mind).  If we take the human mind to be a physical ``machine'' which does
computation, then when both of the above assumptions are translated to the
language of physics, they embody what is known as the ``Bekenstein bound:''
which is an upper limit on the amount of information that can be contained
within a given finite region of space.

One can examine more and more cases but the general story should be
clear. Our abstractions were made up based on a certain understanding of the
laws of physics. In particular, our understanding of physics has evolved
tremendously since 1900!  Thus it is time to revisit these abstractions,
especially with respect to quantum mechanics.
In the words of Girard:
\begin{quote}
  In other terms, what is so good in logic that quantum physics should obey?
  Can't we imagine that our conceptions about logic are wrong, so wrong that
  they are unable to cope with the quantum miracle?
  \\
  Instead of teaching logic to nature, it is more reasonable to learn
  from her. Instead of interpreting quantum into logic, we shall
  interpret logic into quantum (Girard 2007).
\end{quote}

\begin{comment}
\jc{The reason I commented this out is that it is under-justified. The
reader will simply not understand what these next few lines are really
saying.}
Indeed one should take the physical principles underlying quantum mechanics,
the most successful physical theory known to us and adapt computation to
``learn'' from these principles. To illustrate the depth of our crisis, Scott
Aaronson, Umesh Vazirani, and others have proposed the following puzzle.

One of these wild claims must be true!:
\begin{itemize}
\item the extended Church-Turing thesis is false, or
\item quantum physics is false, or
\item there is an efficient classical algorithm for factoring
\end{itemize}
Indeed, if quantum physics is correct then there is an efficient quantum
algorithm for factoring (Shor). If there is no efficient classical algorithm
for factoring then the extended Church-Turing thesis is false.
\end{comment}

%%%%%%%%%
\paragraph*{From Classical to Quantum.}
Conventional classical models of computation, including boolean
circuits, the Turing machine, and the $\lambda$-calculus, are founded on
primitives which correspond to \emph{irreversible} physical processes which
gratuitously erase information.  For example, a \emph{nand} gate is an
irreversible logical operation in the sense that its inputs cannot generally
be recovered from observing its output, and so is the operation of overriding
a cell on a Turing machine tape with a new symbol, and so is a
$\beta$-reduction which typically erases or duplicates values in a way that
is destructive and irreversible.

These irreversible abstractions chosen as the basis of our
models of computing are at odds with the underlying
reversible physical reality. The information theoretic underpinnings
of physics is best understood through \emph{Landauer's
principle}~\cite{Landauer:1961}. As mentioned previously in the
introduction, Landauer, through his analysis of
Maxwell's Demon, established the idea that \emph{information is a
physical quantity} in much the sense that one would think of an
electron as a physical quantity. More precisely, his principle implies
that a certain minimum amount of
thermodynamic work has to be carried out to erase one bit of
information. Experimental verification of the Landauer principle
happened in recent years~\cite{berut2012experimental}. If information
has a physical significance, then it is subject to conservation laws
just as mass and energy are.

%%%%%%%%%
\paragraph*{Conservation of Information.}
To make this idea concrete and to provide a taste of the applications
it opens, consider a tiny 2-bit password = \verb|"10"|. The password
checker looks like:

% verbatim for now, will go to something decent later
\begin{verbatim}
check-password (guess) =
  if guess == "10"
  then True
  else False
\end{verbatim}

One can ask how much information is leaked by this program assuming
the attacker has no prior knowledge except that the password is 2
bits, i.e., the four possible 2-bits are equally likely. If the
attacker guesses \verb|"10"| (with probability $1/4$) the password (2
bits) is leaked. If the attacker guesses one of the other choices
(with probability $3/4$) the number of possibilities is reduced from 4
to 3, i.e., the attacker learns $\log{4} - \log{3}$ bits of
information. So in general the attacker learns\footnote{If the
  password is 8 restricted ASCII characters (6 bits), the attacker
  learns 0.00001 bits in the first probe.}:
\[\begin{array}{ll}
   &  1/4 * 2 + 3/4 (\log{4} - \log{3}) \\
  =&  1/4 \log{4} + 3/4 \log{4/3} \\
  =&  - 1/4 \log{1/4} - 3/4 \log{3/4} \\
  \sim& 0.8 \mbox{~bits~in~the~first~probe}
\end{array}\]

One can alternatively look at the situation by viewing the input as a
random variable with 4 possibilities and a uniform distribution (i.e.,
with 2 bits of information). The output is another random variable
with 4 possibilities but with the distribution
$\{ (True, 1/4), (False, 3/4) \}$ which contains 0.8 bits of
information. Thus 2 input bits of information were given to the
function and only 0.8 were produced. Where did the 1.2 bits of
information go? Once we accept the thesis that information is a
physical entity this question cannot be ignored.

The Landauer Principle states that erasing information requires
energy. In other words, the password checking function must have
erased 1.2 bits during its calculation, which dissipate as heat in any
physical realization of the function. In conventional models of
computation, this erasure is \emph{implicit}, i.e., it occurs as a
side effect of calculation. We wish to look at models where
erasure is simply not allowed.

There are, in fact, many different principles of physics which are
at odds with our current models of computation. Even within quantum
mechanics itself, there are other ideas (such as superposition or
entanglement) which are not present in current models. We will however
restrict ourselves to exploring a single dimension in which our
foundations should be revised: \textbf{conservation of information}.
We will follow its consequences, which will turn out to be far reaching.

\begin{quote}
  The laws of physics are essentially algorithms for calculation. These
  algorithms are significant only to the extent that they are executable in
  our real physical world. Our usual laws of physics depend on the
  mathematician's real number system. With that comes the presumption that
  given any accuracy requirement, there exist a number of calculations steps,
  which if executed, will satisfy that accuracy requirement. But the real
  world is unlikely to supply us with unlimited memory of unlimited Turing
  machine tapes, Therefore, continuum mathematics is not executable, and
  physical laws which invoke that can not really be satisfactory. They are
  references to illusionary procedures. Rolf Landauer, The physical nature of
  information, Physics Letters A 217 (1996) pp. 188-193:
\end{quote}

A high level approach to conservation of information would proceed
as follows:
\begin{enumerate}
\item Create a means to measure information.
\item Show that this measure is sound with respect to our current understanding
of what information ought to be.
\item Assert that whatever ``quantity'' of information a system has, must
remain invariant throughout the lifetime of the system.
\end{enumerate}

Certainly, whatever our notion of information, it can neither be created,
duplicated, nor erased. Of course, if we make our notion of information
too rigid, it will be impossible to even \textbf{modify} it! For computation
to occur, modification must be possible.  But of what kind?

\amr{We should also have a quick survey of other works in reversible
computation? Most of the other work tries to start from well known
models of computation or well known programming languages, and then
adapts them to be reversible. Pi is different in that it starts from
the semantic implications of reversibility. Then, because it adds
types, rather than control-flow (or even data-flow) as its next layer
of ``understanding'', this leads to equivalences, isomorphisms, etc.
This then leads, quite naturally, to finding that the ``proof
language'' of semirings (and Rig Groupoids at level 2) is actually a
programming language. And it is Pi. This is a neat twist on
Curry-Howard because CH is about \textbf{inhabitation} only. But with
``conservation of information'' as the basis, a different kind of
correspondance arises; in fact, this one may well be an actual
isomorphism.}

%%%%%%%%%
\paragraph*{Information.}
We can now give a first precise definition of information preservation.
Let
`$b$' be a (not necessarily finite) type whose values are labeled
$b^1, b^2, \ldots$. Let $\xi$ be a random variable of type $b$ that is
equal to $b^i$ with probability $p_i$. The entropy of $\xi$ is defined
as $-\sum p_i \log{p_i}$~\cite{Shannon1948}.  Consider a function
$f : b_1 \rightarrow b_2$ where $b_2$ is a (not necessarily finite)
type whose values are labeled $b_2^1, b_2^2, \ldots$. The output
entropy of the function is given by $- \sum q_j \log{q_j}$ where $q_j$
indicates the probability of the output of the function to have value
${b_2}^j$. We say a function is \emph{information-preserving} if its
output entropy is equal to the entropy of its input. In later sections,
we will refine this view.

\amr{
Talk about conservation of information in biology, computer science,
evolutionary computing, and search. (See
\url{http://www.evolutionnews.org/2012/08/conservation_of063671.html}.
}

In our current age, both Landauer's Principle and recent investigations
of computational models well-suited to quantum mechanics, push us to
revisit our current models of computation. Information, on top of
having a physical manifestation, appears to be \emph{conserved},
along with other quantities like energy, mass and momentum.

But what does it mean for a computation to preserve information?
What is the missing principle that will guide the creation of a new
model of computation that ``conserve information''?
It is nature to look again at physics for inspiration --
in our current understanding of physics all of the primitive rules
are reversible. Conventional computation is not. Can
we embrace this simple principle as the building block of a model of
computation? What would computation look like when viewed from this
vantage point? Can non-trivial computation be performed in a
reversible manner? And, if so, does it have applications?

Traditional models of computation are all unityped, generally
operating on unstructured sequences of symbols. But recent
investigations, in the intersection between mathematics and
computer science (particularly in Homotopy Type Theory~\cite{hottbook})
reveal that not only can mathematics be typed, but that doing so
reveals further structure -- in both mathematics and in computation.

And that brings us to the principal objective of this survey: explore the
domain of \emph{typed reversible computation}.

\textbf{punch line: allowable operations that computers can perform
  while embracing the laws of physics are operations that preserve
  everthing in abstract space of ``data''; doughnut into cup kind of
  transformations.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Programs as Reversible Deformations} 

To better understand conservation of information, we can look
for analogous ideas outside of computing. Turning again to physics,
but this time at the macro scale, what does it mean to transform an
object in such a way that we do not lose the fundamental character
of the object?

For rigid objects (like a chair), the only such transformations are
translations and rotations. But what about something more flexible,
such as a water balloon?  They can be \textbf{deformed} in various ways,
but still retain their fundamental character -- as long as we do not
puncture them or over-stretch them. Ignoring material characteristics
(i.e. over-stretching), what is special about these deformations, as well
as for translations and rotations, is that they correspond to
continuous maps, with a continuous inverse. In fact, even more is
true: they are analytic maps, with analytic inverses. For our purpose,
the most important part is that such maps are infinitely differentiable.
In other words, not only is there an inverse to the deformation, but
its derivative is also invertible, and so on.

When we look around, we find many different words for related
concepts: isomorphism, equivalence, sameness, equality, interchangeability,
comparability, and correspondence, to name a few. Some of these
are informal concepts, while others have formal mathematical meaning.
More important, even amongst the formal concepts, there are differences
-- which is why there are so many of them! Thus we seek a concept
which is neither too strong nor too weak, that will express when
some structured information should be treated as ``the same''.

We will take ``the same'' as a fundamental principle. We want
to then derive what it means for data, programs, program
transformations, as well as proofs / deductions, to be ``the
same'' -- in an manner consistent with preservation of 
information.

This stands in stark contrast with most current approaches to
reversible computation, which start from current models of
computation involving irreversible operations and try to find
various ways to \emph{patch things up} so as to be reversible.

The reader will have surely noticed how we have studiously avoided
defining ``the same''. Because there are many such notions, we
also need to walk our way through them to find the one which is
``just right''. We can draw an analogy with topology: in topology,
all point sets can always be equipped with either the discrete or
the indiscrete topology, but both of these extremes are rarely
useful. We will develop our working notion of ``sameness''
as we go through the various components that make up a
programming language.

Starting from a physical perspective, whatever our notion of data
is, we will be interested in programs as representing transformations
of that data which are reversible. In other words, we want our
programs-as-transformations to ``play well'' with the inherent notion
of ``sameness'' that our data will carry. Thus we need to start by
looking at what structure our data has, and that will help us define
an appropriate notion of program. Of course, when programs themselves
are data, things do get more complicated.  In the following sections,
we will look at different natural classes of data, and explore the
corresponding notion of reversible programs.

\amr{In the quantum case, this is clear. Say something about topological
quantum computing.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data I: Finite Sets}

Before we start our investigation of what it means to have typed,
information preserving transformations, let's begin with just
finite sets. Most programming languages provide some collection of primitive
data like booleans, characters, strings, and (bounded) numbers that are
naturally modeled as sets. Infinite sets are much more subtle, and will be
discussed in a later section.

What does it mean for two finite sets to be ``the same''?  Well, clearly the
sets $A = \left\{1, 2, 3\right\}$ and $B = \left\{c, d\right\}$ are different.
Why?  Well, supposed there was a transformation $f : A \rightarrow B$ that
deformed $A$ into $B$, and another $g : B \rightarrow A$ which undid this
transformation. Since $f$ is total, by the pigeonhole principle, two elements
of $A$ would be mapped to the same element of $B$. Supposed that this is $2$ and
$3$, and that they both map to $d$.  But $g(d)$ cannot be both $2$ and $3$, and
so $g$ is not the inverse of $f$. With just a little more work, we can show
that $f$ (and $g$) must be both injective and surjective. In other words,
$f$ (and $g$) must be a bijection between $A$ and $B$.  And of course this 
only happens when $A$ and $B$ have the same number of elements.

More importantly, given a bijection $f : C \rightarrow D$ of finite sets
$C,D$, there always exists another bijection $g : D \rightarrow C$ which is
$f$'s inverse. So, for finite sets, \emph{bijections} act as reversible
deformations.

This discussion is purely ``semantic'', in the sense that it is about
the denotation of simple primitive data (sets) and their reversible
deformations (bijections).  We would like to reverse engineer a programming
language from this denotation. But first, an obvious remark: any two sets
$C$ and $D$ of cardinality $n$ are always in bijective correspondance. So
we can abstract away from the details of the elements of $C$ and $D$ and
instead choose canonical representations -- in much the same way as computers
choose binary words to represent everything.

\begin{defn} For $n\in\Nat$, denote by $\fin{n}$ the set
$\left\{0,1,\ldots,n-1\right\}$.
We will refer to $\fin{n}$ as the canonical set with $n$ elements.
\end{defn}

Bijections on \fin{n} have a specific name: permutations. As is well-known
permutations can be generated by sequences of transpositions. Thus we
can create a small language for writing permutations as
\begin{verbatim}
p ::= id | swap i j | p;p
\end{verbatim}
where $i,j:\Nat$, $i\neq j$ and $i,j < n$. Note that we could remove 
\verb|id|
from the language and drop the $i\neq j$ condition so that 
\verb|swap j j| would represent the identity permutation.

While we are mainly interested in typed data, it is always worthwhile to
first start investigating the untyped setting first, as it gives us insights
into what the \emph{purely operational} view of the theory would be.
This tells us that permutations are an inescapable part of the fabric of
reversible computing. 

However as permutations are untyped, and act on the canonicalized version
of $n$-element sets (i.e. those sets where all the structure has been
forgotten), these are a rather pale shadow of the rich tapestry of
information-perserving transformation of structured data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data II: Structured Finite Types}

The elementary building blocks of type theory are the empty type
($\bot$), the unit type ($\top$), and the sum ($\uplus$) and product
($\times$) types.  These have a nice correspondance (Curry-Howard)
to logic:

\begin{center}
\begin{tabular}{c|c}
Logic & Types \\ \hline
$\true$ & $\top$ \\
$\false$ & $\bot$ \\
$\land$ & $\times$ \\
$\lor$ & $\uplus$ \\
\end{tabular}
\end{center}

This correspondance is rather fruitful. As logical expressions form
a commutative semiring, we would expect that types too form a commutative
semiring. And indeed they do -- at least up to \emph{type equivalence}.  The
natural numbers $\Nat$ are another commutative semiring; it will turn out that,
even though the Curry-Howard correspondance has been extremely fruitful for
programming language research, it is $\Nat$ which will be a better model for
finite structured types.

\begin{defn}
  A \emph{commutative semiring} (sometimes called a \emph{commutative
    rig} (commutative ri\emph{n}g without negative elements) consists of a
  set $R$, two distinguished elements of $R$ named 0 and 1, and two
  binary operations~$+$ and $\cdot$, satisfying the following
  relations for any $a,b,c \in R$:
\begin{equation}
\begin{array}{rcl}
0 + a &=& a \\
a + b &=& b + a \\
a + (b + c) &=& (a + b) + c \\
\\
1 \cdot a &=& a \\
a \cdot b &=& b \cdot a \\
a \cdot (b \cdot c) &=& (a \cdot b) \cdot c \\
\\
0 \cdot a &=& 0 \\
(a + b) \cdot c &=& (a \cdot c) + (b \cdot c)
\end{array}
\label{eq:csemiring}
\end{equation}
\end{defn}

\begin{prop}
The structure $\left(\mathbb{B}, \false, \true, \lor, \land\right)$
is a commutative semiring.
\end{prop}

We cannot immediately use the above definition in a typed setting.
First, types do not naturally want to be put together into a ``set''.
This can be fixed if we replace the set $R$ with a universe $U$, and
rather than insisting that we have $0 \in R$, we have a typing
judgement $\bot : U$ (and similarly for the other items).

Our first instinct would be to similarly replace $=$ with
$\equiv$, i.e. propositional equality. This bring the second
problem to the fore: this is false. For example, unless $A \equiv B$,
$A \times B \equiv B \times A$ is not normally provable%
\footnote{Not without assuming that $U$ is univalent, which we
don't.}. But it should be clear that $A \times B$ and $B \times A$
contain equivalent information. In other words, we would like to
be able to witness that $A \times B$ can be reversibly deformed
into $B \times A$. Which motivates the introduction of 
type \emph{equivalences}. To do this, we need a few important
auxiliary concepts first.

\begin{defn}[homotopy]
\label{def:homotopy}
Two functions $f,g:A \rightarrow B$ are \emph{homotopic} if
$\forall x:A. f(x) = g(x)$. We denote this $f \sim g$.
\end{defn}

\noindent It is easy to prove that homotopies (for any given function
space $A \rightarrow B$) are an equivalence relation.  The simplest
definition of the data which makes up an equivalence is then

\begin{defn}[Quasi-inverse]
\label{def:quasi}
For a function $f : A \rightarrow B$, a \emph{quasi-inverse} is a
triple $(g, \alpha, \beta)$, consisting of a function
$g : B \rightarrow A$ and two homotopies
$\alpha : f \circ g \sim \mathrm{id}_B$ and
$\beta : g \circ f \sim \mathrm{id}_A$.
\end{defn}
 
\begin{defn}[Equivalence of types]
  Two types $A$ and $B$ are equivalent $A \simeq B$ if there exists a
  function $f : A \rightarrow B$ together with a quasi-inverse for $f$.
\end{defn}

\noindent Why \emph{quasi}? The reasons are beyond our scope, but
the interested reader can read sections $2.4$ and $4$ in \cite{hottbook}.
There are several conceptually different, but equivalent, 
``better'' definitions.  We record just one here:

\begin{defn}[Bi-invertibility]
\label{def:biinv}
For a function $f : A \rightarrow B$, a \emph{bi-inverse} is a
pair of functions $g,h : B \rightarrow A$ and two homotopies
$\alpha : f \circ g \sim \mathrm{id}_B$ and
$\beta : h \circ f \sim \mathrm{id}_A$.
\end{defn}
 
\noindent We can then replace quasi-inverse with bi-invertibility in
the definition of type equivalence. The differences will not matter to
us here.

If we replace $=$ by $\simeq$, then we can indeed prove that
types (with $\bot, \top, \uplus, \times$) form a commutative semiring.

The reader familiar with universal algebra should pause
and ponder a bit about what we have done. We have lifted 
\emph{equality} from being in the signature of the ambient logic
and instead put it in the signature of the algebraic structure
of interest. Then, and only then, are we in a solid position
to show that we have a concept (commutative semiring) which is flexible 
enough for types to be an instance.

If we revisit the Curry-Howard correspondance, we notice one
more issue. In logic, it is true that $A \lor A = A$ and
$A \land A = A$. However, neither $A \uplus A$ nor $A \times A$ are
equivalent to $A$. They are however \emph{equi-inhabited}. This is
a fancy way of saying
\[ A \uplus A \ \text{is inhabited} \qquad \Leftrightarrow
  \qquad A \ \text{is inhabited} \]
The above is the real \textit{essence} of the correspondance.
In other words, classical Curry-Howard tells us about
\emph{logical equivalence} of types. This is even a
constructive statement: there are indeed functions
$f : A \uplus A \rightarrow A$ and $g : A \rightarrow A \uplus A$;
however, they are not inverses.

So mere inhabitation falls far short of our goals of being
able to smoothly deform from one type to another.  

Let us thus analyze the crux of the ``problem''. In logic, we have
that $\land$ and $\lor$ are both \emph{idempotent}: this is the
property of any binary operation $*$ where
$\forall a. a * a = a$. And it should be clear that idempotent
operations are \emph{forgetful}: its input has two copies of $a$,
but its output, only one. On the type side, something more subtle
happens. Consider $\top \uplus \top$ versus $\top$; the first has
\emph{two} proofs of inhabitation ($\texttt{inl tt} and \texttt{inr tt}$)
while the second only one ($\texttt{tt}$). These cannot be put in
bijective correspondance. Even though the ``payload'' \texttt{tt}
is the same, forgetting $\texttt{inl}$ (or \texttt{inr}) throws
away information -- something we have expressly disallowed.
Yes, this should remind you of Maxwell's demon: even though the
data is the same, they are tagged differently, and this
data is indeed information, and must be preserved.

\begin{figure}[t]
\[
\begin{array}{rrcll}
\idc :& \tau & \simeq & \tau &: \idc \\
\\
\identlp :&  \bot \uplus \tau & \simeq & \tau &: \identrp \\
\swapp :&  \tau_1 \uplus \tau_2 & \simeq & \tau_2 \uplus \tau_1 &: \swapp \\
\assoclp :&  \tau_1 \uplus (\tau_2 \uplus \tau_3) & \simeq & (\tau_1 \uplus \tau_2) \uplus \tau_3 &: \assocrp \\
\\
\identlt :&  \top \times \tau & \simeq & \tau &: \identrt \\
\swapt :&  \tau_1 \times \tau_2 & \simeq & \tau_2 \times \tau_1 &: \swapt \\
\assoclt :&  \tau_1 \times (\tau_2 \times \tau_3) & \simeq & (\tau_1 \times \tau_2) \times \tau_3 &: \assocrt \\
\\
\distz :&~ \bot \times \tau & \simeq & \bot ~ &: \factorzl \\
\dist :&~ (\tau_1 \uplus \tau_2) \times \tau_3 & \simeq & (\tau_1 \times \tau_3) \uplus (\tau_2 \times \tau_3)~ &: \factor
\end{array}
\]
\caption{Type isomorphisms.}
\label{type-isos}
\end{figure}

Nevertheless, the Curry-Howard correspondance still has some
force. We know that the inhabitants of types formed with
with $\bot, \top, \uplus, \times$ are a commutative semiring. What we
want to know is, which types are equivalent? From a commutative semiring
perspective, this amounts to asking what terms are equal?
We have a set of generators for those equations, namely those
in~(\ref{eq:csemiring}). What we thus need is to create $8$ pairs
of mutually inverse functions which witness these identities.
For concreteness, we show the signatures in Figure~\ref{type-isos}.
As these all come in symmetric pairs (some of which are self-symmetric),
we give names for both directions. The names use $\iso$, $+$ and $*$
as these will be what we use when we reflect these isomorphism into
a programming language, which we proceed to do next.

\begin{figure}[t]
\[
\begin{array}{rrcll}
\idc :& \tau & \iso & \tau &: \idc \\
\\
\identlp :&  0 + \tau & \iso & \tau &: \identrp \\
\swapp :&  \tau_1 + \tau_2 & \iso & \tau_2 + \tau_1 &: \swapp \\
\assoclp :&  \tau_1 + (\tau_2 + \tau_3) & \iso & (\tau_1 + \tau_2) + \tau_3 &: \assocrp \\
\\
\identlt :&  1 * \tau & \iso & \tau &: \identrt \\
\swapt :&  \tau_1 * \tau_2 & \iso & \tau_2 * \tau_1 &: \swapt \\
\assoclt :&  \tau_1 * (\tau_2 * \tau_3) & \iso & (\tau_1 * \tau_2) * \tau_3 &: \assocrt \\
\\
\distz :&~ 0 * \tau & \iso & 0 ~ &: \factorzl \\
\dist :&~ (\tau_1 + \tau_2) * \tau_3 & \iso & (\tau_1 * \tau_3) + (\tau_2 * \tau_3)~ &: \factor
\end{array}
\]
\caption{$\Pi$-terms.}
\label{pi-terms}
\end{figure}

We now have in our hands our desired denotational semantics: we want
to create a language $\Pi$ such that the types and type combinators
map to $\bot, \top, \uplus, \times$, and such that we have ground
terms whose denotation are all $16$ type isomorphisms of
Figure~\ref{type-isos}. This is completely straightforward, as
we can simply do this literally. To make the analogy with 
commutative semirings stand out even more, we will use $0, 1, +$, and $*$
at the type level, and will denote ``equivalence'' by $\iso$.
Thus Figure~\ref{pi-terms} shows the ``constants'' of the language.

\jc{wavefront}

Of course, one does not get a programming language with just (typed)
constants!

\begin{figure}[t]
\[
\Rule{}
{\jdg{}{}{c_1 : \tau_1 \iso \tau_2} \quad \vdash c_2 : \tau_2 \iso \tau_3}
{\jdg{}{}{c_1 \odot c_2 : \tau_1 \iso \tau_3}}
{}
\qquad
\Rule{}
{\jdg{}{}{c_1 : \tau_1 \iso \tau_2} \quad \vdash c_2 : \tau_3 \iso \tau_4}
{\jdg{}{}{c_1 \oplus c_2 : \tau_1 + \tau_3 \iso \tau_2 + \tau_4}}
{}
\]
\[
\Rule{}
{\jdg{}{}{c_1 : \tau_1 \iso \tau_2} \quad \vdash c_2 : \tau_3 \iso \tau_4}
{\jdg{}{}{c_1 \otimes c_2 : \tau_1 * \tau_3 \iso \tau_2 * \tau_4}}
{}
\]
\caption{$\Pi$-combinators.}
\label{pi-combinators}
\end{figure}

The type-erasure will, naturally, be permutations. But where do the
isomorphisms come from?  Well, inhabitants of types with (0,1,+,*)
form a semiring. So, still inspired by Curry-Howard, look to the
proof-terms of semiring identities -- and voila, a nice set of
isomorphisms! We can name these isomorphisms, AND give them
implementations. 

That gives us some ground terms.  Not enough for a PL! So then
we need to be able to 'combine' these. So we add 3 combinators for
3 different kinds of composition.
Gives us a nice, reversible PL for
structured finite types. We call it Pi.

We actually have several choices. We can make Pi structurally reversible
(i.e. there is a syntactic transformation on Pi which maps combinators
to combinators which witnesses reversibility), or build in a `reverse'
combinator which flips things around. The latter lets us make the
language much smaller, but then \emph{programs} in that language are longer.
[Bias: since there will be many more programs written, the fact that the
language definition, and its processors, is larger, is a one-time cost.
Not that big a deal. So pick the version whose meta-theoretical
properties are obvious. It is thus worthwhile to have reversibility be
structural.]

sound and complete isomorphisms for finite types

\begin{verbatim}
examples: 
  (1 + 1) x ((1 + 1) x b) = (b+b) + (b+b)
  conditionals
  toffoli
  fredkin
\end{verbatim}

\begin{figure}[t]
Let $c_1 : t_1 \leftrightarrow t_2$,  $c_2 : t_2 \leftrightarrow t₃$, and $c_3 : t_3 \leftrightarrow t_4$:
\[\def\arraystretch{1.3}
\begin{array}{c}
  {c_1 \circledcirc (c_2 \circledcirc c_3) \Leftrightarrow (c_1 \circledcirc c_2) \circledcirc c_3}
\\
  {(c_1 \oplus (c_2 \oplus c_3)) \circledcirc \assoclp \Leftrightarrow \assoclp \circledcirc ((c_1 \oplus c_2) \oplus c_3)}
\\
  {(c_1 \otimes (c_2 \otimes c_3)) \circledcirc \assoclt \Leftrightarrow \assoclt \circledcirc ((c_1 \otimes c_2) \otimes c_3)}
\\
  {((c_1 \oplus c_2) \oplus c_3) \circledcirc \assocrp \Leftrightarrow \assocrp \circledcirc (c_1 \oplus (c_2 \oplus c_3))}
\\
  {((c_1 \otimes c_2) \otimes c_3) \circledcirc \assocrt \Leftrightarrow \assocrt \circledcirc (c_1 \otimes (c_2 \otimes c_3))}
\\
  {\assocrp \circledcirc \assocrp \Leftrightarrow ((\assocrp \oplus \idc) \circledcirc \assocrp) \circledcirc (\idc \oplus \assocrp)}
\\
  {\assocrt \circledcirc \assocrt \Leftrightarrow ((\assocrt \otimes \idc) \circledcirc \assocrt) \circledcirc (\idc \otimes \assocrt)}
\end{array}\]
\caption{\label{figj}Signatures of level-1 $\Pi$-combinators: associativity}
\end{figure}
  
\begin{figure}[t]
Let $a : t_1 \leftrightarrow t_2$, $b : t_3 \leftrightarrow t_4$, and $c : t₅ \leftrightarrow t₆$:
\[\def\arraystretch{1.3}
\begin{array}{c}
  {((a \oplus b) \otimes c) \circledcirc \dist \Leftrightarrow \dist \circledcirc ((a \otimes c) \oplus (b \otimes c))}
\\
  {(a \otimes (b \oplus c)) \circledcirc \distl \Leftrightarrow \distl \circledcirc ((a \otimes b) \oplus (a \otimes c))}
\\
  {((a \otimes c) \oplus (b \otimes c)) \circledcirc \factor \Leftrightarrow \factor \circledcirc ((a \oplus b) \otimes c)}
\\
  {((a \otimes b) \oplus (a \otimes c)) \circledcirc \factorl \Leftrightarrow \factorl \circledcirc (a \otimes (b \oplus c))}
\end{array}\]
\caption{\label{figi}Signatures of level-1 $\Pi$-combinators: distributivity and factoring}
\end{figure}

\begin{figure}[t]
Let $c, c_1, c_2, c_3 : t_1 \leftrightarrow t_2$ and $c', c'' : t_3 \leftrightarrow t_4$: 
\[\def\arraystretch{1.3}
\begin{array}{c}
  {\idc \circledcirc \, c \Leftrightarrow c}
\quad 
  {c \, \circledcirc \idc \, \Leftrightarrow c}
\quad
  {c\,\, \circledcirc\,! c \Leftrightarrow \idc}
\quad 
  {! c \circledcirc c \Leftrightarrow \idc}
\\
  {c \Leftrightarrow c}
\quad 
\Rule{}
  {c_1 \Leftrightarrow c_2 \quad c_2 \Leftrightarrow c_3}
  {c_1 \Leftrightarrow c_3}
  {} 
\quad
\Rule{}
  {c_1 \Leftrightarrow c' \quad c_2 \Leftrightarrow c''}
  {c_1 \circledcirc c_2 \Leftrightarrow c' \circledcirc c''}
  {}
\end{array}\]
\caption{\label{figh}Signatures of level-1 $\Pi$-combinators: identity and composition}
\end{figure}

\begin{figure}[t]
Let $c₀ : 0 \leftrightarrow 0$, $c_1 : 1 \leftrightarrow 1$, and $c : t_1 \leftrightarrow t_2$:
\[\def\arraystretch{1.3}
\begin{array}{c}
  {\identlp \circledcirc c \Leftrightarrow (c₀ \oplus c) \circledcirc \identlp}
\qquad 
  {\identrp \circledcirc (c₀ \oplus c) \Leftrightarrow c \circledcirc \identrp}
\\
  {\identlsp \circledcirc c \Leftrightarrow (c \oplus c₀) \circledcirc \identlsp}
\qquad
  {\identrsp \circledcirc (c \oplus c₀) \Leftrightarrow c \circledcirc \identrsp}
\\
  {\identlt \circledcirc c \Leftrightarrow (c_1 \otimes c) \circledcirc \identlt}
\qquad
  {\identrt \circledcirc (c_1 \otimes c) \Leftrightarrow c \circledcirc \identrp}
\\
  {\identlst \circledcirc c \Leftrightarrow (c \otimes c_1) \circledcirc \identlst}
\qquad
  {\identrst \circledcirc (c \otimes c_1) \Leftrightarrow c \circledcirc \identrst}
\\
  {\identlt \Leftrightarrow \distl \circledcirc (\identlt \oplus \identlt)}
\\
\identlp \Leftrightarrow \swapp \circledcirc \identrsp
\qquad
\identlt \Leftrightarrow \swapt \circledcirc \identrst
\end{array}\]
\caption{\label{figg}Signatures of level-1 $\Pi$-combinators: unit}
\end{figure}

\begin{figure}[t]
Let $c_1 : t_1 \leftrightarrow t_2$ and $c_2 : t_3 \leftrightarrow t_4$:
\[\def\arraystretch{1.3}
\begin{array}{c}
  {\swapp \circledcirc (c_1 \oplus c_2) \Leftrightarrow (c_2 \oplus c_1) \circledcirc \swapp}
\quad
  {\swapt \circledcirc (c_1 \otimes c_2) \Leftrightarrow (c_2 \otimes c_1) \circledcirc \swapt}
\\
  {(\assocrp \circledcirc \swapp) \circledcirc \assocrp \Leftrightarrow ((\swapp \oplus \idc) \circledcirc \assocrp) \circledcirc (\idc \oplus \swapp)}
\\
  {(\assoclp \circledcirc \swapp) \circledcirc \assoclp \Leftrightarrow ((\idc \oplus \swapp) \circledcirc \assoclp) \circledcirc (\swapp \oplus \idc)}
\\
  {(\assocrt \circledcirc \swapt) \circledcirc \assocrt \Leftrightarrow ((\swapt \otimes \idc) \circledcirc \assocrt) \circledcirc (\idc \otimes \swapt)}
\\
  {(\assoclt \circledcirc \swapt) \circledcirc \assoclt \Leftrightarrow ((\idc \otimes \swapt) \circledcirc \assoclt) \circledcirc (\swapt \otimes \idc)}
\end{array}\]
\caption{\label{figf}Signatures of level-1 $\Pi$-combinators: commutativity and associativity}
\end{figure}

\begin{figure}[t]
Let $c_1 : t_1 \leftrightarrow t_2$, $c_2 : t_3 \leftrightarrow t_4$, $c_3 : t_1 \leftrightarrow t_2$, and $c_4 : t_3 \leftrightarrow t_4$. \\
Let $a_1 : t₅ \leftrightarrow t_1$,  $a_2 : t₆ \leftrightarrow t_2$, $a_3 : t_1 \leftrightarrow t_3$, and $a_4 : t_2 \leftrightarrow t_4$.
\[\def\arraystretch{1.3}
\begin{array}{c}
\Rule{}
  {c_1 \Leftrightarrow c_3 \quad c_2 \Leftrightarrow c_4}
  {c_1 \oplus c_2 \Leftrightarrow c_3 \oplus c_4}
  {}
\qquad
\Rule{}
  {c_1 \Leftrightarrow c_3 \quad c_2 \Leftrightarrow c_4}
  {c_1 \otimes c_2 \Leftrightarrow c_3 \otimes c_4}
  {} 
\\
  {\idc \oplus \, \idc \, \Leftrightarrow \idc}
\qquad
  {\idc \otimes \, \idc \, \Leftrightarrow \idc}
\\
  {(a_1 \circledcirc a_3) \oplus (a_2 \circledcirc a_4) \Leftrightarrow (a_1 \oplus a_2) \circledcirc (a_3 \oplus a_4)}
\\
  {(a_1 \circledcirc a_3) \otimes (a_2 \circledcirc a_4) \Leftrightarrow (a_1 \otimes a_2) \circledcirc (a_3 \otimes a_4)}
\end{array}\]
\caption{\label{fige}Signatures of level-1 $\Pi$-combinators: functors}
\end{figure}

\begin{figure}[t]
\[\def\arraystretch{1.3}
\begin{array}{c}
  {\identlsp \oplus \idc ~\Leftrightarrow~ \assocrp \circledcirc (\idc \oplus \, \identlp)}
\\
  {\identlst \otimes \idc ~\Leftrightarrow~ \assocrt \circledcirc (\idc \otimes \, \identlt)}
\end{array}\]
\caption{\label{figd}Signatures of level-1 $\Pi$-combinators: unit and associativity}
\end{figure}


\begin{figure}[t]
Let $c : t_1 \leftrightarrow t_2$:
\[\def\arraystretch{1.3}
\begin{array}{c}
  {(c \otimes \idc) \circledcirc \absorbl \Leftrightarrow \absorbl \circledcirc \idc}
\quad
  {(\idc \, \otimes c) \circledcirc \absorbr \Leftrightarrow \absorbr \circledcirc \idc}
\\
  {\idc \circledcirc \, \factorzl \Leftrightarrow \factorzl \circledcirc (\idc \otimes c)}
\quad
  {\idc \circledcirc \, \factorzr \Leftrightarrow \factorzr \circledcirc (c \otimes \idc)}
\\
  {\absorbr \Leftrightarrow \absorbl}
\\
  {\absorbr \Leftrightarrow (\distl \circledcirc (\absorbr \oplus \absorbr)) \circledcirc \identlp}
\\
  {\identlst \Leftrightarrow \absorbr}
\qquad
  {\absorbl \Leftrightarrow \swapt \circledcirc \absorbr}
\\
  {\absorbr \Leftrightarrow (\assoclt \circledcirc (\absorbr \otimes \idc)) \circledcirc \absorbr}
\\
  {(\idc \otimes \absorbr) \circledcirc \absorbl \Leftrightarrow (\assoclt \circledcirc (\absorbl \otimes \idc)) \circledcirc \absorbr}
\\
  {\idc \otimes \, \identlp \Leftrightarrow (\distl \circledcirc (\absorbl \oplus \idc)) \circledcirc \identlp}
\end{array}\]
\caption{\label{figc}Signatures of level-1 $\Pi$-combinators: zero}
\end{figure}

\begin{figure}[t]
\[\def\arraystretch{1.3}
\begin{array}{c}
  {((\assoclp \otimes \idc) \circledcirc \dist) \circledcirc (\dist \oplus \idc) \Leftrightarrow (\dist \circledcirc (\idc \oplus \dist)) \circledcirc \assoclp}
\\
  {\assoclt \circledcirc \distl \Leftrightarrow ((\idc \otimes \distl) \circledcirc \distl) \circledcirc (\assoclt \oplus \assoclt)}
\end{array}\]
\vspace{ -0.5em}
\[\def\arraystretch{1.3}
\begin{array}{rcl}
  (\distl \circledcirc (\dist \oplus \dist)) \circledcirc \assoclp &\Leftrightarrow&   
   \dist \circledcirc (\distl \oplus \distl) \circledcirc \assoclp ~\circledcirc \\
&& (\assocrp \oplus \idc) ~\circledcirc \\
&& ((\idc \oplus \swapp) \oplus \idc) ~\circledcirc \\
&&      (\assoclp \oplus \idc)
\end{array}\]
\caption{\label{figb}Signatures of level-1 $\Pi$-combinators: associativity and distributivity}
\end{figure}

\begin{figure}[t]
\[\def\arraystretch{1.3}
\begin{array}{rcl}
  (\idc \otimes \swapp) \circledcirc \distl &\Leftrightarrow& \distl \circledcirc \swapp
\\
  \dist \circledcirc (\swapt \oplus \swapt) &\Leftrightarrow & \swapt \circledcirc \distl
\end{array}\]
\caption{\label{figa}Signatures of level-1 $\Pi$-combinators: commutativity and distributivity}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data III: Reversible Programs between Reversible Programs}

In the previous sections, we examined equivalences between
conventional data structures, i.e, sets of values and structured trees
of values. We now consider a richer, less conventional, but
foundational notion of data: programs themselves. Indeed, universal
computation models fundamentally rely on the fact that \emph{programs
  are (or can be encoded as) data}, e.g., a Turing machine can be
encoded as a string that another Turing machine (or even the same
machine) can manipulate. In our setting, the programs developed in the
previous section are reversible deformations between structured finite
types. We now ask whether these reversible deformations can themselves
be subject to (higher-level) reversible deformations?

Before developing the theory, let's consider a small example
consisting of two deformations between the types $A + B$ and $C+D$:

\begin{center}
\begin{tikzpicture}[scale=0.7,every node/.style={scale=0.8}]
  \draw[>=latex,<->,double,red,thick] (2.25,-1.2) -- (2.25,-2.9) ;
%%  \node at (3.3,-2) {$\mathit{swapl}_{+\Leftrightarrow}$} ;
%%  \node at (2.5,-1.3) {$((c_2~\oplus~c_1)~\circledcirc~\mathit{swap}_{+})$}; 
%%  \node at (2.5,-2.7) {$(\mathit{swap}_{+}~\circledcirc~(c_1~\oplus~c_2))$}; 
  \draw (-2,-2) ellipse (0.5cm and 1cm);
  \draw[fill] (-2,-1.5) circle [radius=0.025];
  \node[below] at (-2.1,-1.5) {$A$};
  \draw[fill] (-2,-2.5) circle [radius=0.025];
  \node[below] at (-2.1,-2.5) {$B$};

  \draw (6.5,-2) ellipse (0.5cm and 1cm);
  \draw[fill] (6.5,-1.5) circle [radius=0.025];
  \node[below] at (6.7,-1.5) {$C$};
  \draw[fill] (6.5,-2.5) circle [radius=0.025];
  \node[below] at (6.7,-2.5) {$D$};

  \draw[<-] (-2,-1.5) to[bend left] (1,0.5) ;
  \draw[<-] (-2,-2.5) to[bend left] (1,-0.5) ;
  \draw[->] (3.5,0.5) to[bend left] (6.5,-1.45) ;
  \draw[->] (3.5,-0.5) to[bend left] (6.5,-2.45) ;

  \draw[<-] (-2,-1.5) to[bend right] (1,-3.5) ;
  \draw[<-] (-2,-2.5) to[bend right] (1,-4.5) ;
  \draw[->] (3.5,-3.5) to[bend right] (6.5,-1.55) ;
  \draw[->] (3.5,-4.5) to[bend right] (6.5,-2.55) ;

  \draw     (2.5,-3)  -- (3.5,-3) -- (3.5,-4) -- (2.5,-4) -- cycle ;
  \draw     (2.5,-4)  -- (3.5,-4) -- (3.5,-5) -- (2.5,-5) -- cycle ;

  \draw     (1,1)  -- (2,1) -- (2,0) -- (1,0) -- cycle ;
  \draw     (1,0)  -- (2,0) -- (2,-1) -- (1,-1) -- cycle ;

  \node at (3,-3.5) {$c_1$};
  \node at (3,-4.5) {$c_2$};

  \node at (1.5,0.5) {$c_2$};
  \node at (1.5,-0.5) {$c_1$};

  \draw     (2,0.5)  -- (2.5,0.5)  ;
  \draw     (2,-0.5) -- (2.5,-0.5) ; 

  \draw     (2.5,0.5)  -- (3.5,-0.5)  ;
  \draw     (2.5,-0.5) -- (3.5,0.5) ; 

  \draw     (1,-3.5)  -- (2,-4.5)    ;
  \draw     (1,-4.5) -- (2,-3.5)   ; 

  \draw     (2,-3.5)  -- (2.5,-3.5)    ; 
  \draw     (2,-4.5) -- (2.5,-4.5)   ; 

\end{tikzpicture}
\end{center}
The top path is the reversible deformation
$(c_2~\oplus~c_1)~\circledcirc~\mathit{swap}_{+}$ which deforms the
type $A$ by $c_2$, deforms the type $B$ by $c_1$, and deforms the
resulting space by a twist that exchanges the two injections into the
sum type. The bottom path performs the twist first and then deforms
the type $A$ by $c_2$ and the type $B$ by $c_1$ as before. If one
could imagine the paths are physical wires and the deformations $c_1$
and $c_2$ as arbitrary deformations on these wires then, holding the
points $A$, $B$, $C$, and $D$ fixed, it is possible to rotate the top
part of the diagram to become identical to the bottom one. In other
words, there exists a higher-level deformation of the program
$(c_2~\oplus~c_1)~\circledcirc~\mathit{swap}_{+}$ to the program
$\mathit{swap}_{+} \circledcirc (c_1~\oplus~c_2)$.

\amr{*** another wavefront ***}


Pi level 2.

Take categorification seriously. Types are indeed groupoids, and
endo-isomorphisms inhabit the Identity 'type' of a type. So + and *
must be operations on groupoids. Look at the category of (small)
categories for inspiration. So we have two monoidal structures.
These are not co-product and product in Groupoid. But this is actually
a good thing.

So we end up looking, semantically, at weak Rig Groupoid. Syntactically,
we take the easiest way there: simply make every coherence isomorphism into
a combinator. Huge proof obligation that this induces actual transformations
on Pi programs. Luckily, it does.  But not straightforwardly -- ``incorrect''
definitions of some Pi combinators (such as those for 0*x = 0) can lead to
some level 2 morphisms not working. 

The interpretation on permutations is unclear, because the Pi representation
has so much more structure, and permutations too have many different 
representations. So it is unclear what maps to what.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

Discuss recursion, or add a section of infinite sets???

CS abstractions are based on ``old'' physics

computer applications are more and more ``physical''

physical principles such as conservation of information should be part
of our foundational abstractions

quest for other principles; other data models; graphs; HoTT

Data IV: Recursive Types

Add recursion to structured finite types, trace, feedback, loop...

Natural numbers: fold/unfold

Partial isos

meta-circular interpreter
 
DANGER: here, we leave the safe land of ``preservation of information'',
because partiality is not reversible. So while trace gives a reversible
program as output, it itself isn't a reversible operation. Like the
demon's brain in Maxwell's thought experiment, trace irreversible work!
So trace, which is fundamental to all work with recursive types, is
only reversible "at level 1". So we need to replace trace with some other
operation for level 2 reversibility to also hold -- and this is an open
problem.

Data V: Exclusive Disjunctions

Quantum over finite field 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgements} We would like to thank the numerous
students and colleagues who participated in various aspects of this
research and who provided valuable feedack and constructive criticism.

\bibliographystyle{acm}
%\pagestyle{headings}
\bibliography{cites}
\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Below are pieces of text that are not immediately useful above, but
%% from which we might still be able to pilfer usefully.

\subsection{Technical Overview}

The technical development derives from ideas which are implicit in the
works of \cite{toffoli:1980, Zuliani:2001:LR,
  malacaria2007assessing,ClarkHM07,Ghica:2007:GSS:1190216.1190269}.
(1) Functions whose output entropy is equal to their input entropy are
information preserving functions.  (2) A function
$f : b_1 \rightarrow b_2$ is \emph{logically reversible} if there
exists an inverse function $f^{\dagger}$ such that for any input
$v_1:b_1$ if there exists $v_2:b_2$ such that $f(v_1)=v_2$ \emph{iff}
$f^{\dagger}(v_2)=v_1$.  (3) Logically reversible functions are
information preserving.

To summarize:
\begin{enumerate}
\item Based on isomorphisms of sum and product
  types~\cite{Fiore:2004}, we develop a strong normalizing model of
  computing, which we call $\Pi$, that is logically reversible and
  information preserving. Every computation expressible in $\Pi$
  is an isomorphism.

\item We extend $\Pi$ with isorecursive type and trace operators from
  category theory to obtain $\Pi$. Every computation expressible in
  $\Pi$ is a partial isomorphism --- i.e. the system admits
  non-termination. $\Pi$ is Turing complete while being information
  preserving and logically reversible.

\item We develop the categorical semantics of these
  models~\cite{rc2011}. \amr{Mention n-categories.} We also develop
  a graphical notation for programming in these models that
  reminiscent of Penrose diagrams for
  categories~\cite{selinger-graphical}, Geometry of Interaction (GoI)
  machines and Proof
  Nets~\cite{Mackie2011,DBLP:conf/popl/Mackie95}. In the graphical
  notation, computation is modeled by the flow of particles in a
  circuit.

\item Since these new models are substantially different from \lcal\
  or familiar high-level languages, a point of concern is how one can
  effectively develop programs in them.  We address this by presenting
  a straightforward technique for deriving $\Pi$ programs by
  systematically translating logically reversible small-step abstract
  machines~\cite{isoint}. Complex programs can be devised in this way;
  we demonstrate the derivation of a meta-circular interpreter for
  $\Pi$.

\item We develop an arrow meta-language to encapsulate information
  effects. This metalanguage serves to encapsulate effects, in much
  the same way that traditional arrows or monads serve to encapsulate
  effects over the \lcal.

\item We show how a conventional irreversible model can be expressed
  in our model. We compile the first-order fragment of typed \lcal\
  extended with sums, products and loops to $\Pi$. This compilation is
  interesting for two reasons: (1) it exposes the implicit information
  effects of \lcal\ in the compilation to $\Pi$ and (2) the
  compilation of $\Pi$ to $\Pi$ shows that information effects can be
  erased by treating them as interactions with an explicit information
  environment.

\item We develop the notion of `duality' in $\Pi$ which gives us not
  one duality (like in linear logic or Filinski's symmetric
  \lcal~\cite{Filinski:89}), but two notions of duality -- an additive
  duality and a multiplicative duality. This gives a crisp semantics
  for negative and fractional types in the context of $\Pi$.

\end{enumerate}

Several intriguing areas of exploration remain and the book concludes
with discussion of some of these, along with other possible
applications (see Ch.~\ref{ch:discussion}).  Of these, the field of
reversible and Quantum computing is closely related and we survey
several connections in depth.  The exact connection between $\Pi$ and
linear logic~\cite{Girard87tcs} is not fully understood though it is
clear that both systems capture related but different notions of
resource usage. A closely related area is the connection with duality
of computation \cite{Filinski:89, DBLP:conf/icfp/CurienH00,
  10.1109/LICS.2010.23,Wadler:2003} --- i.e., the quest for a unifying
framework between `computations', values, continuations and their
logical counterparts. The investigation of duality gives rise to
negative and fractional types which have an intuitive semantics and
are reminiscent of negative information flow, superposition and
entanglement from Quantum Physics~\cite{piee}.


